<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=emulateIE7" />
    <title>Coverage for C:\Users\paper\Anaconda3\Lib\site-packages\torch\nn\utils\rnn.py: 27%</title>
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="jquery.min.js"></script>
    <script type="text/javascript" src="jquery.hotkeys.js"></script>
    <script type="text/javascript" src="jquery.isonscreen.js"></script>
    <script type="text/javascript" src="coverage_html.js"></script>
    <script type="text/javascript">
        jQuery(document).ready(coverage.pyfile_ready);
    </script>
</head>
<body class="pyfile">
<div id="header">
    <div class="content">
        <h1>Coverage for <b>C:\Users\paper\Anaconda3\Lib\site-packages\torch\nn\utils\rnn.py</b> :
            <span class="pc_cov">27%</span>
        </h1>
        <img id="keyboard_icon" src="keybd_closed.png" alt="Show keyboard shortcuts" />
        <h2 class="stats">
            108 statements &nbsp;
            <span class="run shortkey_r button_toggle_run">29 run</span>
            <span class="mis show_mis shortkey_m button_toggle_mis">79 missing</span>
            <span class="exc show_exc shortkey_x button_toggle_exc">0 excluded</span>
        </h2>
    </div>
</div>
<div class="help_panel">
    <img id="panel_icon" src="keybd_open.png" alt="Hide keyboard shortcuts" />
    <p class="legend">Hot-keys on this page</p>
    <div>
    <p class="keyhelp">
        <span class="key">r</span>
        <span class="key">m</span>
        <span class="key">x</span>
        <span class="key">p</span> &nbsp; toggle line displays
    </p>
    <p class="keyhelp">
        <span class="key">j</span>
        <span class="key">k</span> &nbsp; next/prev highlighted chunk
    </p>
    <p class="keyhelp">
        <span class="key">0</span> &nbsp; (zero) top of page
    </p>
    <p class="keyhelp">
        <span class="key">1</span> &nbsp; (one) first highlighted chunk
    </p>
    </div>
</div>
<div id="source">
    <p id="t1" class="run"><span class="n"><a href="#t1">1</a></span><span class="t"><span class="key">from</span> <span class="nam">collections</span> <span class="key">import</span> <span class="nam">namedtuple</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2" class="run"><span class="n"><a href="#t2">2</a></span><span class="t"><span class="key">import</span> <span class="nam">warnings</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3" class="pln"><span class="n"><a href="#t3">3</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4" class="run"><span class="n"><a href="#t4">4</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5" class="run"><span class="n"><a href="#t5">5</a></span><span class="t"><span class="key">from</span> <span class="op">.</span><span class="op">.</span> <span class="key">import</span> <span class="nam">_VF</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6" class="run"><span class="n"><a href="#t6">6</a></span><span class="t"><span class="key">from</span> <span class="op">...</span><span class="nam">_jit_internal</span> <span class="key">import</span> <span class="nam">Optional</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7" class="pln"><span class="n"><a href="#t7">7</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t8" class="pln"><span class="n"><a href="#t8">8</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t9" class="run"><span class="n"><a href="#t9">9</a></span><span class="t"><span class="nam">PackedSequence_</span> <span class="op">=</span> <span class="nam">namedtuple</span><span class="op">(</span><span class="str">'PackedSequence'</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t10" class="pln"><span class="n"><a href="#t10">10</a></span><span class="t">                             <span class="op">[</span><span class="str">'data'</span><span class="op">,</span> <span class="str">'batch_sizes'</span><span class="op">,</span> <span class="str">'sorted_indices'</span><span class="op">,</span> <span class="str">'unsorted_indices'</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t11" class="pln"><span class="n"><a href="#t11">11</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t12" class="pln"><span class="n"><a href="#t12">12</a></span><span class="t"><span class="com"># type annotation for PackedSequence_ to make it compatible with TorchScript</span>&nbsp;</span><span class="r"></span></p>
    <p id="t13" class="run"><span class="n"><a href="#t13">13</a></span><span class="t"><span class="nam">PackedSequence_</span><span class="op">.</span><span class="nam">__annotations__</span> <span class="op">=</span> <span class="op">{</span><span class="str">'data'</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="str">'batch_sizes'</span><span class="op">:</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t14" class="pln"><span class="n"><a href="#t14">14</a></span><span class="t">                                   <span class="str">'sorted_indices'</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t15" class="pln"><span class="n"><a href="#t15">15</a></span><span class="t">                                   <span class="str">'unsorted_indices'</span><span class="op">:</span> <span class="nam">Optional</span><span class="op">[</span><span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">]</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t16" class="pln"><span class="n"><a href="#t16">16</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t17" class="run"><span class="n"><a href="#t17">17</a></span><span class="t"><span class="key">def</span> <span class="nam">bind</span><span class="op">(</span><span class="nam">optional</span><span class="op">,</span> <span class="nam">fn</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t18" class="mis show_mis"><span class="n"><a href="#t18">18</a></span><span class="t">    <span class="key">if</span> <span class="nam">optional</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t19" class="mis show_mis"><span class="n"><a href="#t19">19</a></span><span class="t">        <span class="key">return</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p id="t20" class="mis show_mis"><span class="n"><a href="#t20">20</a></span><span class="t">    <span class="key">return</span> <span class="nam">fn</span><span class="op">(</span><span class="nam">optional</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t21" class="pln"><span class="n"><a href="#t21">21</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t22" class="pln"><span class="n"><a href="#t22">22</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t23" class="run"><span class="n"><a href="#t23">23</a></span><span class="t"><span class="key">class</span> <span class="nam">PackedSequence</span><span class="op">(</span><span class="nam">PackedSequence_</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t24" class="pln"><span class="n"><a href="#t24">24</a></span><span class="t">    <span class="str">r"""Holds the data and list of :attr:`batch_sizes` of a packed sequence.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t25" class="pln"><span class="n"><a href="#t25">25</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t26" class="pln"><span class="n"><a href="#t26">26</a></span><span class="t"><span class="str">    All RNN modules accept packed sequences as inputs.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t27" class="pln"><span class="n"><a href="#t27">27</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t28" class="pln"><span class="n"><a href="#t28">28</a></span><span class="t"><span class="str">    Note:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t29" class="pln"><span class="n"><a href="#t29">29</a></span><span class="t"><span class="str">        Instances of this class should never be created manually. They are meant</span>&nbsp;</span><span class="r"></span></p>
    <p id="t30" class="pln"><span class="n"><a href="#t30">30</a></span><span class="t"><span class="str">        to be instantiated by functions like :func:`pack_padded_sequence`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t31" class="pln"><span class="n"><a href="#t31">31</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t32" class="pln"><span class="n"><a href="#t32">32</a></span><span class="t"><span class="str">        Batch sizes represent the number elements at each sequence step in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t33" class="pln"><span class="n"><a href="#t33">33</a></span><span class="t"><span class="str">        the batch, not the varying sequence lengths passed to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t34" class="pln"><span class="n"><a href="#t34">34</a></span><span class="t"><span class="str">        :func:`pack_padded_sequence`.  For instance, given data ``abc`` and ``x``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t35" class="pln"><span class="n"><a href="#t35">35</a></span><span class="t"><span class="str">        the :class:`PackedSequence` would contain data ``axbc`` with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t36" class="pln"><span class="n"><a href="#t36">36</a></span><span class="t"><span class="str">        ``batch_sizes=[2,1,1]``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t37" class="pln"><span class="n"><a href="#t37">37</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t38" class="pln"><span class="n"><a href="#t38">38</a></span><span class="t"><span class="str">    Attributes:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t39" class="pln"><span class="n"><a href="#t39">39</a></span><span class="t"><span class="str">        data (Tensor): Tensor containing packed sequence</span>&nbsp;</span><span class="r"></span></p>
    <p id="t40" class="pln"><span class="n"><a href="#t40">40</a></span><span class="t"><span class="str">        batch_sizes (Tensor): Tensor of integers holding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t41" class="pln"><span class="n"><a href="#t41">41</a></span><span class="t"><span class="str">            information about the batch size at each sequence step</span>&nbsp;</span><span class="r"></span></p>
    <p id="t42" class="pln"><span class="n"><a href="#t42">42</a></span><span class="t"><span class="str">        sorted_indices (Tensor, optional): Tensor of integers holding how this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t43" class="pln"><span class="n"><a href="#t43">43</a></span><span class="t"><span class="str">            :class:`PackedSequence` is constructed from sequences.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t44" class="pln"><span class="n"><a href="#t44">44</a></span><span class="t"><span class="str">        unsorted_indices (Tensor, optional): Tensor of integers holding how this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t45" class="pln"><span class="n"><a href="#t45">45</a></span><span class="t"><span class="str">            to recover the original sequences with correct order.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t46" class="pln"><span class="n"><a href="#t46">46</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t47" class="pln"><span class="n"><a href="#t47">47</a></span><span class="t"><span class="str">    .. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t48" class="pln"><span class="n"><a href="#t48">48</a></span><span class="t"><span class="str">        :attr:`data` can be on arbitrary device and of arbitrary dtype.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t49" class="pln"><span class="n"><a href="#t49">49</a></span><span class="t"><span class="str">        :attr:`sorted_indices` and :attr:`unsorted_indices` must be ``torch.int64``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t50" class="pln"><span class="n"><a href="#t50">50</a></span><span class="t"><span class="str">        tensors on the same device as :attr:`data`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t51" class="pln"><span class="n"><a href="#t51">51</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t52" class="pln"><span class="n"><a href="#t52">52</a></span><span class="t"><span class="str">        However, :attr:`batch_sizes` should always be a CPU ``torch.int64`` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t53" class="pln"><span class="n"><a href="#t53">53</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t54" class="pln"><span class="n"><a href="#t54">54</a></span><span class="t"><span class="str">        This invariant is maintained throughout :class:`PackedSequence` class,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t55" class="pln"><span class="n"><a href="#t55">55</a></span><span class="t"><span class="str">        and all functions that construct a `:class:PackedSequence` in PyTorch</span>&nbsp;</span><span class="r"></span></p>
    <p id="t56" class="pln"><span class="n"><a href="#t56">56</a></span><span class="t"><span class="str">        (i.e., they only pass in tensors conforming to this constraint).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t57" class="pln"><span class="n"><a href="#t57">57</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t58" class="pln"><span class="n"><a href="#t58">58</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t59" class="pln"><span class="n"><a href="#t59">59</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t60" class="pln"><span class="n"><a href="#t60">60</a></span><span class="t">    <span class="com"># NOTE [ device and dtype of a PackedSequence ]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t61" class="pln"><span class="n"><a href="#t61">61</a></span><span class="t">    <span class="com">#</span>&nbsp;</span><span class="r"></span></p>
    <p id="t62" class="pln"><span class="n"><a href="#t62">62</a></span><span class="t">    <span class="com"># See the note above in doc string (starting with ":attr:`data` can be on</span>&nbsp;</span><span class="r"></span></p>
    <p id="t63" class="pln"><span class="n"><a href="#t63">63</a></span><span class="t">    <span class="com"># arbitrary device...").</span>&nbsp;</span><span class="r"></span></p>
    <p id="t64" class="pln"><span class="n"><a href="#t64">64</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t65" class="run"><span class="n"><a href="#t65">65</a></span><span class="t">    <span class="key">def</span> <span class="nam">__new__</span><span class="op">(</span><span class="nam">cls</span><span class="op">,</span> <span class="nam">data</span><span class="op">,</span> <span class="nam">batch_sizes</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">sorted_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">unsorted_indices</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t66" class="pln"><span class="n"><a href="#t66">66</a></span><span class="t">        <span class="com"># PackedSequence used to only have __init__(self, data, batch_sizes)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t67" class="pln"><span class="n"><a href="#t67">67</a></span><span class="t">        <span class="com"># without a __new__ like this. So to preserve BC for calling in keyword</span>&nbsp;</span><span class="r"></span></p>
    <p id="t68" class="pln"><span class="n"><a href="#t68">68</a></span><span class="t">        <span class="com"># arg style (e.g., `PackedSequence(data=..., batch_sizes=...)`), we have</span>&nbsp;</span><span class="r"></span></p>
    <p id="t69" class="pln"><span class="n"><a href="#t69">69</a></span><span class="t">        <span class="com"># to provide two arguments with exact names `data` and `batch_sizes`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t70" class="pln"><span class="n"><a href="#t70">70</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t71" class="pln"><span class="n"><a href="#t71">71</a></span><span class="t">        <span class="com"># NB: if unsorted_indices is provided, it should be the inverse permutation</span>&nbsp;</span><span class="r"></span></p>
    <p id="t72" class="pln"><span class="n"><a href="#t72">72</a></span><span class="t">        <span class="com"># to sorted_indices. Don't assert it here because the PackedSequence ctor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t73" class="pln"><span class="n"><a href="#t73">73</a></span><span class="t">        <span class="com"># should only be used internally.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t74" class="mis show_mis"><span class="n"><a href="#t74">74</a></span><span class="t">        <span class="key">if</span> <span class="nam">unsorted_indices</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t75" class="mis show_mis"><span class="n"><a href="#t75">75</a></span><span class="t">            <span class="nam">unsorted_indices</span> <span class="op">=</span> <span class="nam">invert_permutation</span><span class="op">(</span><span class="nam">sorted_indices</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t76" class="pln"><span class="n"><a href="#t76">76</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t77" class="pln"><span class="n"><a href="#t77">77</a></span><span class="t">        <span class="com"># support being called as `PackedSequence(data, batch_sizes, sorted_indices)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t78" class="mis show_mis"><span class="n"><a href="#t78">78</a></span><span class="t">        <span class="key">if</span> <span class="nam">batch_sizes</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t79" class="mis show_mis"><span class="n"><a href="#t79">79</a></span><span class="t">            <span class="key">if</span> <span class="nam">batch_sizes</span><span class="op">.</span><span class="nam">device</span><span class="op">.</span><span class="nam">type</span> <span class="op">!=</span> <span class="str">'cpu'</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t80" class="mis show_mis"><span class="n"><a href="#t80">80</a></span><span class="t">                <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t81" class="pln"><span class="n"><a href="#t81">81</a></span><span class="t">                    <span class="str">"batch_sizes should always be on CPU. "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t82" class="pln"><span class="n"><a href="#t82">82</a></span><span class="t">                    <span class="str">"Instances of PackedSequence should never be created manually. "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t83" class="pln"><span class="n"><a href="#t83">83</a></span><span class="t">                    <span class="str">"They should be instantiated by functions like pack_sequence "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t84" class="pln"><span class="n"><a href="#t84">84</a></span><span class="t">                    <span class="str">"and pack_padded_sequences in nn.utils.rnn. "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t85" class="pln"><span class="n"><a href="#t85">85</a></span><span class="t">                    <span class="str">"https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t86" class="mis show_mis"><span class="n"><a href="#t86">86</a></span><span class="t">            <span class="key">return</span> <span class="nam">super</span><span class="op">(</span><span class="nam">PackedSequence</span><span class="op">,</span> <span class="nam">cls</span><span class="op">)</span><span class="op">.</span><span class="nam">__new__</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t87" class="pln"><span class="n"><a href="#t87">87</a></span><span class="t">                <span class="nam">cls</span><span class="op">,</span> <span class="nam">data</span><span class="op">,</span> <span class="nam">batch_sizes</span><span class="op">,</span> <span class="nam">sorted_indices</span><span class="op">,</span> <span class="nam">unsorted_indices</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t88" class="pln"><span class="n"><a href="#t88">88</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t89" class="pln"><span class="n"><a href="#t89">89</a></span><span class="t">        <span class="com"># support being called as `PackedSequence((data, batch_sizes), *, sorted_indices)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t90" class="pln"><span class="n"><a href="#t90">90</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t91" class="mis show_mis"><span class="n"><a href="#t91">91</a></span><span class="t">            <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span> <span class="key">and</span> <span class="nam">len</span><span class="op">(</span><span class="nam">data</span><span class="op">)</span> <span class="op">==</span> <span class="num">2</span>&nbsp;</span><span class="r"></span></p>
    <p id="t92" class="mis show_mis"><span class="n"><a href="#t92">92</a></span><span class="t">            <span class="key">return</span> <span class="nam">super</span><span class="op">(</span><span class="nam">PackedSequence</span><span class="op">,</span> <span class="nam">cls</span><span class="op">)</span><span class="op">.</span><span class="nam">__new__</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t93" class="pln"><span class="n"><a href="#t93">93</a></span><span class="t">                <span class="nam">cls</span><span class="op">,</span> <span class="nam">data</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">data</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">,</span> <span class="nam">sorted_indices</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t94" class="pln"><span class="n"><a href="#t94">94</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t95" class="run"><span class="n"><a href="#t95">95</a></span><span class="t">    <span class="key">def</span> <span class="nam">pin_memory</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t96" class="pln"><span class="n"><a href="#t96">96</a></span><span class="t">        <span class="com"># Why not convert `batch_sizes`?</span>&nbsp;</span><span class="r"></span></p>
    <p id="t97" class="pln"><span class="n"><a href="#t97">97</a></span><span class="t">        <span class="com"># See NOTE [ device and dtype of a PackedSequence ]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t98" class="mis show_mis"><span class="n"><a href="#t98">98</a></span><span class="t">        <span class="key">return</span> <span class="nam">type</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">pin_memory</span><span class="op">(</span><span class="op">)</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">batch_sizes</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t99" class="pln"><span class="n"><a href="#t99">99</a></span><span class="t">                          <span class="nam">bind</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">sorted_indices</span><span class="op">,</span> <span class="key">lambda</span> <span class="nam">t</span><span class="op">:</span> <span class="nam">t</span><span class="op">.</span><span class="nam">pin_memory</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t100" class="pln"><span class="n"><a href="#t100">100</a></span><span class="t">                          <span class="nam">bind</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">unsorted_indices</span><span class="op">,</span> <span class="key">lambda</span> <span class="nam">t</span><span class="op">:</span> <span class="nam">t</span><span class="op">.</span><span class="nam">pin_memory</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t101" class="pln"><span class="n"><a href="#t101">101</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t102" class="run"><span class="n"><a href="#t102">102</a></span><span class="t">    <span class="key">def</span> <span class="nam">cuda</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t103" class="pln"><span class="n"><a href="#t103">103</a></span><span class="t">        <span class="com"># Tests to see if 'cuda' should be added to kwargs</span>&nbsp;</span><span class="r"></span></p>
    <p id="t104" class="mis show_mis"><span class="n"><a href="#t104">104</a></span><span class="t">        <span class="nam">ex</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="op">(</span><span class="op">)</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t105" class="mis show_mis"><span class="n"><a href="#t105">105</a></span><span class="t">        <span class="key">if</span> <span class="nam">ex</span><span class="op">.</span><span class="nam">is_cuda</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t106" class="mis show_mis"><span class="n"><a href="#t106">106</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t107" class="mis show_mis"><span class="n"><a href="#t107">107</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="str">'cuda'</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t108" class="pln"><span class="n"><a href="#t108">108</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t109" class="run"><span class="n"><a href="#t109">109</a></span><span class="t">    <span class="key">def</span> <span class="nam">cpu</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t110" class="pln"><span class="n"><a href="#t110">110</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t111" class="mis show_mis"><span class="n"><a href="#t111">111</a></span><span class="t">        <span class="nam">ex</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="op">(</span><span class="op">)</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t112" class="mis show_mis"><span class="n"><a href="#t112">112</a></span><span class="t">        <span class="key">if</span> <span class="nam">ex</span><span class="op">.</span><span class="nam">device</span><span class="op">.</span><span class="nam">type</span> <span class="op">==</span> <span class="str">'cpu'</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t113" class="mis show_mis"><span class="n"><a href="#t113">113</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t114" class="mis show_mis"><span class="n"><a href="#t114">114</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="str">'cpu'</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t115" class="pln"><span class="n"><a href="#t115">115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t116" class="run"><span class="n"><a href="#t116">116</a></span><span class="t">    <span class="key">def</span> <span class="nam">double</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t117" class="mis show_mis"><span class="n"><a href="#t117">117</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">double</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t118" class="pln"><span class="n"><a href="#t118">118</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t119" class="run"><span class="n"><a href="#t119">119</a></span><span class="t">    <span class="key">def</span> <span class="nam">float</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t120" class="mis show_mis"><span class="n"><a href="#t120">120</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">float</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t121" class="pln"><span class="n"><a href="#t121">121</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t122" class="run"><span class="n"><a href="#t122">122</a></span><span class="t">    <span class="key">def</span> <span class="nam">half</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t123" class="mis show_mis"><span class="n"><a href="#t123">123</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">half</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t124" class="pln"><span class="n"><a href="#t124">124</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t125" class="run"><span class="n"><a href="#t125">125</a></span><span class="t">    <span class="key">def</span> <span class="nam">long</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t126" class="mis show_mis"><span class="n"><a href="#t126">126</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">long</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t127" class="pln"><span class="n"><a href="#t127">127</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t128" class="run"><span class="n"><a href="#t128">128</a></span><span class="t">    <span class="key">def</span> <span class="nam">int</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t129" class="mis show_mis"><span class="n"><a href="#t129">129</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">int</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t130" class="pln"><span class="n"><a href="#t130">130</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t131" class="run"><span class="n"><a href="#t131">131</a></span><span class="t">    <span class="key">def</span> <span class="nam">short</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t132" class="mis show_mis"><span class="n"><a href="#t132">132</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">short</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t133" class="pln"><span class="n"><a href="#t133">133</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t134" class="run"><span class="n"><a href="#t134">134</a></span><span class="t">    <span class="key">def</span> <span class="nam">char</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t135" class="mis show_mis"><span class="n"><a href="#t135">135</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">int8</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t136" class="pln"><span class="n"><a href="#t136">136</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t137" class="run"><span class="n"><a href="#t137">137</a></span><span class="t">    <span class="key">def</span> <span class="nam">byte</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t138" class="mis show_mis"><span class="n"><a href="#t138">138</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">uint8</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t139" class="pln"><span class="n"><a href="#t139">139</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t140" class="run"><span class="n"><a href="#t140">140</a></span><span class="t">    <span class="key">def</span> <span class="nam">to</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t141" class="pln"><span class="n"><a href="#t141">141</a></span><span class="t">        <span class="str">r"""Performs dtype and/or device conversion on `self.data`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t142" class="pln"><span class="n"><a href="#t142">142</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t143" class="pln"><span class="n"><a href="#t143">143</a></span><span class="t"><span class="str">        It has similar signature as :meth:`torch.Tensor.to`, except optional</span>&nbsp;</span><span class="r"></span></p>
    <p id="t144" class="pln"><span class="n"><a href="#t144">144</a></span><span class="t"><span class="str">        arguments like `non_blocking` and `copy` should be passed as kwargs,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t145" class="pln"><span class="n"><a href="#t145">145</a></span><span class="t"><span class="str">        not args, or they will not apply to the index tensors.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t146" class="pln"><span class="n"><a href="#t146">146</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t147" class="pln"><span class="n"><a href="#t147">147</a></span><span class="t"><span class="str">        .. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t148" class="pln"><span class="n"><a href="#t148">148</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t149" class="pln"><span class="n"><a href="#t149">149</a></span><span class="t"><span class="str">            If the ``self.data`` Tensor already has the correct :class:`torch.dtype`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t150" class="pln"><span class="n"><a href="#t150">150</a></span><span class="t"><span class="str">            and :class:`torch.device`, then ``self`` is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t151" class="pln"><span class="n"><a href="#t151">151</a></span><span class="t"><span class="str">            Otherwise, returns a copy with the desired configuration.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t152" class="pln"><span class="n"><a href="#t152">152</a></span><span class="t"><span class="str">        """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t153" class="pln"><span class="n"><a href="#t153">153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t154" class="pln"><span class="n"><a href="#t154">154</a></span><span class="t">        <span class="com"># Why not convert `batch_sizes`?</span>&nbsp;</span><span class="r"></span></p>
    <p id="t155" class="pln"><span class="n"><a href="#t155">155</a></span><span class="t">        <span class="com"># See NOTE [ device and dtype of a PackedSequence ]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t156" class="mis show_mis"><span class="n"><a href="#t156">156</a></span><span class="t">        <span class="nam">data</span> <span class="op">=</span> <span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t157" class="mis show_mis"><span class="n"><a href="#t157">157</a></span><span class="t">        <span class="key">if</span> <span class="nam">data</span> <span class="key">is</span> <span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t158" class="mis show_mis"><span class="n"><a href="#t158">158</a></span><span class="t">            <span class="key">return</span> <span class="nam">self</span>&nbsp;</span><span class="r"></span></p>
    <p id="t159" class="pln"><span class="n"><a href="#t159">159</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t160" class="pln"><span class="n"><a href="#t160">160</a></span><span class="t">            <span class="com"># Does not forward device or dtype arg/kwargs, device is set from data.device</span>&nbsp;</span><span class="r"></span></p>
    <p id="t161" class="mis show_mis"><span class="n"><a href="#t161">161</a></span><span class="t">            <span class="nam">kwargs</span> <span class="op">=</span> <span class="op">{</span><span class="nam">k</span> <span class="op">:</span> <span class="nam">v</span> <span class="key">for</span> <span class="nam">k</span><span class="op">,</span> <span class="nam">v</span> <span class="key">in</span> <span class="nam">filter</span><span class="op">(</span><span class="key">lambda</span> <span class="nam">t</span><span class="op">:</span> <span class="nam">t</span><span class="op">[</span><span class="num">0</span><span class="op">]</span> <span class="op">!=</span> <span class="str">'device'</span> <span class="key">and</span> <span class="nam">t</span><span class="op">[</span><span class="num">0</span><span class="op">]</span> <span class="op">!=</span> <span class="str">'dtype'</span><span class="op">,</span> <span class="nam">kwargs</span><span class="op">.</span><span class="nam">items</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t162" class="mis show_mis"><span class="n"><a href="#t162">162</a></span><span class="t">            <span class="nam">sorted_indices</span> <span class="op">=</span> <span class="nam">bind</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">sorted_indices</span><span class="op">,</span> <span class="key">lambda</span> <span class="nam">t</span><span class="op">:</span> <span class="nam">t</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">data</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t163" class="mis show_mis"><span class="n"><a href="#t163">163</a></span><span class="t">            <span class="nam">unsorted_indices</span> <span class="op">=</span> <span class="nam">bind</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">unsorted_indices</span><span class="op">,</span> <span class="key">lambda</span> <span class="nam">t</span><span class="op">:</span> <span class="nam">t</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">data</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t164" class="mis show_mis"><span class="n"><a href="#t164">164</a></span><span class="t">            <span class="key">return</span> <span class="nam">type</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">batch_sizes</span><span class="op">,</span> <span class="nam">sorted_indices</span><span class="op">,</span> <span class="nam">unsorted_indices</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t165" class="pln"><span class="n"><a href="#t165">165</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t166" class="run"><span class="n"><a href="#t166">166</a></span><span class="t">    <span class="op">@</span><span class="nam">property</span>&nbsp;</span><span class="r"></span></p>
    <p id="t167" class="pln"><span class="n"><a href="#t167">167</a></span><span class="t">    <span class="key">def</span> <span class="nam">is_cuda</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t168" class="pln"><span class="n"><a href="#t168">168</a></span><span class="t">        <span class="str">r"""Returns true if `self.data` stored on a gpu"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t169" class="mis show_mis"><span class="n"><a href="#t169">169</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">is_cuda</span>&nbsp;</span><span class="r"></span></p>
    <p id="t170" class="pln"><span class="n"><a href="#t170">170</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t171" class="run"><span class="n"><a href="#t171">171</a></span><span class="t">    <span class="key">def</span> <span class="nam">is_pinned</span><span class="op">(</span><span class="nam">self</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t172" class="pln"><span class="n"><a href="#t172">172</a></span><span class="t">        <span class="str">r"""Returns true if `self.data` stored on in pinned memory"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t173" class="mis show_mis"><span class="n"><a href="#t173">173</a></span><span class="t">        <span class="key">return</span> <span class="nam">self</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">is_pinned</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t174" class="pln"><span class="n"><a href="#t174">174</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t175" class="pln"><span class="n"><a href="#t175">175</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t176" class="run"><span class="n"><a href="#t176">176</a></span><span class="t"><span class="key">def</span> <span class="nam">invert_permutation</span><span class="op">(</span><span class="nam">permutation</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t177" class="mis show_mis"><span class="n"><a href="#t177">177</a></span><span class="t">    <span class="key">if</span> <span class="nam">permutation</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t178" class="mis show_mis"><span class="n"><a href="#t178">178</a></span><span class="t">        <span class="key">return</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p id="t179" class="mis show_mis"><span class="n"><a href="#t179">179</a></span><span class="t">    <span class="nam">output</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">empty_like</span><span class="op">(</span><span class="nam">permutation</span><span class="op">,</span> <span class="nam">memory_format</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">legacy_contiguous_format</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t180" class="mis show_mis"><span class="n"><a href="#t180">180</a></span><span class="t">    <span class="nam">output</span><span class="op">.</span><span class="nam">scatter_</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">permutation</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t181" class="pln"><span class="n"><a href="#t181">181</a></span><span class="t">                    <span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">permutation</span><span class="op">.</span><span class="nam">numel</span><span class="op">(</span><span class="op">)</span><span class="op">,</span> <span class="nam">device</span><span class="op">=</span><span class="nam">permutation</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t182" class="mis show_mis"><span class="n"><a href="#t182">182</a></span><span class="t">    <span class="key">return</span> <span class="nam">output</span>&nbsp;</span><span class="r"></span></p>
    <p id="t183" class="pln"><span class="n"><a href="#t183">183</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t184" class="pln"><span class="n"><a href="#t184">184</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t185" class="run"><span class="n"><a href="#t185">185</a></span><span class="t"><span class="key">def</span> <span class="nam">pack_padded_sequence</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">lengths</span><span class="op">,</span> <span class="nam">batch_first</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">enforce_sorted</span><span class="op">=</span><span class="key">True</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t186" class="pln"><span class="n"><a href="#t186">186</a></span><span class="t">    <span class="com"># type: (Tensor, Tensor, bool, bool) -> PackedSequence</span>&nbsp;</span><span class="r"></span></p>
    <p id="t187" class="pln"><span class="n"><a href="#t187">187</a></span><span class="t">    <span class="str">r"""Packs a Tensor containing padded sequences of variable length.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t188" class="pln"><span class="n"><a href="#t188">188</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t189" class="pln"><span class="n"><a href="#t189">189</a></span><span class="t"><span class="str">    :attr:`input` can be of size ``T x B x *`` where `T` is the length of the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t190" class="pln"><span class="n"><a href="#t190">190</a></span><span class="t"><span class="str">    longest sequence (equal to ``lengths[0]``), ``B`` is the batch size, and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t191" class="pln"><span class="n"><a href="#t191">191</a></span><span class="t"><span class="str">    ``*`` is any number of dimensions (including 0). If ``batch_first`` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t192" class="pln"><span class="n"><a href="#t192">192</a></span><span class="t"><span class="str">    ``True``, ``B x T x *`` :attr:`input` is expected.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t193" class="pln"><span class="n"><a href="#t193">193</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t194" class="pln"><span class="n"><a href="#t194">194</a></span><span class="t"><span class="str">    For unsorted sequences, use `enforce_sorted = False`. If :attr:`enforce_sorted` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t195" class="pln"><span class="n"><a href="#t195">195</a></span><span class="t"><span class="str">    ``True``, the sequences should be sorted by length in a decreasing order, i.e.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t196" class="pln"><span class="n"><a href="#t196">196</a></span><span class="t"><span class="str">    ``input[:,0]`` should be the longest sequence, and ``input[:,B-1]`` the shortest</span>&nbsp;</span><span class="r"></span></p>
    <p id="t197" class="pln"><span class="n"><a href="#t197">197</a></span><span class="t"><span class="str">    one. `enforce_sorted = True` is only necessary for ONNX export.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t198" class="pln"><span class="n"><a href="#t198">198</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t199" class="pln"><span class="n"><a href="#t199">199</a></span><span class="t"><span class="str">    Note:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t200" class="pln"><span class="n"><a href="#t200">200</a></span><span class="t"><span class="str">        This function accepts any input that has at least two dimensions. You</span>&nbsp;</span><span class="r"></span></p>
    <p id="t201" class="pln"><span class="n"><a href="#t201">201</a></span><span class="t"><span class="str">        can apply it to pack the labels, and use the output of the RNN with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t202" class="pln"><span class="n"><a href="#t202">202</a></span><span class="t"><span class="str">        them to compute the loss directly. A Tensor can be retrieved from</span>&nbsp;</span><span class="r"></span></p>
    <p id="t203" class="pln"><span class="n"><a href="#t203">203</a></span><span class="t"><span class="str">        a :class:`PackedSequence` object by accessing its ``.data`` attribute.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t204" class="pln"><span class="n"><a href="#t204">204</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t205" class="pln"><span class="n"><a href="#t205">205</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t206" class="pln"><span class="n"><a href="#t206">206</a></span><span class="t"><span class="str">        input (Tensor): padded batch of variable length sequences.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t207" class="pln"><span class="n"><a href="#t207">207</a></span><span class="t"><span class="str">        lengths (Tensor): list of sequences lengths of each batch element.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t208" class="pln"><span class="n"><a href="#t208">208</a></span><span class="t"><span class="str">        batch_first (bool, optional): if ``True``, the input is expected in ``B x T x *``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t209" class="pln"><span class="n"><a href="#t209">209</a></span><span class="t"><span class="str">            format.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t210" class="pln"><span class="n"><a href="#t210">210</a></span><span class="t"><span class="str">        enforce_sorted (bool, optional): if ``True``, the input is expected to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t211" class="pln"><span class="n"><a href="#t211">211</a></span><span class="t"><span class="str">            contain sequences sorted by length in a decreasing order. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t212" class="pln"><span class="n"><a href="#t212">212</a></span><span class="t"><span class="str">            ``False``, this condition is not checked. Default: ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t213" class="pln"><span class="n"><a href="#t213">213</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t214" class="pln"><span class="n"><a href="#t214">214</a></span><span class="t"><span class="str">    Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t215" class="pln"><span class="n"><a href="#t215">215</a></span><span class="t"><span class="str">        a :class:`PackedSequence` object</span>&nbsp;</span><span class="r"></span></p>
    <p id="t216" class="pln"><span class="n"><a href="#t216">216</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t217" class="mis show_mis"><span class="n"><a href="#t217">217</a></span><span class="t">    <span class="key">if</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">_get_tracing_state</span><span class="op">(</span><span class="op">)</span> <span class="key">and</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">lengths</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t218" class="mis show_mis"><span class="n"><a href="#t218">218</a></span><span class="t">        <span class="nam">warnings</span><span class="op">.</span><span class="nam">warn</span><span class="op">(</span><span class="str">'pack_padded_sequence has been called with a Python list of '</span>&nbsp;</span><span class="r"></span></p>
    <p id="t219" class="pln"><span class="n"><a href="#t219">219</a></span><span class="t">                      <span class="str">'sequence lengths. The tracer cannot track the data flow of Python '</span>&nbsp;</span><span class="r"></span></p>
    <p id="t220" class="pln"><span class="n"><a href="#t220">220</a></span><span class="t">                      <span class="str">'values, and it will treat them as constants, likely rendering '</span>&nbsp;</span><span class="r"></span></p>
    <p id="t221" class="pln"><span class="n"><a href="#t221">221</a></span><span class="t">                      <span class="str">'the trace incorrect for any other combination of lengths.'</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t222" class="pln"><span class="n"><a href="#t222">222</a></span><span class="t">                      <span class="nam">stacklevel</span><span class="op">=</span><span class="num">2</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t223" class="mis show_mis"><span class="n"><a href="#t223">223</a></span><span class="t">    <span class="nam">lengths</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">as_tensor</span><span class="op">(</span><span class="nam">lengths</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">int64</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t224" class="mis show_mis"><span class="n"><a href="#t224">224</a></span><span class="t">    <span class="key">if</span> <span class="nam">enforce_sorted</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t225" class="mis show_mis"><span class="n"><a href="#t225">225</a></span><span class="t">        <span class="nam">sorted_indices</span> <span class="op">=</span> <span class="key">None</span>&nbsp;</span><span class="r"></span></p>
    <p id="t226" class="pln"><span class="n"><a href="#t226">226</a></span><span class="t">    <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t227" class="mis show_mis"><span class="n"><a href="#t227">227</a></span><span class="t">        <span class="nam">lengths</span><span class="op">,</span> <span class="nam">sorted_indices</span> <span class="op">=</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">sort</span><span class="op">(</span><span class="nam">lengths</span><span class="op">,</span> <span class="nam">descending</span><span class="op">=</span><span class="key">True</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t228" class="mis show_mis"><span class="n"><a href="#t228">228</a></span><span class="t">        <span class="nam">sorted_indices</span> <span class="op">=</span> <span class="nam">sorted_indices</span><span class="op">.</span><span class="nam">to</span><span class="op">(</span><span class="nam">input</span><span class="op">.</span><span class="nam">device</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t229" class="mis show_mis"><span class="n"><a href="#t229">229</a></span><span class="t">        <span class="nam">batch_dim</span> <span class="op">=</span> <span class="num">0</span> <span class="key">if</span> <span class="nam">batch_first</span> <span class="key">else</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t230" class="mis show_mis"><span class="n"><a href="#t230">230</a></span><span class="t">        <span class="nam">input</span> <span class="op">=</span> <span class="nam">input</span><span class="op">.</span><span class="nam">index_select</span><span class="op">(</span><span class="nam">batch_dim</span><span class="op">,</span> <span class="nam">sorted_indices</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t231" class="pln"><span class="n"><a href="#t231">231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t232" class="mis show_mis"><span class="n"><a href="#t232">232</a></span><span class="t">    <span class="nam">data</span><span class="op">,</span> <span class="nam">batch_sizes</span> <span class="op">=</span> <span class="xx">\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t233" class="pln"><span class="n"><a href="#t233">233</a></span><span class="t">        <span class="nam">_VF</span><span class="op">.</span><span class="nam">_pack_padded_sequence</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">lengths</span><span class="op">,</span> <span class="nam">batch_first</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t234" class="mis show_mis"><span class="n"><a href="#t234">234</a></span><span class="t">    <span class="key">return</span> <span class="nam">PackedSequence</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">batch_sizes</span><span class="op">,</span> <span class="nam">sorted_indices</span><span class="op">,</span> <span class="key">None</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t235" class="pln"><span class="n"><a href="#t235">235</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t236" class="pln"><span class="n"><a href="#t236">236</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t237" class="run"><span class="n"><a href="#t237">237</a></span><span class="t"><span class="key">def</span> <span class="nam">pad_packed_sequence</span><span class="op">(</span><span class="nam">sequence</span><span class="op">,</span> <span class="nam">batch_first</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">padding_value</span><span class="op">=</span><span class="num">0.0</span><span class="op">,</span> <span class="nam">total_length</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t238" class="pln"><span class="n"><a href="#t238">238</a></span><span class="t">    <span class="com"># type: (PackedSequence, bool, float, Optional[int]) -> Tuple[Tensor, Tensor]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t239" class="pln"><span class="n"><a href="#t239">239</a></span><span class="t">    <span class="str">r"""Pads a packed batch of variable length sequences.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t240" class="pln"><span class="n"><a href="#t240">240</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t241" class="pln"><span class="n"><a href="#t241">241</a></span><span class="t"><span class="str">    It is an inverse operation to :func:`pack_padded_sequence`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t242" class="pln"><span class="n"><a href="#t242">242</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t243" class="pln"><span class="n"><a href="#t243">243</a></span><span class="t"><span class="str">    The returned Tensor's data will be of size ``T x B x *``, where `T` is the length</span>&nbsp;</span><span class="r"></span></p>
    <p id="t244" class="pln"><span class="n"><a href="#t244">244</a></span><span class="t"><span class="str">    of the longest sequence and `B` is the batch size. If ``batch_first`` is True,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t245" class="pln"><span class="n"><a href="#t245">245</a></span><span class="t"><span class="str">    the data will be transposed into ``B x T x *`` format.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t246" class="pln"><span class="n"><a href="#t246">246</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t247" class="pln"><span class="n"><a href="#t247">247</a></span><span class="t"><span class="str">    Batch elements will be ordered decreasingly by their length.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t248" class="pln"><span class="n"><a href="#t248">248</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t249" class="pln"><span class="n"><a href="#t249">249</a></span><span class="t"><span class="str">    .. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t250" class="pln"><span class="n"><a href="#t250">250</a></span><span class="t"><span class="str">        :attr:`total_length` is useful to implement the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t251" class="pln"><span class="n"><a href="#t251">251</a></span><span class="t"><span class="str">        ``pack sequence -> recurrent network -> unpack sequence`` pattern in a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t252" class="pln"><span class="n"><a href="#t252">252</a></span><span class="t"><span class="str">        :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t253" class="pln"><span class="n"><a href="#t253">253</a></span><span class="t"><span class="str">        See :ref:`this FAQ section &lt;pack-rnn-unpack-with-data-parallelism>` for</span>&nbsp;</span><span class="r"></span></p>
    <p id="t254" class="pln"><span class="n"><a href="#t254">254</a></span><span class="t"><span class="str">        details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t255" class="pln"><span class="n"><a href="#t255">255</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t256" class="pln"><span class="n"><a href="#t256">256</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t257" class="pln"><span class="n"><a href="#t257">257</a></span><span class="t"><span class="str">        sequence (PackedSequence): batch to pad</span>&nbsp;</span><span class="r"></span></p>
    <p id="t258" class="pln"><span class="n"><a href="#t258">258</a></span><span class="t"><span class="str">        batch_first (bool, optional): if ``True``, the output will be in ``B x T x *``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t259" class="pln"><span class="n"><a href="#t259">259</a></span><span class="t"><span class="str">            format.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t260" class="pln"><span class="n"><a href="#t260">260</a></span><span class="t"><span class="str">        padding_value (float, optional): values for padded elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t261" class="pln"><span class="n"><a href="#t261">261</a></span><span class="t"><span class="str">        total_length (int, optional): if not ``None``, the output will be padded to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t262" class="pln"><span class="n"><a href="#t262">262</a></span><span class="t"><span class="str">            have length :attr:`total_length`. This method will throw :class:`ValueError`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t263" class="pln"><span class="n"><a href="#t263">263</a></span><span class="t"><span class="str">            if :attr:`total_length` is less than the max sequence length in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t264" class="pln"><span class="n"><a href="#t264">264</a></span><span class="t"><span class="str">            :attr:`sequence`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t265" class="pln"><span class="n"><a href="#t265">265</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t266" class="pln"><span class="n"><a href="#t266">266</a></span><span class="t"><span class="str">    Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t267" class="pln"><span class="n"><a href="#t267">267</a></span><span class="t"><span class="str">        Tuple of Tensor containing the padded sequence, and a Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t268" class="pln"><span class="n"><a href="#t268">268</a></span><span class="t"><span class="str">        containing the list of lengths of each sequence in the batch.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t269" class="pln"><span class="n"><a href="#t269">269</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t270" class="pln"><span class="n"><a href="#t270">270</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t271" class="mis show_mis"><span class="n"><a href="#t271">271</a></span><span class="t">    <span class="nam">max_seq_length</span> <span class="op">=</span> <span class="nam">sequence</span><span class="op">.</span><span class="nam">batch_sizes</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t272" class="mis show_mis"><span class="n"><a href="#t272">272</a></span><span class="t">    <span class="key">if</span> <span class="nam">total_length</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t273" class="mis show_mis"><span class="n"><a href="#t273">273</a></span><span class="t">        <span class="key">if</span> <span class="nam">total_length</span> <span class="op">&lt;</span> <span class="nam">max_seq_length</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t274" class="mis show_mis"><span class="n"><a href="#t274">274</a></span><span class="t">            <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Expected total_length to be at least the length "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t275" class="pln"><span class="n"><a href="#t275">275</a></span><span class="t">                             <span class="str">"of the longest sequence in input, but got "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t276" class="pln"><span class="n"><a href="#t276">276</a></span><span class="t">                             <span class="str">"total_length={} and max sequence length being {}"</span>&nbsp;</span><span class="r"></span></p>
    <p id="t277" class="pln"><span class="n"><a href="#t277">277</a></span><span class="t">                             <span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="nam">total_length</span><span class="op">,</span> <span class="nam">max_seq_length</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t278" class="mis show_mis"><span class="n"><a href="#t278">278</a></span><span class="t">        <span class="nam">max_seq_length</span> <span class="op">=</span> <span class="nam">total_length</span>&nbsp;</span><span class="r"></span></p>
    <p id="t279" class="mis show_mis"><span class="n"><a href="#t279">279</a></span><span class="t">    <span class="nam">padded_output</span><span class="op">,</span> <span class="nam">lengths</span> <span class="op">=</span> <span class="nam">_VF</span><span class="op">.</span><span class="nam">_pad_packed_sequence</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t280" class="pln"><span class="n"><a href="#t280">280</a></span><span class="t">        <span class="nam">sequence</span><span class="op">.</span><span class="nam">data</span><span class="op">,</span> <span class="nam">sequence</span><span class="op">.</span><span class="nam">batch_sizes</span><span class="op">,</span> <span class="nam">batch_first</span><span class="op">,</span> <span class="nam">padding_value</span><span class="op">,</span> <span class="nam">max_seq_length</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t281" class="mis show_mis"><span class="n"><a href="#t281">281</a></span><span class="t">    <span class="nam">unsorted_indices</span> <span class="op">=</span> <span class="nam">sequence</span><span class="op">.</span><span class="nam">unsorted_indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t282" class="mis show_mis"><span class="n"><a href="#t282">282</a></span><span class="t">    <span class="key">if</span> <span class="nam">unsorted_indices</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t283" class="mis show_mis"><span class="n"><a href="#t283">283</a></span><span class="t">        <span class="nam">batch_dim</span> <span class="op">=</span> <span class="num">0</span> <span class="key">if</span> <span class="nam">batch_first</span> <span class="key">else</span> <span class="num">1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t284" class="mis show_mis"><span class="n"><a href="#t284">284</a></span><span class="t">        <span class="key">return</span> <span class="nam">padded_output</span><span class="op">.</span><span class="nam">index_select</span><span class="op">(</span><span class="nam">batch_dim</span><span class="op">,</span> <span class="nam">unsorted_indices</span><span class="op">)</span><span class="op">,</span> <span class="nam">lengths</span><span class="op">[</span><span class="nam">unsorted_indices</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t285" class="mis show_mis"><span class="n"><a href="#t285">285</a></span><span class="t">    <span class="key">return</span> <span class="nam">padded_output</span><span class="op">,</span> <span class="nam">lengths</span>&nbsp;</span><span class="r"></span></p>
    <p id="t286" class="pln"><span class="n"><a href="#t286">286</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t287" class="pln"><span class="n"><a href="#t287">287</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t288" class="run"><span class="n"><a href="#t288">288</a></span><span class="t"><span class="key">def</span> <span class="nam">pad_sequence</span><span class="op">(</span><span class="nam">sequences</span><span class="op">,</span> <span class="nam">batch_first</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">padding_value</span><span class="op">=</span><span class="num">0</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t289" class="pln"><span class="n"><a href="#t289">289</a></span><span class="t">    <span class="str">r"""Pad a list of variable length Tensors with ``padding_value``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t290" class="pln"><span class="n"><a href="#t290">290</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t291" class="pln"><span class="n"><a href="#t291">291</a></span><span class="t"><span class="str">    ``pad_sequence`` stacks a list of Tensors along a new dimension,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t292" class="pln"><span class="n"><a href="#t292">292</a></span><span class="t"><span class="str">    and pads them to equal length. For example, if the input is list of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t293" class="pln"><span class="n"><a href="#t293">293</a></span><span class="t"><span class="str">    sequences with size ``L x *`` and if batch_first is False, and ``T x B x *``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t294" class="pln"><span class="n"><a href="#t294">294</a></span><span class="t"><span class="str">    otherwise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t295" class="pln"><span class="n"><a href="#t295">295</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t296" class="pln"><span class="n"><a href="#t296">296</a></span><span class="t"><span class="str">    `B` is batch size. It is equal to the number of elements in ``sequences``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t297" class="pln"><span class="n"><a href="#t297">297</a></span><span class="t"><span class="str">    `T` is length of the longest sequence.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t298" class="pln"><span class="n"><a href="#t298">298</a></span><span class="t"><span class="str">    `L` is length of the sequence.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t299" class="pln"><span class="n"><a href="#t299">299</a></span><span class="t"><span class="str">    `*` is any number of trailing dimensions, including none.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t300" class="pln"><span class="n"><a href="#t300">300</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t301" class="pln"><span class="n"><a href="#t301">301</a></span><span class="t"><span class="str">    Example:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t302" class="pln"><span class="n"><a href="#t302">302</a></span><span class="t"><span class="str">        >>> from torch.nn.utils.rnn import pad_sequence</span>&nbsp;</span><span class="r"></span></p>
    <p id="t303" class="pln"><span class="n"><a href="#t303">303</a></span><span class="t"><span class="str">        >>> a = torch.ones(25, 300)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t304" class="pln"><span class="n"><a href="#t304">304</a></span><span class="t"><span class="str">        >>> b = torch.ones(22, 300)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t305" class="pln"><span class="n"><a href="#t305">305</a></span><span class="t"><span class="str">        >>> c = torch.ones(15, 300)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t306" class="pln"><span class="n"><a href="#t306">306</a></span><span class="t"><span class="str">        >>> pad_sequence([a, b, c]).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t307" class="pln"><span class="n"><a href="#t307">307</a></span><span class="t"><span class="str">        torch.Size([25, 3, 300])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t308" class="pln"><span class="n"><a href="#t308">308</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t309" class="pln"><span class="n"><a href="#t309">309</a></span><span class="t"><span class="str">    Note:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t310" class="pln"><span class="n"><a href="#t310">310</a></span><span class="t"><span class="str">        This function returns a Tensor of size ``T x B x *`` or ``B x T x *``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t311" class="pln"><span class="n"><a href="#t311">311</a></span><span class="t"><span class="str">        where `T` is the length of the longest sequence. This function assumes</span>&nbsp;</span><span class="r"></span></p>
    <p id="t312" class="pln"><span class="n"><a href="#t312">312</a></span><span class="t"><span class="str">        trailing dimensions and type of all the Tensors in sequences are same.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t313" class="pln"><span class="n"><a href="#t313">313</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t314" class="pln"><span class="n"><a href="#t314">314</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t315" class="pln"><span class="n"><a href="#t315">315</a></span><span class="t"><span class="str">        sequences (list[Tensor]): list of variable length sequences.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t316" class="pln"><span class="n"><a href="#t316">316</a></span><span class="t"><span class="str">        batch_first (bool, optional): output will be in ``B x T x *`` if True, or in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t317" class="pln"><span class="n"><a href="#t317">317</a></span><span class="t"><span class="str">            ``T x B x *`` otherwise</span>&nbsp;</span><span class="r"></span></p>
    <p id="t318" class="pln"><span class="n"><a href="#t318">318</a></span><span class="t"><span class="str">        padding_value (float, optional): value for padded elements. Default: 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t319" class="pln"><span class="n"><a href="#t319">319</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t320" class="pln"><span class="n"><a href="#t320">320</a></span><span class="t"><span class="str">    Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t321" class="pln"><span class="n"><a href="#t321">321</a></span><span class="t"><span class="str">        Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t322" class="pln"><span class="n"><a href="#t322">322</a></span><span class="t"><span class="str">        Tensor of size ``B x T x *`` otherwise</span>&nbsp;</span><span class="r"></span></p>
    <p id="t323" class="pln"><span class="n"><a href="#t323">323</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t324" class="pln"><span class="n"><a href="#t324">324</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t325" class="pln"><span class="n"><a href="#t325">325</a></span><span class="t">    <span class="com"># assuming trailing dimensions and type of all the Tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t326" class="pln"><span class="n"><a href="#t326">326</a></span><span class="t">    <span class="com"># in sequences are same and fetching those from sequences[0]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t327" class="mis show_mis"><span class="n"><a href="#t327">327</a></span><span class="t">    <span class="nam">max_size</span> <span class="op">=</span> <span class="nam">sequences</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t328" class="mis show_mis"><span class="n"><a href="#t328">328</a></span><span class="t">    <span class="nam">trailing_dims</span> <span class="op">=</span> <span class="nam">max_size</span><span class="op">[</span><span class="num">1</span><span class="op">:</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t329" class="mis show_mis"><span class="n"><a href="#t329">329</a></span><span class="t">    <span class="nam">max_len</span> <span class="op">=</span> <span class="nam">max</span><span class="op">(</span><span class="op">[</span><span class="nam">s</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">0</span><span class="op">)</span> <span class="key">for</span> <span class="nam">s</span> <span class="key">in</span> <span class="nam">sequences</span><span class="op">]</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t330" class="mis show_mis"><span class="n"><a href="#t330">330</a></span><span class="t">    <span class="key">if</span> <span class="nam">batch_first</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t331" class="mis show_mis"><span class="n"><a href="#t331">331</a></span><span class="t">        <span class="nam">out_dims</span> <span class="op">=</span> <span class="op">(</span><span class="nam">len</span><span class="op">(</span><span class="nam">sequences</span><span class="op">)</span><span class="op">,</span> <span class="nam">max_len</span><span class="op">)</span> <span class="op">+</span> <span class="nam">trailing_dims</span>&nbsp;</span><span class="r"></span></p>
    <p id="t332" class="pln"><span class="n"><a href="#t332">332</a></span><span class="t">    <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t333" class="mis show_mis"><span class="n"><a href="#t333">333</a></span><span class="t">        <span class="nam">out_dims</span> <span class="op">=</span> <span class="op">(</span><span class="nam">max_len</span><span class="op">,</span> <span class="nam">len</span><span class="op">(</span><span class="nam">sequences</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="nam">trailing_dims</span>&nbsp;</span><span class="r"></span></p>
    <p id="t334" class="pln"><span class="n"><a href="#t334">334</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t335" class="mis show_mis"><span class="n"><a href="#t335">335</a></span><span class="t">    <span class="nam">out_tensor</span> <span class="op">=</span> <span class="nam">sequences</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">data</span><span class="op">.</span><span class="nam">new</span><span class="op">(</span><span class="op">*</span><span class="nam">out_dims</span><span class="op">)</span><span class="op">.</span><span class="nam">fill_</span><span class="op">(</span><span class="nam">padding_value</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t336" class="mis show_mis"><span class="n"><a href="#t336">336</a></span><span class="t">    <span class="key">for</span> <span class="nam">i</span><span class="op">,</span> <span class="nam">tensor</span> <span class="key">in</span> <span class="nam">enumerate</span><span class="op">(</span><span class="nam">sequences</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t337" class="mis show_mis"><span class="n"><a href="#t337">337</a></span><span class="t">        <span class="nam">length</span> <span class="op">=</span> <span class="nam">tensor</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">0</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t338" class="pln"><span class="n"><a href="#t338">338</a></span><span class="t">        <span class="com"># use index notation to prevent duplicate references to the tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t339" class="mis show_mis"><span class="n"><a href="#t339">339</a></span><span class="t">        <span class="key">if</span> <span class="nam">batch_first</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t340" class="mis show_mis"><span class="n"><a href="#t340">340</a></span><span class="t">            <span class="nam">out_tensor</span><span class="op">[</span><span class="nam">i</span><span class="op">,</span> <span class="op">:</span><span class="nam">length</span><span class="op">,</span> <span class="op">...</span><span class="op">]</span> <span class="op">=</span> <span class="nam">tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t341" class="pln"><span class="n"><a href="#t341">341</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t342" class="mis show_mis"><span class="n"><a href="#t342">342</a></span><span class="t">            <span class="nam">out_tensor</span><span class="op">[</span><span class="op">:</span><span class="nam">length</span><span class="op">,</span> <span class="nam">i</span><span class="op">,</span> <span class="op">...</span><span class="op">]</span> <span class="op">=</span> <span class="nam">tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t343" class="pln"><span class="n"><a href="#t343">343</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t344" class="mis show_mis"><span class="n"><a href="#t344">344</a></span><span class="t">    <span class="key">return</span> <span class="nam">out_tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t345" class="pln"><span class="n"><a href="#t345">345</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t346" class="pln"><span class="n"><a href="#t346">346</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t347" class="run"><span class="n"><a href="#t347">347</a></span><span class="t"><span class="key">def</span> <span class="nam">pack_sequence</span><span class="op">(</span><span class="nam">sequences</span><span class="op">,</span> <span class="nam">enforce_sorted</span><span class="op">=</span><span class="key">True</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t348" class="pln"><span class="n"><a href="#t348">348</a></span><span class="t">    <span class="com"># type: (List[Tensor], bool) -> PackedSequence</span>&nbsp;</span><span class="r"></span></p>
    <p id="t349" class="pln"><span class="n"><a href="#t349">349</a></span><span class="t">    <span class="str">r"""Packs a list of variable length Tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t350" class="pln"><span class="n"><a href="#t350">350</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t351" class="pln"><span class="n"><a href="#t351">351</a></span><span class="t"><span class="str">    ``sequences`` should be a list of Tensors of size ``L x *``, where `L` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t352" class="pln"><span class="n"><a href="#t352">352</a></span><span class="t"><span class="str">    the length of a sequence and `*` is any number of trailing dimensions,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t353" class="pln"><span class="n"><a href="#t353">353</a></span><span class="t"><span class="str">    including zero.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t354" class="pln"><span class="n"><a href="#t354">354</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t355" class="pln"><span class="n"><a href="#t355">355</a></span><span class="t"><span class="str">    For unsorted sequences, use `enforce_sorted = False`. If ``enforce_sorted``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t356" class="pln"><span class="n"><a href="#t356">356</a></span><span class="t"><span class="str">    is ``True``, the sequences should be sorted in the order of decreasing length.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t357" class="pln"><span class="n"><a href="#t357">357</a></span><span class="t"><span class="str">    ``enforce_sorted = True`` is only necessary for ONNX export.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t358" class="pln"><span class="n"><a href="#t358">358</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t359" class="pln"><span class="n"><a href="#t359">359</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t360" class="pln"><span class="n"><a href="#t360">360</a></span><span class="t"><span class="str">    Example:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t361" class="pln"><span class="n"><a href="#t361">361</a></span><span class="t"><span class="str">        >>> from torch.nn.utils.rnn import pack_sequence</span>&nbsp;</span><span class="r"></span></p>
    <p id="t362" class="pln"><span class="n"><a href="#t362">362</a></span><span class="t"><span class="str">        >>> a = torch.tensor([1,2,3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t363" class="pln"><span class="n"><a href="#t363">363</a></span><span class="t"><span class="str">        >>> b = torch.tensor([4,5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t364" class="pln"><span class="n"><a href="#t364">364</a></span><span class="t"><span class="str">        >>> c = torch.tensor([6])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t365" class="pln"><span class="n"><a href="#t365">365</a></span><span class="t"><span class="str">        >>> pack_sequence([a, b, c])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t366" class="pln"><span class="n"><a href="#t366">366</a></span><span class="t"><span class="str">        PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t367" class="pln"><span class="n"><a href="#t367">367</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t368" class="pln"><span class="n"><a href="#t368">368</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t369" class="pln"><span class="n"><a href="#t369">369</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t370" class="pln"><span class="n"><a href="#t370">370</a></span><span class="t"><span class="str">        sequences (list[Tensor]): A list of sequences of decreasing length.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t371" class="pln"><span class="n"><a href="#t371">371</a></span><span class="t"><span class="str">        enforce_sorted (bool, optional): if ``True``, checks that the input</span>&nbsp;</span><span class="r"></span></p>
    <p id="t372" class="pln"><span class="n"><a href="#t372">372</a></span><span class="t"><span class="str">            contains sequences sorted by length in a decreasing order. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t373" class="pln"><span class="n"><a href="#t373">373</a></span><span class="t"><span class="str">            ``False``, this condition is not checked. Default: ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t374" class="pln"><span class="n"><a href="#t374">374</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t375" class="pln"><span class="n"><a href="#t375">375</a></span><span class="t"><span class="str">    Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t376" class="pln"><span class="n"><a href="#t376">376</a></span><span class="t"><span class="str">        a :class:`PackedSequence` object</span>&nbsp;</span><span class="r"></span></p>
    <p id="t377" class="pln"><span class="n"><a href="#t377">377</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t378" class="mis show_mis"><span class="n"><a href="#t378">378</a></span><span class="t">    <span class="nam">lengths</span> <span class="op">=</span> <span class="op">[</span><span class="nam">v</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="num">0</span><span class="op">)</span> <span class="key">for</span> <span class="nam">v</span> <span class="key">in</span> <span class="nam">sequences</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t379" class="mis show_mis"><span class="n"><a href="#t379">379</a></span><span class="t">    <span class="key">return</span> <span class="nam">pack_padded_sequence</span><span class="op">(</span><span class="nam">pad_sequence</span><span class="op">(</span><span class="nam">sequences</span><span class="op">)</span><span class="op">,</span> <span class="nam">lengths</span><span class="op">,</span> <span class="nam">enforce_sorted</span><span class="op">=</span><span class="nam">enforce_sorted</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
</div>
<div id="footer">
    <div class="content">
        <p>
            <a class="nav" href="index.html">&#xab; index</a> &nbsp; &nbsp; <a class="nav" href="https://coverage.readthedocs.io">coverage.py v5.0.3</a>,
            created at 2020-03-12 18:08
        </p>
    </div>
</div>
</body>
</html>
