<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=emulateIE7" />
    <title>Coverage for C:\Users\paper\Anaconda3\Lib\site-packages\torch\_torch_docs.py: 100%</title>
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="jquery.min.js"></script>
    <script type="text/javascript" src="jquery.hotkeys.js"></script>
    <script type="text/javascript" src="jquery.isonscreen.js"></script>
    <script type="text/javascript" src="coverage_html.js"></script>
    <script type="text/javascript">
        jQuery(document).ready(coverage.pyfile_ready);
    </script>
</head>
<body class="pyfile">
<div id="header">
    <div class="content">
        <h1>Coverage for <b>C:\Users\paper\Anaconda3\Lib\site-packages\torch\_torch_docs.py</b> :
            <span class="pc_cov">100%</span>
        </h1>
        <img id="keyboard_icon" src="keybd_closed.png" alt="Show keyboard shortcuts" />
        <h2 class="stats">
            236 statements &nbsp;
            <span class="run shortkey_r button_toggle_run">236 run</span>
            <span class="mis show_mis shortkey_m button_toggle_mis">0 missing</span>
            <span class="exc show_exc shortkey_x button_toggle_exc">0 excluded</span>
        </h2>
    </div>
</div>
<div class="help_panel">
    <img id="panel_icon" src="keybd_open.png" alt="Hide keyboard shortcuts" />
    <p class="legend">Hot-keys on this page</p>
    <div>
    <p class="keyhelp">
        <span class="key">r</span>
        <span class="key">m</span>
        <span class="key">x</span>
        <span class="key">p</span> &nbsp; toggle line displays
    </p>
    <p class="keyhelp">
        <span class="key">j</span>
        <span class="key">k</span> &nbsp; next/prev highlighted chunk
    </p>
    <p class="keyhelp">
        <span class="key">0</span> &nbsp; (zero) top of page
    </p>
    <p class="keyhelp">
        <span class="key">1</span> &nbsp; (one) first highlighted chunk
    </p>
    </div>
</div>
<div id="source">
    <p id="t1" class="pln"><span class="n"><a href="#t1">1</a></span><span class="t"><span class="str">"""Adds docstrings to functions defined in the torch._C"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2" class="pln"><span class="n"><a href="#t2">2</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3" class="run"><span class="n"><a href="#t3">3</a></span><span class="t"><span class="key">import</span> <span class="nam">re</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4" class="pln"><span class="n"><a href="#t4">4</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5" class="run"><span class="n"><a href="#t5">5</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6" class="run"><span class="n"><a href="#t6">6</a></span><span class="t"><span class="key">from</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span> <span class="key">import</span> <span class="nam">_add_docstr</span> <span class="key">as</span> <span class="nam">add_docstr</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7" class="pln"><span class="n"><a href="#t7">7</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t8" class="pln"><span class="n"><a href="#t8">8</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t9" class="run"><span class="n"><a href="#t9">9</a></span><span class="t"><span class="key">def</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="nam">desc</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t10" class="pln"><span class="n"><a href="#t10">10</a></span><span class="t">    <span class="str">"""Maps a description of args to a dictionary of {argname: description}.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t11" class="pln"><span class="n"><a href="#t11">11</a></span><span class="t"><span class="str">    Input:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t12" class="pln"><span class="n"><a href="#t12">12</a></span><span class="t"><span class="str">        ('    weight (Tensor): a weight tensor\n' +</span>&nbsp;</span><span class="r"></span></p>
    <p id="t13" class="pln"><span class="n"><a href="#t13">13</a></span><span class="t"><span class="str">         '        Some optional description')</span>&nbsp;</span><span class="r"></span></p>
    <p id="t14" class="pln"><span class="n"><a href="#t14">14</a></span><span class="t"><span class="str">    Output: {</span>&nbsp;</span><span class="r"></span></p>
    <p id="t15" class="pln"><span class="n"><a href="#t15">15</a></span><span class="t"><span class="str">        'weight': \</span>&nbsp;</span><span class="r"></span></p>
    <p id="t16" class="pln"><span class="n"><a href="#t16">16</a></span><span class="t"><span class="str">        'weight (Tensor): a weight tensor\n        Some optional description'</span>&nbsp;</span><span class="r"></span></p>
    <p id="t17" class="pln"><span class="n"><a href="#t17">17</a></span><span class="t"><span class="str">    }</span>&nbsp;</span><span class="r"></span></p>
    <p id="t18" class="pln"><span class="n"><a href="#t18">18</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t19" class="pln"><span class="n"><a href="#t19">19</a></span><span class="t">    <span class="com"># Split on exactly 4 spaces after a newline</span>&nbsp;</span><span class="r"></span></p>
    <p id="t20" class="run"><span class="n"><a href="#t20">20</a></span><span class="t">    <span class="nam">regx</span> <span class="op">=</span> <span class="nam">re</span><span class="op">.</span><span class="nam">compile</span><span class="op">(</span><span class="str">r"\n\s{4}(?!\s)"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t21" class="run"><span class="n"><a href="#t21">21</a></span><span class="t">    <span class="nam">kwargs</span> <span class="op">=</span> <span class="op">[</span><span class="nam">section</span><span class="op">.</span><span class="nam">strip</span><span class="op">(</span><span class="op">)</span> <span class="key">for</span> <span class="nam">section</span> <span class="key">in</span> <span class="nam">regx</span><span class="op">.</span><span class="nam">split</span><span class="op">(</span><span class="nam">desc</span><span class="op">)</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t22" class="run"><span class="n"><a href="#t22">22</a></span><span class="t">    <span class="nam">kwargs</span> <span class="op">=</span> <span class="op">[</span><span class="nam">section</span> <span class="key">for</span> <span class="nam">section</span> <span class="key">in</span> <span class="nam">kwargs</span> <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">section</span><span class="op">)</span> <span class="op">></span> <span class="num">0</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t23" class="run"><span class="n"><a href="#t23">23</a></span><span class="t">    <span class="key">return</span> <span class="op">{</span><span class="nam">desc</span><span class="op">.</span><span class="nam">split</span><span class="op">(</span><span class="str">' '</span><span class="op">)</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">:</span> <span class="nam">desc</span> <span class="key">for</span> <span class="nam">desc</span> <span class="key">in</span> <span class="nam">kwargs</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t24" class="pln"><span class="n"><a href="#t24">24</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t25" class="pln"><span class="n"><a href="#t25">25</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t26" class="run"><span class="n"><a href="#t26">26</a></span><span class="t"><span class="key">def</span> <span class="nam">merge_dicts</span><span class="op">(</span><span class="op">*</span><span class="nam">dicts</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t27" class="run"><span class="n"><a href="#t27">27</a></span><span class="t">    <span class="key">return</span> <span class="op">{</span><span class="nam">x</span><span class="op">:</span> <span class="nam">d</span><span class="op">[</span><span class="nam">x</span><span class="op">]</span> <span class="key">for</span> <span class="nam">d</span> <span class="key">in</span> <span class="nam">dicts</span> <span class="key">for</span> <span class="nam">x</span> <span class="key">in</span> <span class="nam">d</span><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t28" class="pln"><span class="n"><a href="#t28">28</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t29" class="pln"><span class="n"><a href="#t29">29</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t30" class="run"><span class="n"><a href="#t30">30</a></span><span class="t"><span class="nam">common_args</span> <span class="op">=</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t31" class="pln"><span class="n"><a href="#t31">31</a></span><span class="t"><span class="str">    input (Tensor): the input tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t32" class="pln"><span class="n"><a href="#t32">32</a></span><span class="t"><span class="str">    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling</span>&nbsp;</span><span class="r"></span></p>
    <p id="t33" class="pln"><span class="n"><a href="#t33">33</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t34" class="pln"><span class="n"><a href="#t34">34</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t35" class="pln"><span class="n"><a href="#t35">35</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t36" class="run"><span class="n"><a href="#t36">36</a></span><span class="t"><span class="nam">reduceops_common_args</span> <span class="op">=</span> <span class="nam">merge_dicts</span><span class="op">(</span><span class="nam">common_args</span><span class="op">,</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t37" class="pln"><span class="n"><a href="#t37">37</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t38" class="pln"><span class="n"><a href="#t38">38</a></span><span class="t"><span class="str">        If specified, the input tensor is casted to :attr:`dtype` before the operation</span>&nbsp;</span><span class="r"></span></p>
    <p id="t39" class="pln"><span class="n"><a href="#t39">39</a></span><span class="t"><span class="str">        is performed. This is useful for preventing data type overflows. Default: None.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t40" class="pln"><span class="n"><a href="#t40">40</a></span><span class="t"><span class="str">    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t41" class="pln"><span class="n"><a href="#t41">41</a></span><span class="t"><span class="str">"""</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t42" class="pln"><span class="n"><a href="#t42">42</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t43" class="run"><span class="n"><a href="#t43">43</a></span><span class="t"><span class="nam">multi_dim_common</span> <span class="op">=</span> <span class="nam">merge_dicts</span><span class="op">(</span><span class="nam">reduceops_common_args</span><span class="op">,</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t44" class="pln"><span class="n"><a href="#t44">44</a></span><span class="t"><span class="str">    dim (int or tuple of ints): the dimension or dimensions to reduce.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t45" class="pln"><span class="n"><a href="#t45">45</a></span><span class="t"><span class="str">"""</span><span class="op">)</span><span class="op">,</span> <span class="op">{</span><span class="str">'keepdim_details'</span><span class="op">:</span> <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t46" class="pln"><span class="n"><a href="#t46">46</a></span><span class="t"><span class="str">If :attr:`keepdim` is ``True``, the output tensor is of the same size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t47" class="pln"><span class="n"><a href="#t47">47</a></span><span class="t"><span class="str">as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t48" class="pln"><span class="n"><a href="#t48">48</a></span><span class="t"><span class="str">Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t49" class="pln"><span class="n"><a href="#t49">49</a></span><span class="t"><span class="str">output tensor having 1 (or ``len(dim)``) fewer dimension(s).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t50" class="pln"><span class="n"><a href="#t50">50</a></span><span class="t"><span class="str">"""</span><span class="op">}</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t51" class="pln"><span class="n"><a href="#t51">51</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t52" class="run"><span class="n"><a href="#t52">52</a></span><span class="t"><span class="nam">single_dim_common</span> <span class="op">=</span> <span class="nam">merge_dicts</span><span class="op">(</span><span class="nam">reduceops_common_args</span><span class="op">,</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t53" class="pln"><span class="n"><a href="#t53">53</a></span><span class="t"><span class="str">    dim (int): the dimension to reduce.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t54" class="pln"><span class="n"><a href="#t54">54</a></span><span class="t"><span class="str">"""</span><span class="op">)</span><span class="op">,</span> <span class="op">{</span><span class="str">'keepdim_details'</span><span class="op">:</span> <span class="str">"""If :attr:`keepdim` is ``True``, the output tensor is of the same size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t55" class="pln"><span class="n"><a href="#t55">55</a></span><span class="t"><span class="str">as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t56" class="pln"><span class="n"><a href="#t56">56</a></span><span class="t"><span class="str">Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t57" class="pln"><span class="n"><a href="#t57">57</a></span><span class="t"><span class="str">the output tensor having 1 fewer dimension than :attr:`input`."""</span><span class="op">}</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t58" class="pln"><span class="n"><a href="#t58">58</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t59" class="pln"><span class="n"><a href="#t59">59</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t60" class="run"><span class="n"><a href="#t60">60</a></span><span class="t"><span class="nam">factory_common_args</span> <span class="op">=</span> <span class="nam">merge_dicts</span><span class="op">(</span><span class="nam">common_args</span><span class="op">,</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t61" class="pln"><span class="n"><a href="#t61">61</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t62" class="pln"><span class="n"><a href="#t62">62</a></span><span class="t"><span class="str">        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t63" class="pln"><span class="n"><a href="#t63">63</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t64" class="pln"><span class="n"><a href="#t64">64</a></span><span class="t"><span class="str">        Default: ``torch.strided``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t65" class="pln"><span class="n"><a href="#t65">65</a></span><span class="t"><span class="str">    device (:class:`torch.device`, optional): the desired device of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t66" class="pln"><span class="n"><a href="#t66">66</a></span><span class="t"><span class="str">        Default: if ``None``, uses the current device for the default tensor type</span>&nbsp;</span><span class="r"></span></p>
    <p id="t67" class="pln"><span class="n"><a href="#t67">67</a></span><span class="t"><span class="str">        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU</span>&nbsp;</span><span class="r"></span></p>
    <p id="t68" class="pln"><span class="n"><a href="#t68">68</a></span><span class="t"><span class="str">        for CPU tensor types and the current CUDA device for CUDA tensor types.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t69" class="pln"><span class="n"><a href="#t69">69</a></span><span class="t"><span class="str">    requires_grad (bool, optional): If autograd should record operations on the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t70" class="pln"><span class="n"><a href="#t70">70</a></span><span class="t"><span class="str">        returned tensor. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t71" class="pln"><span class="n"><a href="#t71">71</a></span><span class="t"><span class="str">    pin_memory (bool, optional): If set, returned tensor would be allocated in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t72" class="pln"><span class="n"><a href="#t72">72</a></span><span class="t"><span class="str">        the pinned memory. Works only for CPU tensors. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t73" class="pln"><span class="n"><a href="#t73">73</a></span><span class="t"><span class="str">"""</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t74" class="pln"><span class="n"><a href="#t74">74</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t75" class="run"><span class="n"><a href="#t75">75</a></span><span class="t"><span class="nam">factory_like_common_args</span> <span class="op">=</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t76" class="pln"><span class="n"><a href="#t76">76</a></span><span class="t"><span class="str">    input (Tensor): the size of :attr:`input` will determine size of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t77" class="pln"><span class="n"><a href="#t77">77</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t78" class="pln"><span class="n"><a href="#t78">78</a></span><span class="t"><span class="str">        Default: if ``None``, defaults to the layout of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t79" class="pln"><span class="n"><a href="#t79">79</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t80" class="pln"><span class="n"><a href="#t80">80</a></span><span class="t"><span class="str">        Default: if ``None``, defaults to the dtype of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t81" class="pln"><span class="n"><a href="#t81">81</a></span><span class="t"><span class="str">    device (:class:`torch.device`, optional): the desired device of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t82" class="pln"><span class="n"><a href="#t82">82</a></span><span class="t"><span class="str">        Default: if ``None``, defaults to the device of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t83" class="pln"><span class="n"><a href="#t83">83</a></span><span class="t"><span class="str">    requires_grad (bool, optional): If autograd should record operations on the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t84" class="pln"><span class="n"><a href="#t84">84</a></span><span class="t"><span class="str">        returned tensor. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t85" class="pln"><span class="n"><a href="#t85">85</a></span><span class="t"><span class="str">    pin_memory (bool, optional): If set, returned tensor would be allocated in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t86" class="pln"><span class="n"><a href="#t86">86</a></span><span class="t"><span class="str">        the pinned memory. Works only for CPU tensors. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t87" class="pln"><span class="n"><a href="#t87">87</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t88" class="pln"><span class="n"><a href="#t88">88</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t89" class="run"><span class="n"><a href="#t89">89</a></span><span class="t"><span class="nam">factory_data_common_args</span> <span class="op">=</span> <span class="nam">parse_kwargs</span><span class="op">(</span><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t90" class="pln"><span class="n"><a href="#t90">90</a></span><span class="t"><span class="str">    data (array_like): Initial data for the tensor. Can be a list, tuple,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t91" class="pln"><span class="n"><a href="#t91">91</a></span><span class="t"><span class="str">        NumPy ``ndarray``, scalar, and other types.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t92" class="pln"><span class="n"><a href="#t92">92</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t93" class="pln"><span class="n"><a href="#t93">93</a></span><span class="t"><span class="str">        Default: if ``None``, infers data type from :attr:`data`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t94" class="pln"><span class="n"><a href="#t94">94</a></span><span class="t"><span class="str">    device (:class:`torch.device`, optional): the desired device of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t95" class="pln"><span class="n"><a href="#t95">95</a></span><span class="t"><span class="str">        Default: if ``None``, uses the current device for the default tensor type</span>&nbsp;</span><span class="r"></span></p>
    <p id="t96" class="pln"><span class="n"><a href="#t96">96</a></span><span class="t"><span class="str">        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU</span>&nbsp;</span><span class="r"></span></p>
    <p id="t97" class="pln"><span class="n"><a href="#t97">97</a></span><span class="t"><span class="str">        for CPU tensor types and the current CUDA device for CUDA tensor types.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t98" class="pln"><span class="n"><a href="#t98">98</a></span><span class="t"><span class="str">    requires_grad (bool, optional): If autograd should record operations on the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t99" class="pln"><span class="n"><a href="#t99">99</a></span><span class="t"><span class="str">        returned tensor. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t100" class="pln"><span class="n"><a href="#t100">100</a></span><span class="t"><span class="str">    pin_memory (bool, optional): If set, returned tensor would be allocated in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t101" class="pln"><span class="n"><a href="#t101">101</a></span><span class="t"><span class="str">        the pinned memory. Works only for CPU tensors. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t102" class="pln"><span class="n"><a href="#t102">102</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t103" class="pln"><span class="n"><a href="#t103">103</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t104" class="run"><span class="n"><a href="#t104">104</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">abs</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t105" class="pln"><span class="n"><a href="#t105">105</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t106" class="pln"><span class="n"><a href="#t106">106</a></span><span class="t"><span class="str">abs(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t107" class="pln"><span class="n"><a href="#t107">107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t108" class="pln"><span class="n"><a href="#t108">108</a></span><span class="t"><span class="str">Computes the element-wise absolute value of the given :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t109" class="pln"><span class="n"><a href="#t109">109</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t110" class="pln"><span class="n"><a href="#t110">110</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t111" class="pln"><span class="n"><a href="#t111">111</a></span><span class="t"><span class="str">    \text{out}_{i} = |\text{input}_{i}|</span>&nbsp;</span><span class="r"></span></p>
    <p id="t112" class="pln"><span class="n"><a href="#t112">112</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t113" class="pln"><span class="n"><a href="#t113">113</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t114" class="pln"><span class="n"><a href="#t114">114</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t115" class="pln"><span class="n"><a href="#t115">115</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t116" class="pln"><span class="n"><a href="#t116">116</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t117" class="pln"><span class="n"><a href="#t117">117</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t118" class="pln"><span class="n"><a href="#t118">118</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t119" class="pln"><span class="n"><a href="#t119">119</a></span><span class="t"><span class="str">    >>> torch.abs(torch.tensor([-1, -2, 3]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t120" class="pln"><span class="n"><a href="#t120">120</a></span><span class="t"><span class="str">    tensor([ 1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t121" class="pln"><span class="n"><a href="#t121">121</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t122" class="pln"><span class="n"><a href="#t122">122</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t123" class="run"><span class="n"><a href="#t123">123</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">acos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t124" class="pln"><span class="n"><a href="#t124">124</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t125" class="pln"><span class="n"><a href="#t125">125</a></span><span class="t"><span class="str">acos(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t126" class="pln"><span class="n"><a href="#t126">126</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t127" class="pln"><span class="n"><a href="#t127">127</a></span><span class="t"><span class="str">Returns a new tensor with the arccosine  of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t128" class="pln"><span class="n"><a href="#t128">128</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t129" class="pln"><span class="n"><a href="#t129">129</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t130" class="pln"><span class="n"><a href="#t130">130</a></span><span class="t"><span class="str">    \text{out}_{i} = \cos^{-1}(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t131" class="pln"><span class="n"><a href="#t131">131</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t132" class="pln"><span class="n"><a href="#t132">132</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t133" class="pln"><span class="n"><a href="#t133">133</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t134" class="pln"><span class="n"><a href="#t134">134</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t135" class="pln"><span class="n"><a href="#t135">135</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t136" class="pln"><span class="n"><a href="#t136">136</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t137" class="pln"><span class="n"><a href="#t137">137</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t138" class="pln"><span class="n"><a href="#t138">138</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t139" class="pln"><span class="n"><a href="#t139">139</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t140" class="pln"><span class="n"><a href="#t140">140</a></span><span class="t"><span class="str">    tensor([ 0.3348, -0.5889,  0.2005, -0.1584])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t141" class="pln"><span class="n"><a href="#t141">141</a></span><span class="t"><span class="str">    >>> torch.acos(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t142" class="pln"><span class="n"><a href="#t142">142</a></span><span class="t"><span class="str">    tensor([ 1.2294,  2.2004,  1.3690,  1.7298])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t143" class="pln"><span class="n"><a href="#t143">143</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t144" class="pln"><span class="n"><a href="#t144">144</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t145" class="run"><span class="n"><a href="#t145">145</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">add</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t146" class="pln"><span class="n"><a href="#t146">146</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t147" class="pln"><span class="n"><a href="#t147">147</a></span><span class="t"><span class="str">.. function:: add(input, other, out=None)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t148" class="pln"><span class="n"><a href="#t148">148</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t149" class="pln"><span class="n"><a href="#t149">149</a></span><span class="t"><span class="str">Adds the scalar :attr:`other` to each element of the input :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t150" class="pln"><span class="n"><a href="#t150">150</a></span><span class="t"><span class="str">and returns a new resulting tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t151" class="pln"><span class="n"><a href="#t151">151</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t152" class="pln"><span class="n"><a href="#t152">152</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t153" class="pln"><span class="n"><a href="#t153">153</a></span><span class="t"><span class="str">    \text{{out}} = \text{{input}} + \text{{other}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t154" class="pln"><span class="n"><a href="#t154">154</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t155" class="pln"><span class="n"><a href="#t155">155</a></span><span class="t"><span class="str">If :attr:`input` is of type FloatTensor or DoubleTensor, :attr:`other` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t156" class="pln"><span class="n"><a href="#t156">156</a></span><span class="t"><span class="str">a real number, otherwise it should be an integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t157" class="pln"><span class="n"><a href="#t157">157</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t158" class="pln"><span class="n"><a href="#t158">158</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t159" class="pln"><span class="n"><a href="#t159">159</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t160" class="pln"><span class="n"><a href="#t160">160</a></span><span class="t"><span class="str">    value (Number): the number to be added to each element of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t161" class="pln"><span class="n"><a href="#t161">161</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t162" class="pln"><span class="n"><a href="#t162">162</a></span><span class="t"><span class="str">Keyword arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t163" class="pln"><span class="n"><a href="#t163">163</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t164" class="pln"><span class="n"><a href="#t164">164</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t165" class="pln"><span class="n"><a href="#t165">165</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t166" class="pln"><span class="n"><a href="#t166">166</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t167" class="pln"><span class="n"><a href="#t167">167</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t168" class="pln"><span class="n"><a href="#t168">168</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t169" class="pln"><span class="n"><a href="#t169">169</a></span><span class="t"><span class="str">    tensor([ 0.0202,  1.0985,  1.3506, -0.6056])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t170" class="pln"><span class="n"><a href="#t170">170</a></span><span class="t"><span class="str">    >>> torch.add(a, 20)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t171" class="pln"><span class="n"><a href="#t171">171</a></span><span class="t"><span class="str">    tensor([ 20.0202,  21.0985,  21.3506,  19.3944])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t172" class="pln"><span class="n"><a href="#t172">172</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t173" class="pln"><span class="n"><a href="#t173">173</a></span><span class="t"><span class="str">.. function:: add(input, other, *, alpha=1, out=None)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t174" class="pln"><span class="n"><a href="#t174">174</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t175" class="pln"><span class="n"><a href="#t175">175</a></span><span class="t"><span class="str">Each element of the tensor :attr:`other` is multiplied by the scalar</span>&nbsp;</span><span class="r"></span></p>
    <p id="t176" class="pln"><span class="n"><a href="#t176">176</a></span><span class="t"><span class="str">:attr:`alpha` and added to each element of the tensor :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t177" class="pln"><span class="n"><a href="#t177">177</a></span><span class="t"><span class="str">The resulting tensor is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t178" class="pln"><span class="n"><a href="#t178">178</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t179" class="pln"><span class="n"><a href="#t179">179</a></span><span class="t"><span class="str">The shapes of :attr:`input` and :attr:`other` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t180" class="pln"><span class="n"><a href="#t180">180</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t181" class="pln"><span class="n"><a href="#t181">181</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t182" class="pln"><span class="n"><a href="#t182">182</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t183" class="pln"><span class="n"><a href="#t183">183</a></span><span class="t"><span class="str">    \text{{out}} = \text{{input}} + \text{{alpha}} \times \text{{other}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t184" class="pln"><span class="n"><a href="#t184">184</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t185" class="pln"><span class="n"><a href="#t185">185</a></span><span class="t"><span class="str">If :attr:`other` is of type FloatTensor or DoubleTensor, :attr:`alpha` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t186" class="pln"><span class="n"><a href="#t186">186</a></span><span class="t"><span class="str">a real number, otherwise it should be an integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t187" class="pln"><span class="n"><a href="#t187">187</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t188" class="pln"><span class="n"><a href="#t188">188</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t189" class="pln"><span class="n"><a href="#t189">189</a></span><span class="t"><span class="str">    input (Tensor): the first input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t190" class="pln"><span class="n"><a href="#t190">190</a></span><span class="t"><span class="str">    other (Tensor): the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t191" class="pln"><span class="n"><a href="#t191">191</a></span><span class="t"><span class="str">    alpha (Number): the scalar multiplier for :attr:`other`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t192" class="pln"><span class="n"><a href="#t192">192</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t193" class="pln"><span class="n"><a href="#t193">193</a></span><span class="t"><span class="str">Keyword arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t194" class="pln"><span class="n"><a href="#t194">194</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t195" class="pln"><span class="n"><a href="#t195">195</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t196" class="pln"><span class="n"><a href="#t196">196</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t197" class="pln"><span class="n"><a href="#t197">197</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t198" class="pln"><span class="n"><a href="#t198">198</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t199" class="pln"><span class="n"><a href="#t199">199</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t200" class="pln"><span class="n"><a href="#t200">200</a></span><span class="t"><span class="str">    tensor([-0.9732, -0.3497,  0.6245,  0.4022])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t201" class="pln"><span class="n"><a href="#t201">201</a></span><span class="t"><span class="str">    >>> b = torch.randn(4, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t202" class="pln"><span class="n"><a href="#t202">202</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t203" class="pln"><span class="n"><a href="#t203">203</a></span><span class="t"><span class="str">    tensor([[ 0.3743],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t204" class="pln"><span class="n"><a href="#t204">204</a></span><span class="t"><span class="str">            [-1.7724],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t205" class="pln"><span class="n"><a href="#t205">205</a></span><span class="t"><span class="str">            [-0.5811],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t206" class="pln"><span class="n"><a href="#t206">206</a></span><span class="t"><span class="str">            [-0.8017]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t207" class="pln"><span class="n"><a href="#t207">207</a></span><span class="t"><span class="str">    >>> torch.add(a, 10, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t208" class="pln"><span class="n"><a href="#t208">208</a></span><span class="t"><span class="str">    tensor([[  2.7695,   3.3930,   4.3672,   4.1450],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t209" class="pln"><span class="n"><a href="#t209">209</a></span><span class="t"><span class="str">            [-18.6971, -18.0736, -17.0994, -17.3216],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t210" class="pln"><span class="n"><a href="#t210">210</a></span><span class="t"><span class="str">            [ -6.7845,  -6.1610,  -5.1868,  -5.4090],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t211" class="pln"><span class="n"><a href="#t211">211</a></span><span class="t"><span class="str">            [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t212" class="pln"><span class="n"><a href="#t212">212</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t213" class="pln"><span class="n"><a href="#t213">213</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t214" class="run"><span class="n"><a href="#t214">214</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">addbmm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t215" class="pln"><span class="n"><a href="#t215">215</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t216" class="pln"><span class="n"><a href="#t216">216</a></span><span class="t"><span class="str">addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t217" class="pln"><span class="n"><a href="#t217">217</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t218" class="pln"><span class="n"><a href="#t218">218</a></span><span class="t"><span class="str">Performs a batch matrix-matrix product of matrices stored</span>&nbsp;</span><span class="r"></span></p>
    <p id="t219" class="pln"><span class="n"><a href="#t219">219</a></span><span class="t"><span class="str">in :attr:`batch1` and :attr:`batch2`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t220" class="pln"><span class="n"><a href="#t220">220</a></span><span class="t"><span class="str">with a reduced add step (all matrix multiplications get accumulated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t221" class="pln"><span class="n"><a href="#t221">221</a></span><span class="t"><span class="str">along the first dimension).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t222" class="pln"><span class="n"><a href="#t222">222</a></span><span class="t"><span class="str">:attr:`input` is added to the final result.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t223" class="pln"><span class="n"><a href="#t223">223</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t224" class="pln"><span class="n"><a href="#t224">224</a></span><span class="t"><span class="str">:attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t225" class="pln"><span class="n"><a href="#t225">225</a></span><span class="t"><span class="str">same number of matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t226" class="pln"><span class="n"><a href="#t226">226</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t227" class="pln"><span class="n"><a href="#t227">227</a></span><span class="t"><span class="str">If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t228" class="pln"><span class="n"><a href="#t228">228</a></span><span class="t"><span class="str">:math:`(b \times m \times p)` tensor, :attr:`input` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t229" class="pln"><span class="n"><a href="#t229">229</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with a :math:`(n \times p)` tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t230" class="pln"><span class="n"><a href="#t230">230</a></span><span class="t"><span class="str">and :attr:`out` will be a :math:`(n \times p)` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t231" class="pln"><span class="n"><a href="#t231">231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t232" class="pln"><span class="n"><a href="#t232">232</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t233" class="pln"><span class="n"><a href="#t233">233</a></span><span class="t"><span class="str">    out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t234" class="pln"><span class="n"><a href="#t234">234</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t235" class="pln"><span class="n"><a href="#t235">235</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t236" class="pln"><span class="n"><a href="#t236">236</a></span><span class="t"><span class="str">must be real numbers, otherwise they should be integers.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t237" class="pln"><span class="n"><a href="#t237">237</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t238" class="pln"><span class="n"><a href="#t238">238</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t239" class="pln"><span class="n"><a href="#t239">239</a></span><span class="t"><span class="str">    batch1 (Tensor): the first batch of matrices to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t240" class="pln"><span class="n"><a href="#t240">240</a></span><span class="t"><span class="str">    batch2 (Tensor): the second batch of matrices to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t241" class="pln"><span class="n"><a href="#t241">241</a></span><span class="t"><span class="str">    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t242" class="pln"><span class="n"><a href="#t242">242</a></span><span class="t"><span class="str">    input (Tensor): matrix to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t243" class="pln"><span class="n"><a href="#t243">243</a></span><span class="t"><span class="str">    alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t244" class="pln"><span class="n"><a href="#t244">244</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t245" class="pln"><span class="n"><a href="#t245">245</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t246" class="pln"><span class="n"><a href="#t246">246</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t247" class="pln"><span class="n"><a href="#t247">247</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t248" class="pln"><span class="n"><a href="#t248">248</a></span><span class="t"><span class="str">    >>> M = torch.randn(3, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t249" class="pln"><span class="n"><a href="#t249">249</a></span><span class="t"><span class="str">    >>> batch1 = torch.randn(10, 3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t250" class="pln"><span class="n"><a href="#t250">250</a></span><span class="t"><span class="str">    >>> batch2 = torch.randn(10, 4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t251" class="pln"><span class="n"><a href="#t251">251</a></span><span class="t"><span class="str">    >>> torch.addbmm(M, batch1, batch2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t252" class="pln"><span class="n"><a href="#t252">252</a></span><span class="t"><span class="str">    tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t253" class="pln"><span class="n"><a href="#t253">253</a></span><span class="t"><span class="str">            [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t254" class="pln"><span class="n"><a href="#t254">254</a></span><span class="t"><span class="str">            [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t255" class="pln"><span class="n"><a href="#t255">255</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t256" class="pln"><span class="n"><a href="#t256">256</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t257" class="run"><span class="n"><a href="#t257">257</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">addcdiv</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t258" class="pln"><span class="n"><a href="#t258">258</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t259" class="pln"><span class="n"><a href="#t259">259</a></span><span class="t"><span class="str">addcdiv(input, tensor1, tensor2, *, value=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t260" class="pln"><span class="n"><a href="#t260">260</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t261" class="pln"><span class="n"><a href="#t261">261</a></span><span class="t"><span class="str">Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t262" class="pln"><span class="n"><a href="#t262">262</a></span><span class="t"><span class="str">multiply the result by the scalar :attr:`value` and add it to :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t263" class="pln"><span class="n"><a href="#t263">263</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t264" class="pln"><span class="n"><a href="#t264">264</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t265" class="pln"><span class="n"><a href="#t265">265</a></span><span class="t"><span class="str">    \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t266" class="pln"><span class="n"><a href="#t266">266</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t267" class="pln"><span class="n"><a href="#t267">267</a></span><span class="t"><span class="str">The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t268" class="pln"><span class="n"><a href="#t268">268</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t269" class="pln"><span class="n"><a href="#t269">269</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t270" class="pln"><span class="n"><a href="#t270">270</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t271" class="pln"><span class="n"><a href="#t271">271</a></span><span class="t"><span class="str">a real number, otherwise an integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t272" class="pln"><span class="n"><a href="#t272">272</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t273" class="pln"><span class="n"><a href="#t273">273</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t274" class="pln"><span class="n"><a href="#t274">274</a></span><span class="t"><span class="str">    input (Tensor): the tensor to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t275" class="pln"><span class="n"><a href="#t275">275</a></span><span class="t"><span class="str">    tensor1 (Tensor): the numerator tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t276" class="pln"><span class="n"><a href="#t276">276</a></span><span class="t"><span class="str">    tensor2 (Tensor): the denominator tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t277" class="pln"><span class="n"><a href="#t277">277</a></span><span class="t"><span class="str">    value (Number, optional): multiplier for :math:`\text{{tensor1}} / \text{{tensor2}}`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t278" class="pln"><span class="n"><a href="#t278">278</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t279" class="pln"><span class="n"><a href="#t279">279</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t280" class="pln"><span class="n"><a href="#t280">280</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t281" class="pln"><span class="n"><a href="#t281">281</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t282" class="pln"><span class="n"><a href="#t282">282</a></span><span class="t"><span class="str">    >>> t = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t283" class="pln"><span class="n"><a href="#t283">283</a></span><span class="t"><span class="str">    >>> t1 = torch.randn(3, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t284" class="pln"><span class="n"><a href="#t284">284</a></span><span class="t"><span class="str">    >>> t2 = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t285" class="pln"><span class="n"><a href="#t285">285</a></span><span class="t"><span class="str">    >>> torch.addcdiv(t, t1, t2, value=0.1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t286" class="pln"><span class="n"><a href="#t286">286</a></span><span class="t"><span class="str">    tensor([[-0.2312, -3.6496,  0.1312],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t287" class="pln"><span class="n"><a href="#t287">287</a></span><span class="t"><span class="str">            [-1.0428,  3.4292, -0.1030],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t288" class="pln"><span class="n"><a href="#t288">288</a></span><span class="t"><span class="str">            [-0.5369, -0.9829,  0.0430]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t289" class="pln"><span class="n"><a href="#t289">289</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t290" class="pln"><span class="n"><a href="#t290">290</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t291" class="run"><span class="n"><a href="#t291">291</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">addcmul</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t292" class="pln"><span class="n"><a href="#t292">292</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t293" class="pln"><span class="n"><a href="#t293">293</a></span><span class="t"><span class="str">addcmul(input, tensor1, tensor2, *, value=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t294" class="pln"><span class="n"><a href="#t294">294</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t295" class="pln"><span class="n"><a href="#t295">295</a></span><span class="t"><span class="str">Performs the element-wise multiplication of :attr:`tensor1`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t296" class="pln"><span class="n"><a href="#t296">296</a></span><span class="t"><span class="str">by :attr:`tensor2`, multiply the result by the scalar :attr:`value`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t297" class="pln"><span class="n"><a href="#t297">297</a></span><span class="t"><span class="str">and add it to :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t298" class="pln"><span class="n"><a href="#t298">298</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t299" class="pln"><span class="n"><a href="#t299">299</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t300" class="pln"><span class="n"><a href="#t300">300</a></span><span class="t"><span class="str">    \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i</span>&nbsp;</span><span class="r"></span></p>
    <p id="t301" class="pln"><span class="n"><a href="#t301">301</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t302" class="pln"><span class="n"><a href="#t302">302</a></span><span class="t"><span class="str">The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t303" class="pln"><span class="n"><a href="#t303">303</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t304" class="pln"><span class="n"><a href="#t304">304</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t305" class="pln"><span class="n"><a href="#t305">305</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t306" class="pln"><span class="n"><a href="#t306">306</a></span><span class="t"><span class="str">a real number, otherwise an integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t307" class="pln"><span class="n"><a href="#t307">307</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t308" class="pln"><span class="n"><a href="#t308">308</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t309" class="pln"><span class="n"><a href="#t309">309</a></span><span class="t"><span class="str">    input (Tensor): the tensor to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t310" class="pln"><span class="n"><a href="#t310">310</a></span><span class="t"><span class="str">    tensor1 (Tensor): the tensor to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t311" class="pln"><span class="n"><a href="#t311">311</a></span><span class="t"><span class="str">    tensor2 (Tensor): the tensor to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t312" class="pln"><span class="n"><a href="#t312">312</a></span><span class="t"><span class="str">    value (Number, optional): multiplier for :math:`tensor1 .* tensor2`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t313" class="pln"><span class="n"><a href="#t313">313</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t314" class="pln"><span class="n"><a href="#t314">314</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t315" class="pln"><span class="n"><a href="#t315">315</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t316" class="pln"><span class="n"><a href="#t316">316</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t317" class="pln"><span class="n"><a href="#t317">317</a></span><span class="t"><span class="str">    >>> t = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t318" class="pln"><span class="n"><a href="#t318">318</a></span><span class="t"><span class="str">    >>> t1 = torch.randn(3, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t319" class="pln"><span class="n"><a href="#t319">319</a></span><span class="t"><span class="str">    >>> t2 = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t320" class="pln"><span class="n"><a href="#t320">320</a></span><span class="t"><span class="str">    >>> torch.addcmul(t, t1, t2, value=0.1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t321" class="pln"><span class="n"><a href="#t321">321</a></span><span class="t"><span class="str">    tensor([[-0.8635, -0.6391,  1.6174],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t322" class="pln"><span class="n"><a href="#t322">322</a></span><span class="t"><span class="str">            [-0.7617, -0.5879,  1.7388],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t323" class="pln"><span class="n"><a href="#t323">323</a></span><span class="t"><span class="str">            [-0.8353, -0.6249,  1.6511]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t324" class="pln"><span class="n"><a href="#t324">324</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t325" class="pln"><span class="n"><a href="#t325">325</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t326" class="run"><span class="n"><a href="#t326">326</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">addmm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t327" class="pln"><span class="n"><a href="#t327">327</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t328" class="pln"><span class="n"><a href="#t328">328</a></span><span class="t"><span class="str">addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t329" class="pln"><span class="n"><a href="#t329">329</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t330" class="pln"><span class="n"><a href="#t330">330</a></span><span class="t"><span class="str">Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t331" class="pln"><span class="n"><a href="#t331">331</a></span><span class="t"><span class="str">The matrix :attr:`input` is added to the final result.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t332" class="pln"><span class="n"><a href="#t332">332</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t333" class="pln"><span class="n"><a href="#t333">333</a></span><span class="t"><span class="str">If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t334" class="pln"><span class="n"><a href="#t334">334</a></span><span class="t"><span class="str">:math:`(m \times p)` tensor, then :attr:`input` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t335" class="pln"><span class="n"><a href="#t335">335</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with a :math:`(n \times p)` tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t336" class="pln"><span class="n"><a href="#t336">336</a></span><span class="t"><span class="str">and :attr:`out` will be a :math:`(n \times p)` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t337" class="pln"><span class="n"><a href="#t337">337</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t338" class="pln"><span class="n"><a href="#t338">338</a></span><span class="t"><span class="str">:attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between</span>&nbsp;</span><span class="r"></span></p>
    <p id="t339" class="pln"><span class="n"><a href="#t339">339</a></span><span class="t"><span class="str">:attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t340" class="pln"><span class="n"><a href="#t340">340</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t341" class="pln"><span class="n"><a href="#t341">341</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t342" class="pln"><span class="n"><a href="#t342">342</a></span><span class="t"><span class="str">    \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t343" class="pln"><span class="n"><a href="#t343">343</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t344" class="pln"><span class="n"><a href="#t344">344</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t345" class="pln"><span class="n"><a href="#t345">345</a></span><span class="t"><span class="str">:attr:`alpha` must be real numbers, otherwise they should be integers.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t346" class="pln"><span class="n"><a href="#t346">346</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t347" class="pln"><span class="n"><a href="#t347">347</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t348" class="pln"><span class="n"><a href="#t348">348</a></span><span class="t"><span class="str">    input (Tensor): matrix to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t349" class="pln"><span class="n"><a href="#t349">349</a></span><span class="t"><span class="str">    mat1 (Tensor): the first matrix to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t350" class="pln"><span class="n"><a href="#t350">350</a></span><span class="t"><span class="str">    mat2 (Tensor): the second matrix to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t351" class="pln"><span class="n"><a href="#t351">351</a></span><span class="t"><span class="str">    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t352" class="pln"><span class="n"><a href="#t352">352</a></span><span class="t"><span class="str">    alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t353" class="pln"><span class="n"><a href="#t353">353</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t354" class="pln"><span class="n"><a href="#t354">354</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t355" class="pln"><span class="n"><a href="#t355">355</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t356" class="pln"><span class="n"><a href="#t356">356</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t357" class="pln"><span class="n"><a href="#t357">357</a></span><span class="t"><span class="str">    >>> M = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t358" class="pln"><span class="n"><a href="#t358">358</a></span><span class="t"><span class="str">    >>> mat1 = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t359" class="pln"><span class="n"><a href="#t359">359</a></span><span class="t"><span class="str">    >>> mat2 = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t360" class="pln"><span class="n"><a href="#t360">360</a></span><span class="t"><span class="str">    >>> torch.addmm(M, mat1, mat2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t361" class="pln"><span class="n"><a href="#t361">361</a></span><span class="t"><span class="str">    tensor([[-4.8716,  1.4671, -1.3746],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t362" class="pln"><span class="n"><a href="#t362">362</a></span><span class="t"><span class="str">            [ 0.7573, -3.9555, -2.8681]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t363" class="pln"><span class="n"><a href="#t363">363</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t364" class="pln"><span class="n"><a href="#t364">364</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t365" class="run"><span class="n"><a href="#t365">365</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">addmv</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t366" class="pln"><span class="n"><a href="#t366">366</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t367" class="pln"><span class="n"><a href="#t367">367</a></span><span class="t"><span class="str">addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t368" class="pln"><span class="n"><a href="#t368">368</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t369" class="pln"><span class="n"><a href="#t369">369</a></span><span class="t"><span class="str">Performs a matrix-vector product of the matrix :attr:`mat` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t370" class="pln"><span class="n"><a href="#t370">370</a></span><span class="t"><span class="str">the vector :attr:`vec`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t371" class="pln"><span class="n"><a href="#t371">371</a></span><span class="t"><span class="str">The vector :attr:`input` is added to the final result.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t372" class="pln"><span class="n"><a href="#t372">372</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t373" class="pln"><span class="n"><a href="#t373">373</a></span><span class="t"><span class="str">If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t374" class="pln"><span class="n"><a href="#t374">374</a></span><span class="t"><span class="str">size `m`, then :attr:`input` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t375" class="pln"><span class="n"><a href="#t375">375</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with a 1-D tensor of size `n` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t376" class="pln"><span class="n"><a href="#t376">376</a></span><span class="t"><span class="str">:attr:`out` will be 1-D tensor of size `n`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t377" class="pln"><span class="n"><a href="#t377">377</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t378" class="pln"><span class="n"><a href="#t378">378</a></span><span class="t"><span class="str">:attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between</span>&nbsp;</span><span class="r"></span></p>
    <p id="t379" class="pln"><span class="n"><a href="#t379">379</a></span><span class="t"><span class="str">:attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t380" class="pln"><span class="n"><a href="#t380">380</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t381" class="pln"><span class="n"><a href="#t381">381</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t382" class="pln"><span class="n"><a href="#t382">382</a></span><span class="t"><span class="str">    \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t383" class="pln"><span class="n"><a href="#t383">383</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t384" class="pln"><span class="n"><a href="#t384">384</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t385" class="pln"><span class="n"><a href="#t385">385</a></span><span class="t"><span class="str">:attr:`alpha` must be real numbers, otherwise they should be integers</span>&nbsp;</span><span class="r"></span></p>
    <p id="t386" class="pln"><span class="n"><a href="#t386">386</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t387" class="pln"><span class="n"><a href="#t387">387</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t388" class="pln"><span class="n"><a href="#t388">388</a></span><span class="t"><span class="str">    input (Tensor): vector to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t389" class="pln"><span class="n"><a href="#t389">389</a></span><span class="t"><span class="str">    mat (Tensor): matrix to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t390" class="pln"><span class="n"><a href="#t390">390</a></span><span class="t"><span class="str">    vec (Tensor): vector to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t391" class="pln"><span class="n"><a href="#t391">391</a></span><span class="t"><span class="str">    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t392" class="pln"><span class="n"><a href="#t392">392</a></span><span class="t"><span class="str">    alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t393" class="pln"><span class="n"><a href="#t393">393</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t394" class="pln"><span class="n"><a href="#t394">394</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t395" class="pln"><span class="n"><a href="#t395">395</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t396" class="pln"><span class="n"><a href="#t396">396</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t397" class="pln"><span class="n"><a href="#t397">397</a></span><span class="t"><span class="str">    >>> M = torch.randn(2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t398" class="pln"><span class="n"><a href="#t398">398</a></span><span class="t"><span class="str">    >>> mat = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t399" class="pln"><span class="n"><a href="#t399">399</a></span><span class="t"><span class="str">    >>> vec = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t400" class="pln"><span class="n"><a href="#t400">400</a></span><span class="t"><span class="str">    >>> torch.addmv(M, mat, vec)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t401" class="pln"><span class="n"><a href="#t401">401</a></span><span class="t"><span class="str">    tensor([-0.3768, -5.5565])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t402" class="pln"><span class="n"><a href="#t402">402</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t403" class="pln"><span class="n"><a href="#t403">403</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t404" class="run"><span class="n"><a href="#t404">404</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">addr</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t405" class="pln"><span class="n"><a href="#t405">405</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t406" class="pln"><span class="n"><a href="#t406">406</a></span><span class="t"><span class="str">addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t407" class="pln"><span class="n"><a href="#t407">407</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t408" class="pln"><span class="n"><a href="#t408">408</a></span><span class="t"><span class="str">Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t409" class="pln"><span class="n"><a href="#t409">409</a></span><span class="t"><span class="str">and adds it to the matrix :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t410" class="pln"><span class="n"><a href="#t410">410</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t411" class="pln"><span class="n"><a href="#t411">411</a></span><span class="t"><span class="str">Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t412" class="pln"><span class="n"><a href="#t412">412</a></span><span class="t"><span class="str">outer product between :attr:`vec1` and :attr:`vec2` and the added matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t413" class="pln"><span class="n"><a href="#t413">413</a></span><span class="t"><span class="str">:attr:`input` respectively.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t414" class="pln"><span class="n"><a href="#t414">414</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t415" class="pln"><span class="n"><a href="#t415">415</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t416" class="pln"><span class="n"><a href="#t416">416</a></span><span class="t"><span class="str">    \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t417" class="pln"><span class="n"><a href="#t417">417</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t418" class="pln"><span class="n"><a href="#t418">418</a></span><span class="t"><span class="str">If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t419" class="pln"><span class="n"><a href="#t419">419</a></span><span class="t"><span class="str">of size `m`, then :attr:`input` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t420" class="pln"><span class="n"><a href="#t420">420</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with a matrix of size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t421" class="pln"><span class="n"><a href="#t421">421</a></span><span class="t"><span class="str">:math:`(n \times m)` and :attr:`out` will be a matrix of size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t422" class="pln"><span class="n"><a href="#t422">422</a></span><span class="t"><span class="str">:math:`(n \times m)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t423" class="pln"><span class="n"><a href="#t423">423</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t424" class="pln"><span class="n"><a href="#t424">424</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t425" class="pln"><span class="n"><a href="#t425">425</a></span><span class="t"><span class="str">:attr:`alpha` must be real numbers, otherwise they should be integers</span>&nbsp;</span><span class="r"></span></p>
    <p id="t426" class="pln"><span class="n"><a href="#t426">426</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t427" class="pln"><span class="n"><a href="#t427">427</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t428" class="pln"><span class="n"><a href="#t428">428</a></span><span class="t"><span class="str">    input (Tensor): matrix to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t429" class="pln"><span class="n"><a href="#t429">429</a></span><span class="t"><span class="str">    vec1 (Tensor): the first vector of the outer product</span>&nbsp;</span><span class="r"></span></p>
    <p id="t430" class="pln"><span class="n"><a href="#t430">430</a></span><span class="t"><span class="str">    vec2 (Tensor): the second vector of the outer product</span>&nbsp;</span><span class="r"></span></p>
    <p id="t431" class="pln"><span class="n"><a href="#t431">431</a></span><span class="t"><span class="str">    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t432" class="pln"><span class="n"><a href="#t432">432</a></span><span class="t"><span class="str">    alpha (Number, optional): multiplier for :math:`\text{{vec1}} \otimes \text{{vec2}}` (:math:`\alpha`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t433" class="pln"><span class="n"><a href="#t433">433</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t434" class="pln"><span class="n"><a href="#t434">434</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t435" class="pln"><span class="n"><a href="#t435">435</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t436" class="pln"><span class="n"><a href="#t436">436</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t437" class="pln"><span class="n"><a href="#t437">437</a></span><span class="t"><span class="str">    >>> vec1 = torch.arange(1., 4.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t438" class="pln"><span class="n"><a href="#t438">438</a></span><span class="t"><span class="str">    >>> vec2 = torch.arange(1., 3.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t439" class="pln"><span class="n"><a href="#t439">439</a></span><span class="t"><span class="str">    >>> M = torch.zeros(3, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t440" class="pln"><span class="n"><a href="#t440">440</a></span><span class="t"><span class="str">    >>> torch.addr(M, vec1, vec2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t441" class="pln"><span class="n"><a href="#t441">441</a></span><span class="t"><span class="str">    tensor([[ 1.,  2.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t442" class="pln"><span class="n"><a href="#t442">442</a></span><span class="t"><span class="str">            [ 2.,  4.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t443" class="pln"><span class="n"><a href="#t443">443</a></span><span class="t"><span class="str">            [ 3.,  6.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t444" class="pln"><span class="n"><a href="#t444">444</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t445" class="pln"><span class="n"><a href="#t445">445</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t446" class="run"><span class="n"><a href="#t446">446</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">allclose</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t447" class="pln"><span class="n"><a href="#t447">447</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t448" class="pln"><span class="n"><a href="#t448">448</a></span><span class="t"><span class="str">allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -> bool</span>&nbsp;</span><span class="r"></span></p>
    <p id="t449" class="pln"><span class="n"><a href="#t449">449</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t450" class="pln"><span class="n"><a href="#t450">450</a></span><span class="t"><span class="str">This function checks if all :attr:`input` and :attr:`other` satisfy the condition:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t451" class="pln"><span class="n"><a href="#t451">451</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t452" class="pln"><span class="n"><a href="#t452">452</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t453" class="pln"><span class="n"><a href="#t453">453</a></span><span class="t"><span class="str">    \lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</span>&nbsp;</span><span class="r"></span></p>
    <p id="t454" class="pln"><span class="n"><a href="#t454">454</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t455" class="pln"><span class="n"><a href="#t455">455</a></span><span class="t"><span class="str">elementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t456" class="pln"><span class="n"><a href="#t456">456</a></span><span class="t"><span class="str">`numpy.allclose &lt;https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html>`_</span>&nbsp;</span><span class="r"></span></p>
    <p id="t457" class="pln"><span class="n"><a href="#t457">457</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t458" class="pln"><span class="n"><a href="#t458">458</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t459" class="pln"><span class="n"><a href="#t459">459</a></span><span class="t"><span class="str">    input (Tensor): first tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t460" class="pln"><span class="n"><a href="#t460">460</a></span><span class="t"><span class="str">    other (Tensor): second tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t461" class="pln"><span class="n"><a href="#t461">461</a></span><span class="t"><span class="str">    atol (float, optional): absolute tolerance. Default: 1e-08</span>&nbsp;</span><span class="r"></span></p>
    <p id="t462" class="pln"><span class="n"><a href="#t462">462</a></span><span class="t"><span class="str">    rtol (float, optional): relative tolerance. Default: 1e-05</span>&nbsp;</span><span class="r"></span></p>
    <p id="t463" class="pln"><span class="n"><a href="#t463">463</a></span><span class="t"><span class="str">    equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be compared as equal. Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t464" class="pln"><span class="n"><a href="#t464">464</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t465" class="pln"><span class="n"><a href="#t465">465</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t466" class="pln"><span class="n"><a href="#t466">466</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t467" class="pln"><span class="n"><a href="#t467">467</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t468" class="pln"><span class="n"><a href="#t468">468</a></span><span class="t"><span class="str">    False</span>&nbsp;</span><span class="r"></span></p>
    <p id="t469" class="pln"><span class="n"><a href="#t469">469</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t470" class="pln"><span class="n"><a href="#t470">470</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t471" class="pln"><span class="n"><a href="#t471">471</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t472" class="pln"><span class="n"><a href="#t472">472</a></span><span class="t"><span class="str">    False</span>&nbsp;</span><span class="r"></span></p>
    <p id="t473" class="pln"><span class="n"><a href="#t473">473</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t474" class="pln"><span class="n"><a href="#t474">474</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t475" class="pln"><span class="n"><a href="#t475">475</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t476" class="pln"><span class="n"><a href="#t476">476</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t477" class="run"><span class="n"><a href="#t477">477</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">angle</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t478" class="pln"><span class="n"><a href="#t478">478</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t479" class="pln"><span class="n"><a href="#t479">479</a></span><span class="t"><span class="str">angle(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t480" class="pln"><span class="n"><a href="#t480">480</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t481" class="pln"><span class="n"><a href="#t481">481</a></span><span class="t"><span class="str">Computes the element-wise angle (in radians) of the given :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t482" class="pln"><span class="n"><a href="#t482">482</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t483" class="pln"><span class="n"><a href="#t483">483</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t484" class="pln"><span class="n"><a href="#t484">484</a></span><span class="t"><span class="str">    \text{out}_{i} = angle(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t485" class="pln"><span class="n"><a href="#t485">485</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t486" class="pln"><span class="n"><a href="#t486">486</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t487" class="pln"><span class="n"><a href="#t487">487</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t488" class="pln"><span class="n"><a href="#t488">488</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t489" class="pln"><span class="n"><a href="#t489">489</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t490" class="pln"><span class="n"><a href="#t490">490</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t491" class="pln"><span class="n"><a href="#t491">491</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t492" class="pln"><span class="n"><a href="#t492">492</a></span><span class="t"><span class="str">    >>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159</span>&nbsp;</span><span class="r"></span></p>
    <p id="t493" class="pln"><span class="n"><a href="#t493">493</a></span><span class="t"><span class="str">    tensor([ 135.,  135,  -45])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t494" class="pln"><span class="n"><a href="#t494">494</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t495" class="pln"><span class="n"><a href="#t495">495</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t496" class="run"><span class="n"><a href="#t496">496</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">as_strided</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t497" class="pln"><span class="n"><a href="#t497">497</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t498" class="pln"><span class="n"><a href="#t498">498</a></span><span class="t"><span class="str">as_strided(input, size, stride, storage_offset=0) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t499" class="pln"><span class="n"><a href="#t499">499</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t500" class="pln"><span class="n"><a href="#t500">500</a></span><span class="t"><span class="str">Create a view of an existing `torch.Tensor` :attr:`input` with specified</span>&nbsp;</span><span class="r"></span></p>
    <p id="t501" class="pln"><span class="n"><a href="#t501">501</a></span><span class="t"><span class="str">:attr:`size`, :attr:`stride` and :attr:`storage_offset`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t502" class="pln"><span class="n"><a href="#t502">502</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t503" class="pln"><span class="n"><a href="#t503">503</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t504" class="pln"><span class="n"><a href="#t504">504</a></span><span class="t"><span class="str">    More than one element of a created tensor may refer to a single memory</span>&nbsp;</span><span class="r"></span></p>
    <p id="t505" class="pln"><span class="n"><a href="#t505">505</a></span><span class="t"><span class="str">    location. As a result, in-place operations (especially ones that are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t506" class="pln"><span class="n"><a href="#t506">506</a></span><span class="t"><span class="str">    vectorized) may result in incorrect behavior. If you need to write to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t507" class="pln"><span class="n"><a href="#t507">507</a></span><span class="t"><span class="str">    the tensors, please clone them first.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t508" class="pln"><span class="n"><a href="#t508">508</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t509" class="pln"><span class="n"><a href="#t509">509</a></span><span class="t"><span class="str">    Many PyTorch functions, which return a view of a tensor, are internally</span>&nbsp;</span><span class="r"></span></p>
    <p id="t510" class="pln"><span class="n"><a href="#t510">510</a></span><span class="t"><span class="str">    implemented with this function. Those functions, like</span>&nbsp;</span><span class="r"></span></p>
    <p id="t511" class="pln"><span class="n"><a href="#t511">511</a></span><span class="t"><span class="str">    :meth:`torch.Tensor.expand`, are easier to read and are therefore more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t512" class="pln"><span class="n"><a href="#t512">512</a></span><span class="t"><span class="str">    advisable to use.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t513" class="pln"><span class="n"><a href="#t513">513</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t514" class="pln"><span class="n"><a href="#t514">514</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t515" class="pln"><span class="n"><a href="#t515">515</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t516" class="pln"><span class="n"><a href="#t516">516</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t517" class="pln"><span class="n"><a href="#t517">517</a></span><span class="t"><span class="str">    size (tuple or ints): the shape of the output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t518" class="pln"><span class="n"><a href="#t518">518</a></span><span class="t"><span class="str">    stride (tuple or ints): the stride of the output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t519" class="pln"><span class="n"><a href="#t519">519</a></span><span class="t"><span class="str">    storage_offset (int, optional): the offset in the underlying storage of the output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t520" class="pln"><span class="n"><a href="#t520">520</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t521" class="pln"><span class="n"><a href="#t521">521</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t522" class="pln"><span class="n"><a href="#t522">522</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t523" class="pln"><span class="n"><a href="#t523">523</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t524" class="pln"><span class="n"><a href="#t524">524</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t525" class="pln"><span class="n"><a href="#t525">525</a></span><span class="t"><span class="str">    tensor([[ 0.9039,  0.6291,  1.0795],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t526" class="pln"><span class="n"><a href="#t526">526</a></span><span class="t"><span class="str">            [ 0.1586,  2.1939, -0.4900],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t527" class="pln"><span class="n"><a href="#t527">527</a></span><span class="t"><span class="str">            [-0.1909, -0.7503,  1.9355]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t528" class="pln"><span class="n"><a href="#t528">528</a></span><span class="t"><span class="str">    >>> t = torch.as_strided(x, (2, 2), (1, 2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t529" class="pln"><span class="n"><a href="#t529">529</a></span><span class="t"><span class="str">    >>> t</span>&nbsp;</span><span class="r"></span></p>
    <p id="t530" class="pln"><span class="n"><a href="#t530">530</a></span><span class="t"><span class="str">    tensor([[0.9039, 1.0795],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t531" class="pln"><span class="n"><a href="#t531">531</a></span><span class="t"><span class="str">            [0.6291, 0.1586]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t532" class="pln"><span class="n"><a href="#t532">532</a></span><span class="t"><span class="str">    >>> t = torch.as_strided(x, (2, 2), (1, 2), 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t533" class="pln"><span class="n"><a href="#t533">533</a></span><span class="t"><span class="str">    tensor([[0.6291, 0.1586],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t534" class="pln"><span class="n"><a href="#t534">534</a></span><span class="t"><span class="str">            [1.0795, 2.1939]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t535" class="pln"><span class="n"><a href="#t535">535</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t536" class="pln"><span class="n"><a href="#t536">536</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t537" class="run"><span class="n"><a href="#t537">537</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">as_tensor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t538" class="pln"><span class="n"><a href="#t538">538</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t539" class="pln"><span class="n"><a href="#t539">539</a></span><span class="t"><span class="str">as_tensor(data, dtype=None, device=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t540" class="pln"><span class="n"><a href="#t540">540</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t541" class="pln"><span class="n"><a href="#t541">541</a></span><span class="t"><span class="str">Convert the data into a `torch.Tensor`. If the data is already a `Tensor` with the same `dtype` and `device`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t542" class="pln"><span class="n"><a href="#t542">542</a></span><span class="t"><span class="str">no copy will be performed, otherwise a new `Tensor` will be returned with computational graph retained if data</span>&nbsp;</span><span class="r"></span></p>
    <p id="t543" class="pln"><span class="n"><a href="#t543">543</a></span><span class="t"><span class="str">`Tensor` has ``requires_grad=True``. Similarly, if the data is an ``ndarray`` of the corresponding `dtype` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t544" class="pln"><span class="n"><a href="#t544">544</a></span><span class="t"><span class="str">the `device` is the cpu, no copy will be performed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t545" class="pln"><span class="n"><a href="#t545">545</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t546" class="pln"><span class="n"><a href="#t546">546</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t547" class="pln"><span class="n"><a href="#t547">547</a></span><span class="t"><span class="str">    {data}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t548" class="pln"><span class="n"><a href="#t548">548</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t549" class="pln"><span class="n"><a href="#t549">549</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t550" class="pln"><span class="n"><a href="#t550">550</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t551" class="pln"><span class="n"><a href="#t551">551</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t552" class="pln"><span class="n"><a href="#t552">552</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t553" class="pln"><span class="n"><a href="#t553">553</a></span><span class="t"><span class="str">    >>> a = numpy.array([1, 2, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t554" class="pln"><span class="n"><a href="#t554">554</a></span><span class="t"><span class="str">    >>> t = torch.as_tensor(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t555" class="pln"><span class="n"><a href="#t555">555</a></span><span class="t"><span class="str">    >>> t</span>&nbsp;</span><span class="r"></span></p>
    <p id="t556" class="pln"><span class="n"><a href="#t556">556</a></span><span class="t"><span class="str">    tensor([ 1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t557" class="pln"><span class="n"><a href="#t557">557</a></span><span class="t"><span class="str">    >>> t[0] = -1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t558" class="pln"><span class="n"><a href="#t558">558</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t559" class="pln"><span class="n"><a href="#t559">559</a></span><span class="t"><span class="str">    array([-1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t560" class="pln"><span class="n"><a href="#t560">560</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t561" class="pln"><span class="n"><a href="#t561">561</a></span><span class="t"><span class="str">    >>> a = numpy.array([1, 2, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t562" class="pln"><span class="n"><a href="#t562">562</a></span><span class="t"><span class="str">    >>> t = torch.as_tensor(a, device=torch.device('cuda'))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t563" class="pln"><span class="n"><a href="#t563">563</a></span><span class="t"><span class="str">    >>> t</span>&nbsp;</span><span class="r"></span></p>
    <p id="t564" class="pln"><span class="n"><a href="#t564">564</a></span><span class="t"><span class="str">    tensor([ 1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t565" class="pln"><span class="n"><a href="#t565">565</a></span><span class="t"><span class="str">    >>> t[0] = -1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t566" class="pln"><span class="n"><a href="#t566">566</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t567" class="pln"><span class="n"><a href="#t567">567</a></span><span class="t"><span class="str">    array([1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t568" class="pln"><span class="n"><a href="#t568">568</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_data_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t569" class="pln"><span class="n"><a href="#t569">569</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t570" class="run"><span class="n"><a href="#t570">570</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">asin</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t571" class="pln"><span class="n"><a href="#t571">571</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t572" class="pln"><span class="n"><a href="#t572">572</a></span><span class="t"><span class="str">asin(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t573" class="pln"><span class="n"><a href="#t573">573</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t574" class="pln"><span class="n"><a href="#t574">574</a></span><span class="t"><span class="str">Returns a new tensor with the arcsine  of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t575" class="pln"><span class="n"><a href="#t575">575</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t576" class="pln"><span class="n"><a href="#t576">576</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t577" class="pln"><span class="n"><a href="#t577">577</a></span><span class="t"><span class="str">    \text{out}_{i} = \sin^{-1}(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t578" class="pln"><span class="n"><a href="#t578">578</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t579" class="pln"><span class="n"><a href="#t579">579</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t580" class="pln"><span class="n"><a href="#t580">580</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t581" class="pln"><span class="n"><a href="#t581">581</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t582" class="pln"><span class="n"><a href="#t582">582</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t583" class="pln"><span class="n"><a href="#t583">583</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t584" class="pln"><span class="n"><a href="#t584">584</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t585" class="pln"><span class="n"><a href="#t585">585</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t586" class="pln"><span class="n"><a href="#t586">586</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t587" class="pln"><span class="n"><a href="#t587">587</a></span><span class="t"><span class="str">    tensor([-0.5962,  1.4985, -0.4396,  1.4525])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t588" class="pln"><span class="n"><a href="#t588">588</a></span><span class="t"><span class="str">    >>> torch.asin(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t589" class="pln"><span class="n"><a href="#t589">589</a></span><span class="t"><span class="str">    tensor([-0.6387,     nan, -0.4552,     nan])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t590" class="pln"><span class="n"><a href="#t590">590</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t591" class="pln"><span class="n"><a href="#t591">591</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t592" class="run"><span class="n"><a href="#t592">592</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">atan</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t593" class="pln"><span class="n"><a href="#t593">593</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t594" class="pln"><span class="n"><a href="#t594">594</a></span><span class="t"><span class="str">atan(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t595" class="pln"><span class="n"><a href="#t595">595</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t596" class="pln"><span class="n"><a href="#t596">596</a></span><span class="t"><span class="str">Returns a new tensor with the arctangent  of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t597" class="pln"><span class="n"><a href="#t597">597</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t598" class="pln"><span class="n"><a href="#t598">598</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t599" class="pln"><span class="n"><a href="#t599">599</a></span><span class="t"><span class="str">    \text{out}_{i} = \tan^{-1}(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t600" class="pln"><span class="n"><a href="#t600">600</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t601" class="pln"><span class="n"><a href="#t601">601</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t602" class="pln"><span class="n"><a href="#t602">602</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t603" class="pln"><span class="n"><a href="#t603">603</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t604" class="pln"><span class="n"><a href="#t604">604</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t605" class="pln"><span class="n"><a href="#t605">605</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t606" class="pln"><span class="n"><a href="#t606">606</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t607" class="pln"><span class="n"><a href="#t607">607</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t608" class="pln"><span class="n"><a href="#t608">608</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t609" class="pln"><span class="n"><a href="#t609">609</a></span><span class="t"><span class="str">    tensor([ 0.2341,  0.2539, -0.6256, -0.6448])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t610" class="pln"><span class="n"><a href="#t610">610</a></span><span class="t"><span class="str">    >>> torch.atan(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t611" class="pln"><span class="n"><a href="#t611">611</a></span><span class="t"><span class="str">    tensor([ 0.2299,  0.2487, -0.5591, -0.5727])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t612" class="pln"><span class="n"><a href="#t612">612</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t613" class="pln"><span class="n"><a href="#t613">613</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t614" class="run"><span class="n"><a href="#t614">614</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">atan2</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t615" class="pln"><span class="n"><a href="#t615">615</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t616" class="pln"><span class="n"><a href="#t616">616</a></span><span class="t"><span class="str">atan2(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t617" class="pln"><span class="n"><a href="#t617">617</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t618" class="pln"><span class="n"><a href="#t618">618</a></span><span class="t"><span class="str">Element-wise arctangent of :math:`\text{{input}}_{{i}} / \text{{other}}_{{i}}`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t619" class="pln"><span class="n"><a href="#t619">619</a></span><span class="t"><span class="str">with consideration of the quadrant. Returns a new tensor with the signed angles</span>&nbsp;</span><span class="r"></span></p>
    <p id="t620" class="pln"><span class="n"><a href="#t620">620</a></span><span class="t"><span class="str">in radians between vector :math:`(\text{{other}}_{{i}}, \text{{input}}_{{i}})`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t621" class="pln"><span class="n"><a href="#t621">621</a></span><span class="t"><span class="str">and vector :math:`(1, 0)`. (Note that :math:`\text{{other}}_{{i}}`, the second</span>&nbsp;</span><span class="r"></span></p>
    <p id="t622" class="pln"><span class="n"><a href="#t622">622</a></span><span class="t"><span class="str">parameter, is the x-coordinate, while :math:`\text{{input}}_{{i}}`, the first</span>&nbsp;</span><span class="r"></span></p>
    <p id="t623" class="pln"><span class="n"><a href="#t623">623</a></span><span class="t"><span class="str">parameter, is the y-coordinate.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t624" class="pln"><span class="n"><a href="#t624">624</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t625" class="pln"><span class="n"><a href="#t625">625</a></span><span class="t"><span class="str">The shapes of ``input`` and ``other`` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t626" class="pln"><span class="n"><a href="#t626">626</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t627" class="pln"><span class="n"><a href="#t627">627</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t628" class="pln"><span class="n"><a href="#t628">628</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t629" class="pln"><span class="n"><a href="#t629">629</a></span><span class="t"><span class="str">    input (Tensor): the first input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t630" class="pln"><span class="n"><a href="#t630">630</a></span><span class="t"><span class="str">    other (Tensor): the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t631" class="pln"><span class="n"><a href="#t631">631</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t632" class="pln"><span class="n"><a href="#t632">632</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t633" class="pln"><span class="n"><a href="#t633">633</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t634" class="pln"><span class="n"><a href="#t634">634</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t635" class="pln"><span class="n"><a href="#t635">635</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t636" class="pln"><span class="n"><a href="#t636">636</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t637" class="pln"><span class="n"><a href="#t637">637</a></span><span class="t"><span class="str">    tensor([ 0.9041,  0.0196, -0.3108, -2.4423])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t638" class="pln"><span class="n"><a href="#t638">638</a></span><span class="t"><span class="str">    >>> torch.atan2(a, torch.randn(4))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t639" class="pln"><span class="n"><a href="#t639">639</a></span><span class="t"><span class="str">    tensor([ 0.9833,  0.0811, -1.9743, -1.4151])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t640" class="pln"><span class="n"><a href="#t640">640</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t641" class="pln"><span class="n"><a href="#t641">641</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t642" class="run"><span class="n"><a href="#t642">642</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">baddbmm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t643" class="pln"><span class="n"><a href="#t643">643</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t644" class="pln"><span class="n"><a href="#t644">644</a></span><span class="t"><span class="str">baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t645" class="pln"><span class="n"><a href="#t645">645</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t646" class="pln"><span class="n"><a href="#t646">646</a></span><span class="t"><span class="str">Performs a batch matrix-matrix product of matrices in :attr:`batch1`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t647" class="pln"><span class="n"><a href="#t647">647</a></span><span class="t"><span class="str">and :attr:`batch2`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t648" class="pln"><span class="n"><a href="#t648">648</a></span><span class="t"><span class="str">:attr:`input` is added to the final result.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t649" class="pln"><span class="n"><a href="#t649">649</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t650" class="pln"><span class="n"><a href="#t650">650</a></span><span class="t"><span class="str">:attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t651" class="pln"><span class="n"><a href="#t651">651</a></span><span class="t"><span class="str">number of matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t652" class="pln"><span class="n"><a href="#t652">652</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t653" class="pln"><span class="n"><a href="#t653">653</a></span><span class="t"><span class="str">If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t654" class="pln"><span class="n"><a href="#t654">654</a></span><span class="t"><span class="str">:math:`(b \times m \times p)` tensor, then :attr:`input` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t655" class="pln"><span class="n"><a href="#t655">655</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t656" class="pln"><span class="n"><a href="#t656">656</a></span><span class="t"><span class="str">:math:`(b \times n \times p)` tensor and :attr:`out` will be a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t657" class="pln"><span class="n"><a href="#t657">657</a></span><span class="t"><span class="str">:math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t658" class="pln"><span class="n"><a href="#t658">658</a></span><span class="t"><span class="str">same as the scaling factors used in :meth:`torch.addbmm`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t659" class="pln"><span class="n"><a href="#t659">659</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t660" class="pln"><span class="n"><a href="#t660">660</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t661" class="pln"><span class="n"><a href="#t661">661</a></span><span class="t"><span class="str">    \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t662" class="pln"><span class="n"><a href="#t662">662</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t663" class="pln"><span class="n"><a href="#t663">663</a></span><span class="t"><span class="str">For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t664" class="pln"><span class="n"><a href="#t664">664</a></span><span class="t"><span class="str">:attr:`alpha` must be real numbers, otherwise they should be integers.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t665" class="pln"><span class="n"><a href="#t665">665</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t666" class="pln"><span class="n"><a href="#t666">666</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t667" class="pln"><span class="n"><a href="#t667">667</a></span><span class="t"><span class="str">    input (Tensor): the tensor to be added</span>&nbsp;</span><span class="r"></span></p>
    <p id="t668" class="pln"><span class="n"><a href="#t668">668</a></span><span class="t"><span class="str">    batch1 (Tensor): the first batch of matrices to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t669" class="pln"><span class="n"><a href="#t669">669</a></span><span class="t"><span class="str">    batch2 (Tensor): the second batch of matrices to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t670" class="pln"><span class="n"><a href="#t670">670</a></span><span class="t"><span class="str">    beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t671" class="pln"><span class="n"><a href="#t671">671</a></span><span class="t"><span class="str">    alpha (Number, optional): multiplier for :math:`\text{{batch1}} \mathbin{{@}} \text{{batch2}}` (:math:`\alpha`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t672" class="pln"><span class="n"><a href="#t672">672</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t673" class="pln"><span class="n"><a href="#t673">673</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t674" class="pln"><span class="n"><a href="#t674">674</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t675" class="pln"><span class="n"><a href="#t675">675</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t676" class="pln"><span class="n"><a href="#t676">676</a></span><span class="t"><span class="str">    >>> M = torch.randn(10, 3, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t677" class="pln"><span class="n"><a href="#t677">677</a></span><span class="t"><span class="str">    >>> batch1 = torch.randn(10, 3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t678" class="pln"><span class="n"><a href="#t678">678</a></span><span class="t"><span class="str">    >>> batch2 = torch.randn(10, 4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t679" class="pln"><span class="n"><a href="#t679">679</a></span><span class="t"><span class="str">    >>> torch.baddbmm(M, batch1, batch2).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t680" class="pln"><span class="n"><a href="#t680">680</a></span><span class="t"><span class="str">    torch.Size([10, 3, 5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t681" class="pln"><span class="n"><a href="#t681">681</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t682" class="pln"><span class="n"><a href="#t682">682</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t683" class="run"><span class="n"><a href="#t683">683</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bernoulli</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t684" class="pln"><span class="n"><a href="#t684">684</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t685" class="pln"><span class="n"><a href="#t685">685</a></span><span class="t"><span class="str">bernoulli(input, *, generator=None, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t686" class="pln"><span class="n"><a href="#t686">686</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t687" class="pln"><span class="n"><a href="#t687">687</a></span><span class="t"><span class="str">Draws binary random numbers (0 or 1) from a Bernoulli distribution.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t688" class="pln"><span class="n"><a href="#t688">688</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t689" class="pln"><span class="n"><a href="#t689">689</a></span><span class="t"><span class="str">The :attr:`input` tensor should be a tensor containing probabilities</span>&nbsp;</span><span class="r"></span></p>
    <p id="t690" class="pln"><span class="n"><a href="#t690">690</a></span><span class="t"><span class="str">to be used for drawing the binary random number.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t691" class="pln"><span class="n"><a href="#t691">691</a></span><span class="t"><span class="str">Hence, all values in :attr:`input` have to be in the range:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t692" class="pln"><span class="n"><a href="#t692">692</a></span><span class="t"><span class="str">:math:`0 \leq \text{input}_i \leq 1`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t693" class="pln"><span class="n"><a href="#t693">693</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t694" class="pln"><span class="n"><a href="#t694">694</a></span><span class="t"><span class="str">The :math:`\text{i}^{th}` element of the output tensor will draw a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t695" class="pln"><span class="n"><a href="#t695">695</a></span><span class="t"><span class="str">value :math:`1` according to the :math:`\text{i}^{th}` probability value given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t696" class="pln"><span class="n"><a href="#t696">696</a></span><span class="t"><span class="str">in :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t697" class="pln"><span class="n"><a href="#t697">697</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t698" class="pln"><span class="n"><a href="#t698">698</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t699" class="pln"><span class="n"><a href="#t699">699</a></span><span class="t"><span class="str">    \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t700" class="pln"><span class="n"><a href="#t700">700</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t701" class="pln"><span class="n"><a href="#t701">701</a></span><span class="t"><span class="str">The returned :attr:`out` tensor only has values 0 or 1 and is of the same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t702" class="pln"><span class="n"><a href="#t702">702</a></span><span class="t"><span class="str">shape as :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t703" class="pln"><span class="n"><a href="#t703">703</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t704" class="pln"><span class="n"><a href="#t704">704</a></span><span class="t"><span class="str">:attr:`out` can have integral ``dtype``, but :attr:`input` must have floating</span>&nbsp;</span><span class="r"></span></p>
    <p id="t705" class="pln"><span class="n"><a href="#t705">705</a></span><span class="t"><span class="str">point ``dtype``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t706" class="pln"><span class="n"><a href="#t706">706</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t707" class="pln"><span class="n"><a href="#t707">707</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t708" class="pln"><span class="n"><a href="#t708">708</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of probability values for the Bernoulli distribution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t709" class="pln"><span class="n"><a href="#t709">709</a></span><span class="t"><span class="str">    {generator}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t710" class="pln"><span class="n"><a href="#t710">710</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t711" class="pln"><span class="n"><a href="#t711">711</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t712" class="pln"><span class="n"><a href="#t712">712</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t713" class="pln"><span class="n"><a href="#t713">713</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t714" class="pln"><span class="n"><a href="#t714">714</a></span><span class="t"><span class="str">    >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t715" class="pln"><span class="n"><a href="#t715">715</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t716" class="pln"><span class="n"><a href="#t716">716</a></span><span class="t"><span class="str">    tensor([[ 0.1737,  0.0950,  0.3609],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t717" class="pln"><span class="n"><a href="#t717">717</a></span><span class="t"><span class="str">            [ 0.7148,  0.0289,  0.2676],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t718" class="pln"><span class="n"><a href="#t718">718</a></span><span class="t"><span class="str">            [ 0.9456,  0.8937,  0.7202]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t719" class="pln"><span class="n"><a href="#t719">719</a></span><span class="t"><span class="str">    >>> torch.bernoulli(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t720" class="pln"><span class="n"><a href="#t720">720</a></span><span class="t"><span class="str">    tensor([[ 1.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t721" class="pln"><span class="n"><a href="#t721">721</a></span><span class="t"><span class="str">            [ 0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t722" class="pln"><span class="n"><a href="#t722">722</a></span><span class="t"><span class="str">            [ 1.,  1.,  1.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t723" class="pln"><span class="n"><a href="#t723">723</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t724" class="pln"><span class="n"><a href="#t724">724</a></span><span class="t"><span class="str">    >>> a = torch.ones(3, 3) # probability of drawing "1" is 1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t725" class="pln"><span class="n"><a href="#t725">725</a></span><span class="t"><span class="str">    >>> torch.bernoulli(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t726" class="pln"><span class="n"><a href="#t726">726</a></span><span class="t"><span class="str">    tensor([[ 1.,  1.,  1.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t727" class="pln"><span class="n"><a href="#t727">727</a></span><span class="t"><span class="str">            [ 1.,  1.,  1.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t728" class="pln"><span class="n"><a href="#t728">728</a></span><span class="t"><span class="str">            [ 1.,  1.,  1.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t729" class="pln"><span class="n"><a href="#t729">729</a></span><span class="t"><span class="str">    >>> a = torch.zeros(3, 3) # probability of drawing "1" is 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t730" class="pln"><span class="n"><a href="#t730">730</a></span><span class="t"><span class="str">    >>> torch.bernoulli(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t731" class="pln"><span class="n"><a href="#t731">731</a></span><span class="t"><span class="str">    tensor([[ 0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t732" class="pln"><span class="n"><a href="#t732">732</a></span><span class="t"><span class="str">            [ 0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t733" class="pln"><span class="n"><a href="#t733">733</a></span><span class="t"><span class="str">            [ 0.,  0.,  0.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t734" class="pln"><span class="n"><a href="#t734">734</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t735" class="pln"><span class="n"><a href="#t735">735</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t736" class="run"><span class="n"><a href="#t736">736</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bincount</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t737" class="pln"><span class="n"><a href="#t737">737</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t738" class="pln"><span class="n"><a href="#t738">738</a></span><span class="t"><span class="str">bincount(input, weights=None, minlength=0) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t739" class="pln"><span class="n"><a href="#t739">739</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t740" class="pln"><span class="n"><a href="#t740">740</a></span><span class="t"><span class="str">Count the frequency of each value in an array of non-negative ints.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t741" class="pln"><span class="n"><a href="#t741">741</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t742" class="pln"><span class="n"><a href="#t742">742</a></span><span class="t"><span class="str">The number of bins (size 1) is one larger than the largest value in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t743" class="pln"><span class="n"><a href="#t743">743</a></span><span class="t"><span class="str">:attr:`input` unless :attr:`input` is empty, in which case the result is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t744" class="pln"><span class="n"><a href="#t744">744</a></span><span class="t"><span class="str">tensor of size 0. If :attr:`minlength` is specified, the number of bins is at least</span>&nbsp;</span><span class="r"></span></p>
    <p id="t745" class="pln"><span class="n"><a href="#t745">745</a></span><span class="t"><span class="str">:attr:`minlength` and if :attr:`input` is empty, then the result is tensor of size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t746" class="pln"><span class="n"><a href="#t746">746</a></span><span class="t"><span class="str">:attr:`minlength` filled with zeros. If ``n`` is the value at position ``i``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t747" class="pln"><span class="n"><a href="#t747">747</a></span><span class="t"><span class="str">``out[n] += weights[i]`` if :attr:`weights` is specified else</span>&nbsp;</span><span class="r"></span></p>
    <p id="t748" class="pln"><span class="n"><a href="#t748">748</a></span><span class="t"><span class="str">``out[n] += 1``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t749" class="pln"><span class="n"><a href="#t749">749</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t750" class="pln"><span class="n"><a href="#t750">750</a></span><span class="t"><span class="str">.. include:: cuda_deterministic.rst</span>&nbsp;</span><span class="r"></span></p>
    <p id="t751" class="pln"><span class="n"><a href="#t751">751</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t752" class="pln"><span class="n"><a href="#t752">752</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t753" class="pln"><span class="n"><a href="#t753">753</a></span><span class="t"><span class="str">    input (Tensor): 1-d int tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t754" class="pln"><span class="n"><a href="#t754">754</a></span><span class="t"><span class="str">    weights (Tensor): optional, weight for each value in the input tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t755" class="pln"><span class="n"><a href="#t755">755</a></span><span class="t"><span class="str">        Should be of same size as input tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t756" class="pln"><span class="n"><a href="#t756">756</a></span><span class="t"><span class="str">    minlength (int): optional, minimum number of bins. Should be non-negative.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t757" class="pln"><span class="n"><a href="#t757">757</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t758" class="pln"><span class="n"><a href="#t758">758</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t759" class="pln"><span class="n"><a href="#t759">759</a></span><span class="t"><span class="str">    output (Tensor): a tensor of shape ``Size([max(input) + 1])`` if</span>&nbsp;</span><span class="r"></span></p>
    <p id="t760" class="pln"><span class="n"><a href="#t760">760</a></span><span class="t"><span class="str">    :attr:`input` is non-empty, else ``Size(0)``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t761" class="pln"><span class="n"><a href="#t761">761</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t762" class="pln"><span class="n"><a href="#t762">762</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t763" class="pln"><span class="n"><a href="#t763">763</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t764" class="pln"><span class="n"><a href="#t764">764</a></span><span class="t"><span class="str">    >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t765" class="pln"><span class="n"><a href="#t765">765</a></span><span class="t"><span class="str">    >>> weights = torch.linspace(0, 1, steps=5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t766" class="pln"><span class="n"><a href="#t766">766</a></span><span class="t"><span class="str">    >>> input, weights</span>&nbsp;</span><span class="r"></span></p>
    <p id="t767" class="pln"><span class="n"><a href="#t767">767</a></span><span class="t"><span class="str">    (tensor([4, 3, 6, 3, 4]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t768" class="pln"><span class="n"><a href="#t768">768</a></span><span class="t"><span class="str">     tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t769" class="pln"><span class="n"><a href="#t769">769</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t770" class="pln"><span class="n"><a href="#t770">770</a></span><span class="t"><span class="str">    >>> torch.bincount(input)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t771" class="pln"><span class="n"><a href="#t771">771</a></span><span class="t"><span class="str">    tensor([0, 0, 0, 2, 2, 0, 1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t772" class="pln"><span class="n"><a href="#t772">772</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t773" class="pln"><span class="n"><a href="#t773">773</a></span><span class="t"><span class="str">    >>> input.bincount(weights)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t774" class="pln"><span class="n"><a href="#t774">774</a></span><span class="t"><span class="str">    tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t775" class="pln"><span class="n"><a href="#t775">775</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t776" class="pln"><span class="n"><a href="#t776">776</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t777" class="run"><span class="n"><a href="#t777">777</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bitwise_not</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t778" class="pln"><span class="n"><a href="#t778">778</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t779" class="pln"><span class="n"><a href="#t779">779</a></span><span class="t"><span class="str">bitwise_not(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t780" class="pln"><span class="n"><a href="#t780">780</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t781" class="pln"><span class="n"><a href="#t781">781</a></span><span class="t"><span class="str">Computes the bitwise NOT of the given input tensor. The input tensor must be of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t782" class="pln"><span class="n"><a href="#t782">782</a></span><span class="t"><span class="str">integral or Boolean types. For bool tensors, it computes the logical NOT.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t783" class="pln"><span class="n"><a href="#t783">783</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t784" class="pln"><span class="n"><a href="#t784">784</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t785" class="pln"><span class="n"><a href="#t785">785</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t786" class="pln"><span class="n"><a href="#t786">786</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t787" class="pln"><span class="n"><a href="#t787">787</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t788" class="pln"><span class="n"><a href="#t788">788</a></span><span class="t"><span class="str">Example:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t789" class="pln"><span class="n"><a href="#t789">789</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t790" class="pln"><span class="n"><a href="#t790">790</a></span><span class="t"><span class="str">    >>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t791" class="pln"><span class="n"><a href="#t791">791</a></span><span class="t"><span class="str">    tensor([ 0,  1, -4], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t792" class="pln"><span class="n"><a href="#t792">792</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t793" class="pln"><span class="n"><a href="#t793">793</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t794" class="run"><span class="n"><a href="#t794">794</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bmm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t795" class="pln"><span class="n"><a href="#t795">795</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t796" class="pln"><span class="n"><a href="#t796">796</a></span><span class="t"><span class="str">bmm(input, mat2, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t797" class="pln"><span class="n"><a href="#t797">797</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t798" class="pln"><span class="n"><a href="#t798">798</a></span><span class="t"><span class="str">Performs a batch matrix-matrix product of matrices stored in :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t799" class="pln"><span class="n"><a href="#t799">799</a></span><span class="t"><span class="str">and :attr:`mat2`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t800" class="pln"><span class="n"><a href="#t800">800</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t801" class="pln"><span class="n"><a href="#t801">801</a></span><span class="t"><span class="str">:attr:`input` and :attr:`mat2` must be 3-D tensors each containing</span>&nbsp;</span><span class="r"></span></p>
    <p id="t802" class="pln"><span class="n"><a href="#t802">802</a></span><span class="t"><span class="str">the same number of matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t803" class="pln"><span class="n"><a href="#t803">803</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t804" class="pln"><span class="n"><a href="#t804">804</a></span><span class="t"><span class="str">If :attr:`input` is a :math:`(b \times n \times m)` tensor, :attr:`mat2` is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t805" class="pln"><span class="n"><a href="#t805">805</a></span><span class="t"><span class="str">:math:`(b \times m \times p)` tensor, :attr:`out` will be a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t806" class="pln"><span class="n"><a href="#t806">806</a></span><span class="t"><span class="str">:math:`(b \times n \times p)` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t807" class="pln"><span class="n"><a href="#t807">807</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t808" class="pln"><span class="n"><a href="#t808">808</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t809" class="pln"><span class="n"><a href="#t809">809</a></span><span class="t"><span class="str">    \text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i</span>&nbsp;</span><span class="r"></span></p>
    <p id="t810" class="pln"><span class="n"><a href="#t810">810</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t811" class="pln"><span class="n"><a href="#t811">811</a></span><span class="t"><span class="str">.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t812" class="pln"><span class="n"><a href="#t812">812</a></span><span class="t"><span class="str">          For broadcasting matrix products, see :func:`torch.matmul`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t813" class="pln"><span class="n"><a href="#t813">813</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t814" class="pln"><span class="n"><a href="#t814">814</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t815" class="pln"><span class="n"><a href="#t815">815</a></span><span class="t"><span class="str">    input (Tensor): the first batch of matrices to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t816" class="pln"><span class="n"><a href="#t816">816</a></span><span class="t"><span class="str">    mat2 (Tensor): the second batch of matrices to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t817" class="pln"><span class="n"><a href="#t817">817</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t818" class="pln"><span class="n"><a href="#t818">818</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t819" class="pln"><span class="n"><a href="#t819">819</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t820" class="pln"><span class="n"><a href="#t820">820</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t821" class="pln"><span class="n"><a href="#t821">821</a></span><span class="t"><span class="str">    >>> input = torch.randn(10, 3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t822" class="pln"><span class="n"><a href="#t822">822</a></span><span class="t"><span class="str">    >>> mat2 = torch.randn(10, 4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t823" class="pln"><span class="n"><a href="#t823">823</a></span><span class="t"><span class="str">    >>> res = torch.bmm(input, mat2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t824" class="pln"><span class="n"><a href="#t824">824</a></span><span class="t"><span class="str">    >>> res.size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t825" class="pln"><span class="n"><a href="#t825">825</a></span><span class="t"><span class="str">    torch.Size([10, 3, 5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t826" class="pln"><span class="n"><a href="#t826">826</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t827" class="pln"><span class="n"><a href="#t827">827</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t828" class="run"><span class="n"><a href="#t828">828</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bitwise_and</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t829" class="pln"><span class="n"><a href="#t829">829</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t830" class="pln"><span class="n"><a href="#t830">830</a></span><span class="t"><span class="str">bitwise_and(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t831" class="pln"><span class="n"><a href="#t831">831</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t832" class="pln"><span class="n"><a href="#t832">832</a></span><span class="t"><span class="str">Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t833" class="pln"><span class="n"><a href="#t833">833</a></span><span class="t"><span class="str">integral or Boolean types. For bool tensors, it computes the logical AND.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t834" class="pln"><span class="n"><a href="#t834">834</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t835" class="pln"><span class="n"><a href="#t835">835</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t836" class="pln"><span class="n"><a href="#t836">836</a></span><span class="t"><span class="str">    input: the first input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t837" class="pln"><span class="n"><a href="#t837">837</a></span><span class="t"><span class="str">    other: the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t838" class="pln"><span class="n"><a href="#t838">838</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t839" class="pln"><span class="n"><a href="#t839">839</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t840" class="pln"><span class="n"><a href="#t840">840</a></span><span class="t"><span class="str">Example:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t841" class="pln"><span class="n"><a href="#t841">841</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t842" class="pln"><span class="n"><a href="#t842">842</a></span><span class="t"><span class="str">    >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t843" class="pln"><span class="n"><a href="#t843">843</a></span><span class="t"><span class="str">    tensor([1, 0,  3], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t844" class="pln"><span class="n"><a href="#t844">844</a></span><span class="t"><span class="str">    >>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t845" class="pln"><span class="n"><a href="#t845">845</a></span><span class="t"><span class="str">    tensor([ False, True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t846" class="pln"><span class="n"><a href="#t846">846</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t847" class="pln"><span class="n"><a href="#t847">847</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t848" class="run"><span class="n"><a href="#t848">848</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bitwise_or</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t849" class="pln"><span class="n"><a href="#t849">849</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t850" class="pln"><span class="n"><a href="#t850">850</a></span><span class="t"><span class="str">bitwise_or(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t851" class="pln"><span class="n"><a href="#t851">851</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t852" class="pln"><span class="n"><a href="#t852">852</a></span><span class="t"><span class="str">Computes the bitwise OR of :attr:`input` and :attr:`other`. The input tensor must be of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t853" class="pln"><span class="n"><a href="#t853">853</a></span><span class="t"><span class="str">integral or Boolean types. For bool tensors, it computes the logical OR.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t854" class="pln"><span class="n"><a href="#t854">854</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t855" class="pln"><span class="n"><a href="#t855">855</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t856" class="pln"><span class="n"><a href="#t856">856</a></span><span class="t"><span class="str">    input: the first input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t857" class="pln"><span class="n"><a href="#t857">857</a></span><span class="t"><span class="str">    other: the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t858" class="pln"><span class="n"><a href="#t858">858</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t859" class="pln"><span class="n"><a href="#t859">859</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t860" class="pln"><span class="n"><a href="#t860">860</a></span><span class="t"><span class="str">Example:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t861" class="pln"><span class="n"><a href="#t861">861</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t862" class="pln"><span class="n"><a href="#t862">862</a></span><span class="t"><span class="str">    >>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t863" class="pln"><span class="n"><a href="#t863">863</a></span><span class="t"><span class="str">    tensor([-1, -2,  3], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t864" class="pln"><span class="n"><a href="#t864">864</a></span><span class="t"><span class="str">    >>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t865" class="pln"><span class="n"><a href="#t865">865</a></span><span class="t"><span class="str">    tensor([ True, True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t866" class="pln"><span class="n"><a href="#t866">866</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t867" class="pln"><span class="n"><a href="#t867">867</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t868" class="run"><span class="n"><a href="#t868">868</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bitwise_xor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t869" class="pln"><span class="n"><a href="#t869">869</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t870" class="pln"><span class="n"><a href="#t870">870</a></span><span class="t"><span class="str">bitwise_xor(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t871" class="pln"><span class="n"><a href="#t871">871</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t872" class="pln"><span class="n"><a href="#t872">872</a></span><span class="t"><span class="str">Computes the bitwise XOR of :attr:`input` and :attr:`other`. The input tensor must be of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t873" class="pln"><span class="n"><a href="#t873">873</a></span><span class="t"><span class="str">integral or Boolean types. For bool tensors, it computes the logical XOR.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t874" class="pln"><span class="n"><a href="#t874">874</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t875" class="pln"><span class="n"><a href="#t875">875</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t876" class="pln"><span class="n"><a href="#t876">876</a></span><span class="t"><span class="str">    input: the first input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t877" class="pln"><span class="n"><a href="#t877">877</a></span><span class="t"><span class="str">    other: the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t878" class="pln"><span class="n"><a href="#t878">878</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t879" class="pln"><span class="n"><a href="#t879">879</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t880" class="pln"><span class="n"><a href="#t880">880</a></span><span class="t"><span class="str">Example:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t881" class="pln"><span class="n"><a href="#t881">881</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t882" class="pln"><span class="n"><a href="#t882">882</a></span><span class="t"><span class="str">    >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t883" class="pln"><span class="n"><a href="#t883">883</a></span><span class="t"><span class="str">    tensor([-2, -2,  0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t884" class="pln"><span class="n"><a href="#t884">884</a></span><span class="t"><span class="str">    >>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t885" class="pln"><span class="n"><a href="#t885">885</a></span><span class="t"><span class="str">    tensor([ True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t886" class="pln"><span class="n"><a href="#t886">886</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t887" class="pln"><span class="n"><a href="#t887">887</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t888" class="run"><span class="n"><a href="#t888">888</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">stack</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t889" class="pln"><span class="n"><a href="#t889">889</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t890" class="pln"><span class="n"><a href="#t890">890</a></span><span class="t"><span class="str">stack(tensors, dim=0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t891" class="pln"><span class="n"><a href="#t891">891</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t892" class="pln"><span class="n"><a href="#t892">892</a></span><span class="t"><span class="str">Concatenates sequence of tensors along a new dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t893" class="pln"><span class="n"><a href="#t893">893</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t894" class="pln"><span class="n"><a href="#t894">894</a></span><span class="t"><span class="str">All tensors need to be of the same size.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t895" class="pln"><span class="n"><a href="#t895">895</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t896" class="pln"><span class="n"><a href="#t896">896</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t897" class="pln"><span class="n"><a href="#t897">897</a></span><span class="t"><span class="str">    tensors (sequence of Tensors): sequence of tensors to concatenate</span>&nbsp;</span><span class="r"></span></p>
    <p id="t898" class="pln"><span class="n"><a href="#t898">898</a></span><span class="t"><span class="str">    dim (int): dimension to insert. Has to be between 0 and the number</span>&nbsp;</span><span class="r"></span></p>
    <p id="t899" class="pln"><span class="n"><a href="#t899">899</a></span><span class="t"><span class="str">        of dimensions of concatenated tensors (inclusive)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t900" class="pln"><span class="n"><a href="#t900">900</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t901" class="pln"><span class="n"><a href="#t901">901</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t902" class="pln"><span class="n"><a href="#t902">902</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t903" class="run"><span class="n"><a href="#t903">903</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">chunk</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t904" class="pln"><span class="n"><a href="#t904">904</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t905" class="pln"><span class="n"><a href="#t905">905</a></span><span class="t"><span class="str">chunk(input, chunks, dim=0) -> List of Tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t906" class="pln"><span class="n"><a href="#t906">906</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t907" class="pln"><span class="n"><a href="#t907">907</a></span><span class="t"><span class="str">Splits a tensor into a specific number of chunks. Each chunk is a view of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t908" class="pln"><span class="n"><a href="#t908">908</a></span><span class="t"><span class="str">the input tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t909" class="pln"><span class="n"><a href="#t909">909</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t910" class="pln"><span class="n"><a href="#t910">910</a></span><span class="t"><span class="str">Last chunk will be smaller if the tensor size along the given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t911" class="pln"><span class="n"><a href="#t911">911</a></span><span class="t"><span class="str">:attr:`dim` is not divisible by :attr:`chunks`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t912" class="pln"><span class="n"><a href="#t912">912</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t913" class="pln"><span class="n"><a href="#t913">913</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t914" class="pln"><span class="n"><a href="#t914">914</a></span><span class="t"><span class="str">    input (Tensor): the tensor to split</span>&nbsp;</span><span class="r"></span></p>
    <p id="t915" class="pln"><span class="n"><a href="#t915">915</a></span><span class="t"><span class="str">    chunks (int): number of chunks to return</span>&nbsp;</span><span class="r"></span></p>
    <p id="t916" class="pln"><span class="n"><a href="#t916">916</a></span><span class="t"><span class="str">    dim (int): dimension along which to split the tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t917" class="pln"><span class="n"><a href="#t917">917</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t918" class="pln"><span class="n"><a href="#t918">918</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t919" class="run"><span class="n"><a href="#t919">919</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">can_cast</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t920" class="pln"><span class="n"><a href="#t920">920</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t921" class="pln"><span class="n"><a href="#t921">921</a></span><span class="t"><span class="str">can_cast(from, to) -> bool</span>&nbsp;</span><span class="r"></span></p>
    <p id="t922" class="pln"><span class="n"><a href="#t922">922</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t923" class="pln"><span class="n"><a href="#t923">923</a></span><span class="t"><span class="str">Determines if a type conversion is allowed under PyTorch casting rules</span>&nbsp;</span><span class="r"></span></p>
    <p id="t924" class="pln"><span class="n"><a href="#t924">924</a></span><span class="t"><span class="str">described in the type promotion :ref:`documentation &lt;type-promotion-doc>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t925" class="pln"><span class="n"><a href="#t925">925</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t926" class="pln"><span class="n"><a href="#t926">926</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t927" class="pln"><span class="n"><a href="#t927">927</a></span><span class="t"><span class="str">    from (dtype): The original :class:`torch.dtype`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t928" class="pln"><span class="n"><a href="#t928">928</a></span><span class="t"><span class="str">    to (dtype): The target :class:`torch.dtype`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t929" class="pln"><span class="n"><a href="#t929">929</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t930" class="pln"><span class="n"><a href="#t930">930</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t931" class="pln"><span class="n"><a href="#t931">931</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t932" class="pln"><span class="n"><a href="#t932">932</a></span><span class="t"><span class="str">    >>> torch.can_cast(torch.double, torch.float)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t933" class="pln"><span class="n"><a href="#t933">933</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t934" class="pln"><span class="n"><a href="#t934">934</a></span><span class="t"><span class="str">    >>> torch.can_cast(torch.float, torch.int)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t935" class="pln"><span class="n"><a href="#t935">935</a></span><span class="t"><span class="str">    False</span>&nbsp;</span><span class="r"></span></p>
    <p id="t936" class="pln"><span class="n"><a href="#t936">936</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t937" class="pln"><span class="n"><a href="#t937">937</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t938" class="run"><span class="n"><a href="#t938">938</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cat</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t939" class="pln"><span class="n"><a href="#t939">939</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t940" class="pln"><span class="n"><a href="#t940">940</a></span><span class="t"><span class="str">cat(tensors, dim=0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t941" class="pln"><span class="n"><a href="#t941">941</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t942" class="pln"><span class="n"><a href="#t942">942</a></span><span class="t"><span class="str">Concatenates the given sequence of :attr:`seq` tensors in the given dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t943" class="pln"><span class="n"><a href="#t943">943</a></span><span class="t"><span class="str">All tensors must either have the same shape (except in the concatenating</span>&nbsp;</span><span class="r"></span></p>
    <p id="t944" class="pln"><span class="n"><a href="#t944">944</a></span><span class="t"><span class="str">dimension) or be empty.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t945" class="pln"><span class="n"><a href="#t945">945</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t946" class="pln"><span class="n"><a href="#t946">946</a></span><span class="t"><span class="str">:func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t947" class="pln"><span class="n"><a href="#t947">947</a></span><span class="t"><span class="str">and :func:`torch.chunk`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t948" class="pln"><span class="n"><a href="#t948">948</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t949" class="pln"><span class="n"><a href="#t949">949</a></span><span class="t"><span class="str">:func:`torch.cat` can be best understood via examples.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t950" class="pln"><span class="n"><a href="#t950">950</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t951" class="pln"><span class="n"><a href="#t951">951</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t952" class="pln"><span class="n"><a href="#t952">952</a></span><span class="t"><span class="str">    tensors (sequence of Tensors): any python sequence of tensors of the same type.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t953" class="pln"><span class="n"><a href="#t953">953</a></span><span class="t"><span class="str">        Non-empty tensors provided must have the same shape, except in the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t954" class="pln"><span class="n"><a href="#t954">954</a></span><span class="t"><span class="str">        cat dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t955" class="pln"><span class="n"><a href="#t955">955</a></span><span class="t"><span class="str">    dim (int, optional): the dimension over which the tensors are concatenated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t956" class="pln"><span class="n"><a href="#t956">956</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t957" class="pln"><span class="n"><a href="#t957">957</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t958" class="pln"><span class="n"><a href="#t958">958</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t959" class="pln"><span class="n"><a href="#t959">959</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t960" class="pln"><span class="n"><a href="#t960">960</a></span><span class="t"><span class="str">    >>> x = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t961" class="pln"><span class="n"><a href="#t961">961</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t962" class="pln"><span class="n"><a href="#t962">962</a></span><span class="t"><span class="str">    tensor([[ 0.6580, -1.0969, -0.4614],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t963" class="pln"><span class="n"><a href="#t963">963</a></span><span class="t"><span class="str">            [-0.1034, -0.5790,  0.1497]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t964" class="pln"><span class="n"><a href="#t964">964</a></span><span class="t"><span class="str">    >>> torch.cat((x, x, x), 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t965" class="pln"><span class="n"><a href="#t965">965</a></span><span class="t"><span class="str">    tensor([[ 0.6580, -1.0969, -0.4614],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t966" class="pln"><span class="n"><a href="#t966">966</a></span><span class="t"><span class="str">            [-0.1034, -0.5790,  0.1497],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t967" class="pln"><span class="n"><a href="#t967">967</a></span><span class="t"><span class="str">            [ 0.6580, -1.0969, -0.4614],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t968" class="pln"><span class="n"><a href="#t968">968</a></span><span class="t"><span class="str">            [-0.1034, -0.5790,  0.1497],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t969" class="pln"><span class="n"><a href="#t969">969</a></span><span class="t"><span class="str">            [ 0.6580, -1.0969, -0.4614],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t970" class="pln"><span class="n"><a href="#t970">970</a></span><span class="t"><span class="str">            [-0.1034, -0.5790,  0.1497]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t971" class="pln"><span class="n"><a href="#t971">971</a></span><span class="t"><span class="str">    >>> torch.cat((x, x, x), 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t972" class="pln"><span class="n"><a href="#t972">972</a></span><span class="t"><span class="str">    tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t973" class="pln"><span class="n"><a href="#t973">973</a></span><span class="t"><span class="str">             -1.0969, -0.4614],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t974" class="pln"><span class="n"><a href="#t974">974</a></span><span class="t"><span class="str">            [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t975" class="pln"><span class="n"><a href="#t975">975</a></span><span class="t"><span class="str">             -0.5790,  0.1497]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t976" class="pln"><span class="n"><a href="#t976">976</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t977" class="pln"><span class="n"><a href="#t977">977</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t978" class="run"><span class="n"><a href="#t978">978</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ceil</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t979" class="pln"><span class="n"><a href="#t979">979</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t980" class="pln"><span class="n"><a href="#t980">980</a></span><span class="t"><span class="str">ceil(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t981" class="pln"><span class="n"><a href="#t981">981</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t982" class="pln"><span class="n"><a href="#t982">982</a></span><span class="t"><span class="str">Returns a new tensor with the ceil of the elements of :attr:`input`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t983" class="pln"><span class="n"><a href="#t983">983</a></span><span class="t"><span class="str">the smallest integer greater than or equal to each element.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t984" class="pln"><span class="n"><a href="#t984">984</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t985" class="pln"><span class="n"><a href="#t985">985</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t986" class="pln"><span class="n"><a href="#t986">986</a></span><span class="t"><span class="str">    \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil = \left\lfloor \text{input}_{i} \right\rfloor + 1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t987" class="pln"><span class="n"><a href="#t987">987</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t988" class="pln"><span class="n"><a href="#t988">988</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t989" class="pln"><span class="n"><a href="#t989">989</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t990" class="pln"><span class="n"><a href="#t990">990</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t991" class="pln"><span class="n"><a href="#t991">991</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t992" class="pln"><span class="n"><a href="#t992">992</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t993" class="pln"><span class="n"><a href="#t993">993</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t994" class="pln"><span class="n"><a href="#t994">994</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t995" class="pln"><span class="n"><a href="#t995">995</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t996" class="pln"><span class="n"><a href="#t996">996</a></span><span class="t"><span class="str">    tensor([-0.6341, -1.4208, -1.0900,  0.5826])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t997" class="pln"><span class="n"><a href="#t997">997</a></span><span class="t"><span class="str">    >>> torch.ceil(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t998" class="pln"><span class="n"><a href="#t998">998</a></span><span class="t"><span class="str">    tensor([-0., -1., -1.,  1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t999" class="pln"><span class="n"><a href="#t999">999</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1000" class="pln"><span class="n"><a href="#t1000">1000</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1001" class="run"><span class="n"><a href="#t1001">1001</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">real</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1002" class="pln"><span class="n"><a href="#t1002">1002</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1003" class="pln"><span class="n"><a href="#t1003">1003</a></span><span class="t"><span class="str">real(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1004" class="pln"><span class="n"><a href="#t1004">1004</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1005" class="pln"><span class="n"><a href="#t1005">1005</a></span><span class="t"><span class="str">Computes the element-wise real value of the given :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1006" class="pln"><span class="n"><a href="#t1006">1006</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1007" class="pln"><span class="n"><a href="#t1007">1007</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1008" class="pln"><span class="n"><a href="#t1008">1008</a></span><span class="t"><span class="str">    \text{out}_{i} = real(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1009" class="pln"><span class="n"><a href="#t1009">1009</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1010" class="pln"><span class="n"><a href="#t1010">1010</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1011" class="pln"><span class="n"><a href="#t1011">1011</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1012" class="pln"><span class="n"><a href="#t1012">1012</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1013" class="pln"><span class="n"><a href="#t1013">1013</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1014" class="pln"><span class="n"><a href="#t1014">1014</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1015" class="pln"><span class="n"><a href="#t1015">1015</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1016" class="pln"><span class="n"><a href="#t1016">1016</a></span><span class="t"><span class="str">    >>> torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1017" class="pln"><span class="n"><a href="#t1017">1017</a></span><span class="t"><span class="str">    tensor([ -1,  -2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1018" class="pln"><span class="n"><a href="#t1018">1018</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1019" class="pln"><span class="n"><a href="#t1019">1019</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1020" class="run"><span class="n"><a href="#t1020">1020</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">reciprocal</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1021" class="pln"><span class="n"><a href="#t1021">1021</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1022" class="pln"><span class="n"><a href="#t1022">1022</a></span><span class="t"><span class="str">reciprocal(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1023" class="pln"><span class="n"><a href="#t1023">1023</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1024" class="pln"><span class="n"><a href="#t1024">1024</a></span><span class="t"><span class="str">Returns a new tensor with the reciprocal of the elements of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1025" class="pln"><span class="n"><a href="#t1025">1025</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1026" class="pln"><span class="n"><a href="#t1026">1026</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1027" class="pln"><span class="n"><a href="#t1027">1027</a></span><span class="t"><span class="str">    \text{out}_{i} = \frac{1}{\text{input}_{i}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1028" class="pln"><span class="n"><a href="#t1028">1028</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1029" class="pln"><span class="n"><a href="#t1029">1029</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1030" class="pln"><span class="n"><a href="#t1030">1030</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1031" class="pln"><span class="n"><a href="#t1031">1031</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1032" class="pln"><span class="n"><a href="#t1032">1032</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1033" class="pln"><span class="n"><a href="#t1033">1033</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1034" class="pln"><span class="n"><a href="#t1034">1034</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1035" class="pln"><span class="n"><a href="#t1035">1035</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1036" class="pln"><span class="n"><a href="#t1036">1036</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1037" class="pln"><span class="n"><a href="#t1037">1037</a></span><span class="t"><span class="str">    tensor([-0.4595, -2.1219, -1.4314,  0.7298])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1038" class="pln"><span class="n"><a href="#t1038">1038</a></span><span class="t"><span class="str">    >>> torch.reciprocal(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1039" class="pln"><span class="n"><a href="#t1039">1039</a></span><span class="t"><span class="str">    tensor([-2.1763, -0.4713, -0.6986,  1.3702])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1040" class="pln"><span class="n"><a href="#t1040">1040</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1041" class="pln"><span class="n"><a href="#t1041">1041</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1042" class="run"><span class="n"><a href="#t1042">1042</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cholesky</span><span class="op">,</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1043" class="pln"><span class="n"><a href="#t1043">1043</a></span><span class="t"><span class="str">cholesky(input, upper=False, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1044" class="pln"><span class="n"><a href="#t1044">1044</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1045" class="pln"><span class="n"><a href="#t1045">1045</a></span><span class="t"><span class="str">Computes the Cholesky decomposition of a symmetric positive-definite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1046" class="pln"><span class="n"><a href="#t1046">1046</a></span><span class="t"><span class="str">matrix :math:`A` or for batches of symmetric positive-definite matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1047" class="pln"><span class="n"><a href="#t1047">1047</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1048" class="pln"><span class="n"><a href="#t1048">1048</a></span><span class="t"><span class="str">If :attr:`upper` is ``True``, the returned matrix ``U`` is upper-triangular, and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1049" class="pln"><span class="n"><a href="#t1049">1049</a></span><span class="t"><span class="str">the decomposition has the form:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1050" class="pln"><span class="n"><a href="#t1050">1050</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1051" class="pln"><span class="n"><a href="#t1051">1051</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1052" class="pln"><span class="n"><a href="#t1052">1052</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1053" class="pln"><span class="n"><a href="#t1053">1053</a></span><span class="t"><span class="str">  A = U^TU</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1054" class="pln"><span class="n"><a href="#t1054">1054</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1055" class="pln"><span class="n"><a href="#t1055">1055</a></span><span class="t"><span class="str">If :attr:`upper` is ``False``, the returned matrix ``L`` is lower-triangular, and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1056" class="pln"><span class="n"><a href="#t1056">1056</a></span><span class="t"><span class="str">the decomposition has the form:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1057" class="pln"><span class="n"><a href="#t1057">1057</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1058" class="pln"><span class="n"><a href="#t1058">1058</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1059" class="pln"><span class="n"><a href="#t1059">1059</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1060" class="pln"><span class="n"><a href="#t1060">1060</a></span><span class="t"><span class="str">    A = LL^T</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1061" class="pln"><span class="n"><a href="#t1061">1061</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1062" class="pln"><span class="n"><a href="#t1062">1062</a></span><span class="t"><span class="str">If :attr:`upper` is ``True``, and :math:`A` is a batch of symmetric positive-definite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1063" class="pln"><span class="n"><a href="#t1063">1063</a></span><span class="t"><span class="str">matrices, then the returned tensor will be composed of upper-triangular Cholesky factors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1064" class="pln"><span class="n"><a href="#t1064">1064</a></span><span class="t"><span class="str">of each of the individual matrices. Similarly, when :attr:`upper` is ``False``, the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1065" class="pln"><span class="n"><a href="#t1065">1065</a></span><span class="t"><span class="str">tensor will be composed of lower-triangular Cholesky factors of each of the individual</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1066" class="pln"><span class="n"><a href="#t1066">1066</a></span><span class="t"><span class="str">matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1067" class="pln"><span class="n"><a href="#t1067">1067</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1068" class="pln"><span class="n"><a href="#t1068">1068</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1069" class="pln"><span class="n"><a href="#t1069">1069</a></span><span class="t"><span class="str">    input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1070" class="pln"><span class="n"><a href="#t1070">1070</a></span><span class="t"><span class="str">                batch dimensions consisting of symmetric positive-definite matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1071" class="pln"><span class="n"><a href="#t1071">1071</a></span><span class="t"><span class="str">    upper (bool, optional): flag that indicates whether to return a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1072" class="pln"><span class="n"><a href="#t1072">1072</a></span><span class="t"><span class="str">                            upper or lower triangular matrix. Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1073" class="pln"><span class="n"><a href="#t1073">1073</a></span><span class="t"><span class="str">    out (Tensor, optional): the output matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1074" class="pln"><span class="n"><a href="#t1074">1074</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1075" class="pln"><span class="n"><a href="#t1075">1075</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1076" class="pln"><span class="n"><a href="#t1076">1076</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1077" class="pln"><span class="n"><a href="#t1077">1077</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1078" class="pln"><span class="n"><a href="#t1078">1078</a></span><span class="t"><span class="str">    >>> a = torch.mm(a, a.t()) # make symmetric positive-definite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1079" class="pln"><span class="n"><a href="#t1079">1079</a></span><span class="t"><span class="str">    >>> l = torch.cholesky(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1080" class="pln"><span class="n"><a href="#t1080">1080</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1081" class="pln"><span class="n"><a href="#t1081">1081</a></span><span class="t"><span class="str">    tensor([[ 2.4112, -0.7486,  1.4551],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1082" class="pln"><span class="n"><a href="#t1082">1082</a></span><span class="t"><span class="str">            [-0.7486,  1.3544,  0.1294],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1083" class="pln"><span class="n"><a href="#t1083">1083</a></span><span class="t"><span class="str">            [ 1.4551,  0.1294,  1.6724]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1084" class="pln"><span class="n"><a href="#t1084">1084</a></span><span class="t"><span class="str">    >>> l</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1085" class="pln"><span class="n"><a href="#t1085">1085</a></span><span class="t"><span class="str">    tensor([[ 1.5528,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1086" class="pln"><span class="n"><a href="#t1086">1086</a></span><span class="t"><span class="str">            [-0.4821,  1.0592,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1087" class="pln"><span class="n"><a href="#t1087">1087</a></span><span class="t"><span class="str">            [ 0.9371,  0.5487,  0.7023]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1088" class="pln"><span class="n"><a href="#t1088">1088</a></span><span class="t"><span class="str">    >>> torch.mm(l, l.t())</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1089" class="pln"><span class="n"><a href="#t1089">1089</a></span><span class="t"><span class="str">    tensor([[ 2.4112, -0.7486,  1.4551],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1090" class="pln"><span class="n"><a href="#t1090">1090</a></span><span class="t"><span class="str">            [-0.7486,  1.3544,  0.1294],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1091" class="pln"><span class="n"><a href="#t1091">1091</a></span><span class="t"><span class="str">            [ 1.4551,  0.1294,  1.6724]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1092" class="pln"><span class="n"><a href="#t1092">1092</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1093" class="pln"><span class="n"><a href="#t1093">1093</a></span><span class="t"><span class="str">    >>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1094" class="pln"><span class="n"><a href="#t1094">1094</a></span><span class="t"><span class="str">    >>> l = torch.cholesky(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1095" class="pln"><span class="n"><a href="#t1095">1095</a></span><span class="t"><span class="str">    >>> z = torch.matmul(l, l.transpose(-1, -2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1096" class="pln"><span class="n"><a href="#t1096">1096</a></span><span class="t"><span class="str">    >>> torch.max(torch.abs(z - a)) # Max non-zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1097" class="pln"><span class="n"><a href="#t1097">1097</a></span><span class="t"><span class="str">    tensor(2.3842e-07)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1098" class="pln"><span class="n"><a href="#t1098">1098</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1099" class="pln"><span class="n"><a href="#t1099">1099</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1100" class="run"><span class="n"><a href="#t1100">1100</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cholesky_solve</span><span class="op">,</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1101" class="pln"><span class="n"><a href="#t1101">1101</a></span><span class="t"><span class="str">cholesky_solve(input, input2, upper=False, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1102" class="pln"><span class="n"><a href="#t1102">1102</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1103" class="pln"><span class="n"><a href="#t1103">1103</a></span><span class="t"><span class="str">Solves a linear system of equations with a positive semidefinite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1104" class="pln"><span class="n"><a href="#t1104">1104</a></span><span class="t"><span class="str">matrix to be inverted given its Cholesky factor matrix :math:`u`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1105" class="pln"><span class="n"><a href="#t1105">1105</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1106" class="pln"><span class="n"><a href="#t1106">1106</a></span><span class="t"><span class="str">If :attr:`upper` is ``False``, :math:`u` is and lower triangular and `c` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1107" class="pln"><span class="n"><a href="#t1107">1107</a></span><span class="t"><span class="str">returned such that:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1108" class="pln"><span class="n"><a href="#t1108">1108</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1109" class="pln"><span class="n"><a href="#t1109">1109</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1110" class="pln"><span class="n"><a href="#t1110">1110</a></span><span class="t"><span class="str">    c = (u u^T)^{{-1}} b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1111" class="pln"><span class="n"><a href="#t1111">1111</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1112" class="pln"><span class="n"><a href="#t1112">1112</a></span><span class="t"><span class="str">If :attr:`upper` is ``True`` or not provided, :math:`u` is upper triangular</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1113" class="pln"><span class="n"><a href="#t1113">1113</a></span><span class="t"><span class="str">and `c` is returned such that:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1114" class="pln"><span class="n"><a href="#t1114">1114</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1115" class="pln"><span class="n"><a href="#t1115">1115</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1116" class="pln"><span class="n"><a href="#t1116">1116</a></span><span class="t"><span class="str">    c = (u^T u)^{{-1}} b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1117" class="pln"><span class="n"><a href="#t1117">1117</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1118" class="pln"><span class="n"><a href="#t1118">1118</a></span><span class="t"><span class="str">`torch.cholesky_solve(b, u)` can take in 2D inputs `b, u` or inputs that are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1119" class="pln"><span class="n"><a href="#t1119">1119</a></span><span class="t"><span class="str">batches of 2D matrices. If the inputs are batches, then returns</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1120" class="pln"><span class="n"><a href="#t1120">1120</a></span><span class="t"><span class="str">batched outputs `c`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1121" class="pln"><span class="n"><a href="#t1121">1121</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1122" class="pln"><span class="n"><a href="#t1122">1122</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1123" class="pln"><span class="n"><a href="#t1123">1123</a></span><span class="t"><span class="str">    input (Tensor): input matrix :math:`b` of size :math:`(*, m, k)`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1124" class="pln"><span class="n"><a href="#t1124">1124</a></span><span class="t"><span class="str">                where :math:`*` is zero or more batch dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1125" class="pln"><span class="n"><a href="#t1125">1125</a></span><span class="t"><span class="str">    input2 (Tensor): input matrix :math:`u` of size :math:`(*, m, m)`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1126" class="pln"><span class="n"><a href="#t1126">1126</a></span><span class="t"><span class="str">                where :math:`*` is zero of more batch dimensions composed of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1127" class="pln"><span class="n"><a href="#t1127">1127</a></span><span class="t"><span class="str">                upper or lower triangular Cholesky factor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1128" class="pln"><span class="n"><a href="#t1128">1128</a></span><span class="t"><span class="str">    upper (bool, optional): whether to consider the Cholesky factor as a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1129" class="pln"><span class="n"><a href="#t1129">1129</a></span><span class="t"><span class="str">                            lower or upper triangular matrix. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1130" class="pln"><span class="n"><a href="#t1130">1130</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor for `c`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1131" class="pln"><span class="n"><a href="#t1131">1131</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1132" class="pln"><span class="n"><a href="#t1132">1132</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1133" class="pln"><span class="n"><a href="#t1133">1133</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1134" class="pln"><span class="n"><a href="#t1134">1134</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1135" class="pln"><span class="n"><a href="#t1135">1135</a></span><span class="t"><span class="str">    >>> a = torch.mm(a, a.t()) # make symmetric positive definite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1136" class="pln"><span class="n"><a href="#t1136">1136</a></span><span class="t"><span class="str">    >>> u = torch.cholesky(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1137" class="pln"><span class="n"><a href="#t1137">1137</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1138" class="pln"><span class="n"><a href="#t1138">1138</a></span><span class="t"><span class="str">    tensor([[ 0.7747, -1.9549,  1.3086],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1139" class="pln"><span class="n"><a href="#t1139">1139</a></span><span class="t"><span class="str">            [-1.9549,  6.7546, -5.4114],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1140" class="pln"><span class="n"><a href="#t1140">1140</a></span><span class="t"><span class="str">            [ 1.3086, -5.4114,  4.8733]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1141" class="pln"><span class="n"><a href="#t1141">1141</a></span><span class="t"><span class="str">    >>> b = torch.randn(3, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1142" class="pln"><span class="n"><a href="#t1142">1142</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1143" class="pln"><span class="n"><a href="#t1143">1143</a></span><span class="t"><span class="str">    tensor([[-0.6355,  0.9891],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1144" class="pln"><span class="n"><a href="#t1144">1144</a></span><span class="t"><span class="str">            [ 0.1974,  1.4706],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1145" class="pln"><span class="n"><a href="#t1145">1145</a></span><span class="t"><span class="str">            [-0.4115, -0.6225]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1146" class="pln"><span class="n"><a href="#t1146">1146</a></span><span class="t"><span class="str">    >>> torch.cholesky_solve(b, u)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1147" class="pln"><span class="n"><a href="#t1147">1147</a></span><span class="t"><span class="str">    tensor([[ -8.1625,  19.6097],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1148" class="pln"><span class="n"><a href="#t1148">1148</a></span><span class="t"><span class="str">            [ -5.8398,  14.2387],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1149" class="pln"><span class="n"><a href="#t1149">1149</a></span><span class="t"><span class="str">            [ -4.3771,  10.4173]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1150" class="pln"><span class="n"><a href="#t1150">1150</a></span><span class="t"><span class="str">    >>> torch.mm(a.inverse(), b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1151" class="pln"><span class="n"><a href="#t1151">1151</a></span><span class="t"><span class="str">    tensor([[ -8.1626,  19.6097],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1152" class="pln"><span class="n"><a href="#t1152">1152</a></span><span class="t"><span class="str">            [ -5.8398,  14.2387],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1153" class="pln"><span class="n"><a href="#t1153">1153</a></span><span class="t"><span class="str">            [ -4.3771,  10.4173]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1154" class="pln"><span class="n"><a href="#t1154">1154</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1155" class="pln"><span class="n"><a href="#t1155">1155</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1156" class="run"><span class="n"><a href="#t1156">1156</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cholesky_inverse</span><span class="op">,</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1157" class="pln"><span class="n"><a href="#t1157">1157</a></span><span class="t"><span class="str">cholesky_inverse(input, upper=False, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1158" class="pln"><span class="n"><a href="#t1158">1158</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1159" class="pln"><span class="n"><a href="#t1159">1159</a></span><span class="t"><span class="str">Computes the inverse of a symmetric positive-definite matrix :math:`A` using its</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1160" class="pln"><span class="n"><a href="#t1160">1160</a></span><span class="t"><span class="str">Cholesky factor :math:`u`: returns matrix ``inv``. The inverse is computed using</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1161" class="pln"><span class="n"><a href="#t1161">1161</a></span><span class="t"><span class="str">LAPACK routines ``dpotri`` and ``spotri`` (and the corresponding MAGMA routines).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1162" class="pln"><span class="n"><a href="#t1162">1162</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1163" class="pln"><span class="n"><a href="#t1163">1163</a></span><span class="t"><span class="str">If :attr:`upper` is ``False``, :math:`u` is lower triangular</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1164" class="pln"><span class="n"><a href="#t1164">1164</a></span><span class="t"><span class="str">such that the returned tensor is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1165" class="pln"><span class="n"><a href="#t1165">1165</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1166" class="pln"><span class="n"><a href="#t1166">1166</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1167" class="pln"><span class="n"><a href="#t1167">1167</a></span><span class="t"><span class="str">    inv = (uu^{{T}})^{{-1}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1168" class="pln"><span class="n"><a href="#t1168">1168</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1169" class="pln"><span class="n"><a href="#t1169">1169</a></span><span class="t"><span class="str">If :attr:`upper` is ``True`` or not provided, :math:`u` is upper</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1170" class="pln"><span class="n"><a href="#t1170">1170</a></span><span class="t"><span class="str">triangular such that the returned tensor is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1171" class="pln"><span class="n"><a href="#t1171">1171</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1172" class="pln"><span class="n"><a href="#t1172">1172</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1173" class="pln"><span class="n"><a href="#t1173">1173</a></span><span class="t"><span class="str">    inv = (u^T u)^{{-1}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1174" class="pln"><span class="n"><a href="#t1174">1174</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1175" class="pln"><span class="n"><a href="#t1175">1175</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1176" class="pln"><span class="n"><a href="#t1176">1176</a></span><span class="t"><span class="str">    input (Tensor): the input 2-D tensor :math:`u`, a upper or lower triangular</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1177" class="pln"><span class="n"><a href="#t1177">1177</a></span><span class="t"><span class="str">           Cholesky factor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1178" class="pln"><span class="n"><a href="#t1178">1178</a></span><span class="t"><span class="str">    upper (bool, optional): whether to return a lower (default) or upper triangular matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1179" class="pln"><span class="n"><a href="#t1179">1179</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor for `inv`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1180" class="pln"><span class="n"><a href="#t1180">1180</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1181" class="pln"><span class="n"><a href="#t1181">1181</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1182" class="pln"><span class="n"><a href="#t1182">1182</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1183" class="pln"><span class="n"><a href="#t1183">1183</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1184" class="pln"><span class="n"><a href="#t1184">1184</a></span><span class="t"><span class="str">    >>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1185" class="pln"><span class="n"><a href="#t1185">1185</a></span><span class="t"><span class="str">    >>> u = torch.cholesky(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1186" class="pln"><span class="n"><a href="#t1186">1186</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1187" class="pln"><span class="n"><a href="#t1187">1187</a></span><span class="t"><span class="str">    tensor([[  0.9935,  -0.6353,   1.5806],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1188" class="pln"><span class="n"><a href="#t1188">1188</a></span><span class="t"><span class="str">            [ -0.6353,   0.8769,  -1.7183],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1189" class="pln"><span class="n"><a href="#t1189">1189</a></span><span class="t"><span class="str">            [  1.5806,  -1.7183,  10.6618]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1190" class="pln"><span class="n"><a href="#t1190">1190</a></span><span class="t"><span class="str">    >>> torch.cholesky_inverse(u)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1191" class="pln"><span class="n"><a href="#t1191">1191</a></span><span class="t"><span class="str">    tensor([[ 1.9314,  1.2251, -0.0889],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1192" class="pln"><span class="n"><a href="#t1192">1192</a></span><span class="t"><span class="str">            [ 1.2251,  2.4439,  0.2122],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1193" class="pln"><span class="n"><a href="#t1193">1193</a></span><span class="t"><span class="str">            [-0.0889,  0.2122,  0.1412]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1194" class="pln"><span class="n"><a href="#t1194">1194</a></span><span class="t"><span class="str">    >>> a.inverse()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1195" class="pln"><span class="n"><a href="#t1195">1195</a></span><span class="t"><span class="str">    tensor([[ 1.9314,  1.2251, -0.0889],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1196" class="pln"><span class="n"><a href="#t1196">1196</a></span><span class="t"><span class="str">            [ 1.2251,  2.4439,  0.2122],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1197" class="pln"><span class="n"><a href="#t1197">1197</a></span><span class="t"><span class="str">            [-0.0889,  0.2122,  0.1412]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1198" class="pln"><span class="n"><a href="#t1198">1198</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1199" class="pln"><span class="n"><a href="#t1199">1199</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1200" class="run"><span class="n"><a href="#t1200">1200</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">clamp</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1201" class="pln"><span class="n"><a href="#t1201">1201</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1202" class="pln"><span class="n"><a href="#t1202">1202</a></span><span class="t"><span class="str">clamp(input, min, max, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1203" class="pln"><span class="n"><a href="#t1203">1203</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1204" class="pln"><span class="n"><a href="#t1204">1204</a></span><span class="t"><span class="str">Clamp all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]` and return</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1205" class="pln"><span class="n"><a href="#t1205">1205</a></span><span class="t"><span class="str">a resulting tensor:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1206" class="pln"><span class="n"><a href="#t1206">1206</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1207" class="pln"><span class="n"><a href="#t1207">1207</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1208" class="pln"><span class="n"><a href="#t1208">1208</a></span><span class="t"><span class="str">    y_i = \begin{cases}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1209" class="pln"><span class="n"><a href="#t1209">1209</a></span><span class="t"><span class="str">        \text{min} &amp; \text{if } x_i &lt; \text{min} \\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1210" class="pln"><span class="n"><a href="#t1210">1210</a></span><span class="t"><span class="str">        x_i &amp; \text{if } \text{min} \leq x_i \leq \text{max} \\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1211" class="pln"><span class="n"><a href="#t1211">1211</a></span><span class="t"><span class="str">        \text{max} &amp; \text{if } x_i > \text{max}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1212" class="pln"><span class="n"><a href="#t1212">1212</a></span><span class="t"><span class="str">    \end{cases}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1213" class="pln"><span class="n"><a href="#t1213">1213</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1214" class="pln"><span class="n"><a href="#t1214">1214</a></span><span class="t"><span class="str">If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, args :attr:`min`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1215" class="pln"><span class="n"><a href="#t1215">1215</a></span><span class="t"><span class="str">and :attr:`max` must be real numbers, otherwise they should be integers.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1216" class="pln"><span class="n"><a href="#t1216">1216</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1217" class="pln"><span class="n"><a href="#t1217">1217</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1218" class="pln"><span class="n"><a href="#t1218">1218</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1219" class="pln"><span class="n"><a href="#t1219">1219</a></span><span class="t"><span class="str">    min (Number): lower-bound of the range to be clamped to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1220" class="pln"><span class="n"><a href="#t1220">1220</a></span><span class="t"><span class="str">    max (Number): upper-bound of the range to be clamped to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1221" class="pln"><span class="n"><a href="#t1221">1221</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1222" class="pln"><span class="n"><a href="#t1222">1222</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1223" class="pln"><span class="n"><a href="#t1223">1223</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1224" class="pln"><span class="n"><a href="#t1224">1224</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1225" class="pln"><span class="n"><a href="#t1225">1225</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1226" class="pln"><span class="n"><a href="#t1226">1226</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1227" class="pln"><span class="n"><a href="#t1227">1227</a></span><span class="t"><span class="str">    tensor([-1.7120,  0.1734, -0.0478, -0.0922])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1228" class="pln"><span class="n"><a href="#t1228">1228</a></span><span class="t"><span class="str">    >>> torch.clamp(a, min=-0.5, max=0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1229" class="pln"><span class="n"><a href="#t1229">1229</a></span><span class="t"><span class="str">    tensor([-0.5000,  0.1734, -0.0478, -0.0922])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1230" class="pln"><span class="n"><a href="#t1230">1230</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1231" class="pln"><span class="n"><a href="#t1231">1231</a></span><span class="t"><span class="str">.. function:: clamp(input, *, min, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1232" class="pln"><span class="n"><a href="#t1232">1232</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1233" class="pln"><span class="n"><a href="#t1233">1233</a></span><span class="t"><span class="str">Clamps all elements in :attr:`input` to be larger or equal :attr:`min`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1234" class="pln"><span class="n"><a href="#t1234">1234</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1235" class="pln"><span class="n"><a href="#t1235">1235</a></span><span class="t"><span class="str">If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, :attr:`value`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1236" class="pln"><span class="n"><a href="#t1236">1236</a></span><span class="t"><span class="str">should be a real number, otherwise it should be an integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1237" class="pln"><span class="n"><a href="#t1237">1237</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1238" class="pln"><span class="n"><a href="#t1238">1238</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1239" class="pln"><span class="n"><a href="#t1239">1239</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1240" class="pln"><span class="n"><a href="#t1240">1240</a></span><span class="t"><span class="str">    value (Number): minimal value of each element in the output</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1241" class="pln"><span class="n"><a href="#t1241">1241</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1242" class="pln"><span class="n"><a href="#t1242">1242</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1243" class="pln"><span class="n"><a href="#t1243">1243</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1244" class="pln"><span class="n"><a href="#t1244">1244</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1245" class="pln"><span class="n"><a href="#t1245">1245</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1246" class="pln"><span class="n"><a href="#t1246">1246</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1247" class="pln"><span class="n"><a href="#t1247">1247</a></span><span class="t"><span class="str">    tensor([-0.0299, -2.3184,  2.1593, -0.8883])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1248" class="pln"><span class="n"><a href="#t1248">1248</a></span><span class="t"><span class="str">    >>> torch.clamp(a, min=0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1249" class="pln"><span class="n"><a href="#t1249">1249</a></span><span class="t"><span class="str">    tensor([ 0.5000,  0.5000,  2.1593,  0.5000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1250" class="pln"><span class="n"><a href="#t1250">1250</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1251" class="pln"><span class="n"><a href="#t1251">1251</a></span><span class="t"><span class="str">.. function:: clamp(input, *, max, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1252" class="pln"><span class="n"><a href="#t1252">1252</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1253" class="pln"><span class="n"><a href="#t1253">1253</a></span><span class="t"><span class="str">Clamps all elements in :attr:`input` to be smaller or equal :attr:`max`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1254" class="pln"><span class="n"><a href="#t1254">1254</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1255" class="pln"><span class="n"><a href="#t1255">1255</a></span><span class="t"><span class="str">If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, :attr:`value`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1256" class="pln"><span class="n"><a href="#t1256">1256</a></span><span class="t"><span class="str">should be a real number, otherwise it should be an integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1257" class="pln"><span class="n"><a href="#t1257">1257</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1258" class="pln"><span class="n"><a href="#t1258">1258</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1259" class="pln"><span class="n"><a href="#t1259">1259</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1260" class="pln"><span class="n"><a href="#t1260">1260</a></span><span class="t"><span class="str">    value (Number): maximal value of each element in the output</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1261" class="pln"><span class="n"><a href="#t1261">1261</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1262" class="pln"><span class="n"><a href="#t1262">1262</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1263" class="pln"><span class="n"><a href="#t1263">1263</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1264" class="pln"><span class="n"><a href="#t1264">1264</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1265" class="pln"><span class="n"><a href="#t1265">1265</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1266" class="pln"><span class="n"><a href="#t1266">1266</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1267" class="pln"><span class="n"><a href="#t1267">1267</a></span><span class="t"><span class="str">    tensor([ 0.7753, -0.4702, -0.4599,  1.1899])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1268" class="pln"><span class="n"><a href="#t1268">1268</a></span><span class="t"><span class="str">    >>> torch.clamp(a, max=0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1269" class="pln"><span class="n"><a href="#t1269">1269</a></span><span class="t"><span class="str">    tensor([ 0.5000, -0.4702, -0.4599,  0.5000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1270" class="pln"><span class="n"><a href="#t1270">1270</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1271" class="pln"><span class="n"><a href="#t1271">1271</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1272" class="run"><span class="n"><a href="#t1272">1272</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">conj</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1273" class="pln"><span class="n"><a href="#t1273">1273</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1274" class="pln"><span class="n"><a href="#t1274">1274</a></span><span class="t"><span class="str">conj(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1275" class="pln"><span class="n"><a href="#t1275">1275</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1276" class="pln"><span class="n"><a href="#t1276">1276</a></span><span class="t"><span class="str">Computes the element-wise conjugate of the given :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1277" class="pln"><span class="n"><a href="#t1277">1277</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1278" class="pln"><span class="n"><a href="#t1278">1278</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1279" class="pln"><span class="n"><a href="#t1279">1279</a></span><span class="t"><span class="str">    \text{out}_{i} = conj(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1280" class="pln"><span class="n"><a href="#t1280">1280</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1281" class="pln"><span class="n"><a href="#t1281">1281</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1282" class="pln"><span class="n"><a href="#t1282">1282</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1283" class="pln"><span class="n"><a href="#t1283">1283</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1284" class="pln"><span class="n"><a href="#t1284">1284</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1285" class="pln"><span class="n"><a href="#t1285">1285</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1286" class="pln"><span class="n"><a href="#t1286">1286</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1287" class="pln"><span class="n"><a href="#t1287">1287</a></span><span class="t"><span class="str">    >>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1288" class="pln"><span class="n"><a href="#t1288">1288</a></span><span class="t"><span class="str">    tensor([-1 - 1j, -2 - 2j, 3 + 3j])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1289" class="pln"><span class="n"><a href="#t1289">1289</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1290" class="pln"><span class="n"><a href="#t1290">1290</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1291" class="run"><span class="n"><a href="#t1291">1291</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cos</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1292" class="pln"><span class="n"><a href="#t1292">1292</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1293" class="pln"><span class="n"><a href="#t1293">1293</a></span><span class="t"><span class="str">cos(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1294" class="pln"><span class="n"><a href="#t1294">1294</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1295" class="pln"><span class="n"><a href="#t1295">1295</a></span><span class="t"><span class="str">Returns a new tensor with the cosine  of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1296" class="pln"><span class="n"><a href="#t1296">1296</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1297" class="pln"><span class="n"><a href="#t1297">1297</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1298" class="pln"><span class="n"><a href="#t1298">1298</a></span><span class="t"><span class="str">    \text{out}_{i} = \cos(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1299" class="pln"><span class="n"><a href="#t1299">1299</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1300" class="pln"><span class="n"><a href="#t1300">1300</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1301" class="pln"><span class="n"><a href="#t1301">1301</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1302" class="pln"><span class="n"><a href="#t1302">1302</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1303" class="pln"><span class="n"><a href="#t1303">1303</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1304" class="pln"><span class="n"><a href="#t1304">1304</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1305" class="pln"><span class="n"><a href="#t1305">1305</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1306" class="pln"><span class="n"><a href="#t1306">1306</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1307" class="pln"><span class="n"><a href="#t1307">1307</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1308" class="pln"><span class="n"><a href="#t1308">1308</a></span><span class="t"><span class="str">    tensor([ 1.4309,  1.2706, -0.8562,  0.9796])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1309" class="pln"><span class="n"><a href="#t1309">1309</a></span><span class="t"><span class="str">    >>> torch.cos(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1310" class="pln"><span class="n"><a href="#t1310">1310</a></span><span class="t"><span class="str">    tensor([ 0.1395,  0.2957,  0.6553,  0.5574])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1311" class="pln"><span class="n"><a href="#t1311">1311</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1312" class="pln"><span class="n"><a href="#t1312">1312</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1313" class="run"><span class="n"><a href="#t1313">1313</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cosh</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1314" class="pln"><span class="n"><a href="#t1314">1314</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1315" class="pln"><span class="n"><a href="#t1315">1315</a></span><span class="t"><span class="str">cosh(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1316" class="pln"><span class="n"><a href="#t1316">1316</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1317" class="pln"><span class="n"><a href="#t1317">1317</a></span><span class="t"><span class="str">Returns a new tensor with the hyperbolic cosine  of the elements of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1318" class="pln"><span class="n"><a href="#t1318">1318</a></span><span class="t"><span class="str">:attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1319" class="pln"><span class="n"><a href="#t1319">1319</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1320" class="pln"><span class="n"><a href="#t1320">1320</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1321" class="pln"><span class="n"><a href="#t1321">1321</a></span><span class="t"><span class="str">    \text{out}_{i} = \cosh(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1322" class="pln"><span class="n"><a href="#t1322">1322</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1323" class="pln"><span class="n"><a href="#t1323">1323</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1324" class="pln"><span class="n"><a href="#t1324">1324</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1325" class="pln"><span class="n"><a href="#t1325">1325</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1326" class="pln"><span class="n"><a href="#t1326">1326</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1327" class="pln"><span class="n"><a href="#t1327">1327</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1328" class="pln"><span class="n"><a href="#t1328">1328</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1329" class="pln"><span class="n"><a href="#t1329">1329</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1330" class="pln"><span class="n"><a href="#t1330">1330</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1331" class="pln"><span class="n"><a href="#t1331">1331</a></span><span class="t"><span class="str">    tensor([ 0.1632,  1.1835, -0.6979, -0.7325])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1332" class="pln"><span class="n"><a href="#t1332">1332</a></span><span class="t"><span class="str">    >>> torch.cosh(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1333" class="pln"><span class="n"><a href="#t1333">1333</a></span><span class="t"><span class="str">    tensor([ 1.0133,  1.7860,  1.2536,  1.2805])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1334" class="pln"><span class="n"><a href="#t1334">1334</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1335" class="pln"><span class="n"><a href="#t1335">1335</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1336" class="run"><span class="n"><a href="#t1336">1336</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cross</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1337" class="pln"><span class="n"><a href="#t1337">1337</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1338" class="pln"><span class="n"><a href="#t1338">1338</a></span><span class="t"><span class="str">cross(input, other, dim=-1, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1339" class="pln"><span class="n"><a href="#t1339">1339</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1340" class="pln"><span class="n"><a href="#t1340">1340</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1341" class="pln"><span class="n"><a href="#t1341">1341</a></span><span class="t"><span class="str">Returns the cross product of vectors in dimension :attr:`dim` of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1342" class="pln"><span class="n"><a href="#t1342">1342</a></span><span class="t"><span class="str">and :attr:`other`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1343" class="pln"><span class="n"><a href="#t1343">1343</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1344" class="pln"><span class="n"><a href="#t1344">1344</a></span><span class="t"><span class="str">:attr:`input` and :attr:`other` must have the same size, and the size of their</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1345" class="pln"><span class="n"><a href="#t1345">1345</a></span><span class="t"><span class="str">:attr:`dim` dimension should be 3.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1346" class="pln"><span class="n"><a href="#t1346">1346</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1347" class="pln"><span class="n"><a href="#t1347">1347</a></span><span class="t"><span class="str">If :attr:`dim` is not given, it defaults to the first dimension found with the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1348" class="pln"><span class="n"><a href="#t1348">1348</a></span><span class="t"><span class="str">size 3.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1349" class="pln"><span class="n"><a href="#t1349">1349</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1350" class="pln"><span class="n"><a href="#t1350">1350</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1351" class="pln"><span class="n"><a href="#t1351">1351</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1352" class="pln"><span class="n"><a href="#t1352">1352</a></span><span class="t"><span class="str">    other (Tensor): the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1353" class="pln"><span class="n"><a href="#t1353">1353</a></span><span class="t"><span class="str">    dim  (int, optional): the dimension to take the cross-product in.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1354" class="pln"><span class="n"><a href="#t1354">1354</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1355" class="pln"><span class="n"><a href="#t1355">1355</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1356" class="pln"><span class="n"><a href="#t1356">1356</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1357" class="pln"><span class="n"><a href="#t1357">1357</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1358" class="pln"><span class="n"><a href="#t1358">1358</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1359" class="pln"><span class="n"><a href="#t1359">1359</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1360" class="pln"><span class="n"><a href="#t1360">1360</a></span><span class="t"><span class="str">    tensor([[-0.3956,  1.1455,  1.6895],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1361" class="pln"><span class="n"><a href="#t1361">1361</a></span><span class="t"><span class="str">            [-0.5849,  1.3672,  0.3599],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1362" class="pln"><span class="n"><a href="#t1362">1362</a></span><span class="t"><span class="str">            [-1.1626,  0.7180, -0.0521],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1363" class="pln"><span class="n"><a href="#t1363">1363</a></span><span class="t"><span class="str">            [-0.1339,  0.9902, -2.0225]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1364" class="pln"><span class="n"><a href="#t1364">1364</a></span><span class="t"><span class="str">    >>> b = torch.randn(4, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1365" class="pln"><span class="n"><a href="#t1365">1365</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1366" class="pln"><span class="n"><a href="#t1366">1366</a></span><span class="t"><span class="str">    tensor([[-0.0257, -1.4725, -1.2251],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1367" class="pln"><span class="n"><a href="#t1367">1367</a></span><span class="t"><span class="str">            [-1.1479, -0.7005, -1.9757],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1368" class="pln"><span class="n"><a href="#t1368">1368</a></span><span class="t"><span class="str">            [-1.3904,  0.3726, -1.1836],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1369" class="pln"><span class="n"><a href="#t1369">1369</a></span><span class="t"><span class="str">            [-0.9688, -0.7153,  0.2159]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1370" class="pln"><span class="n"><a href="#t1370">1370</a></span><span class="t"><span class="str">    >>> torch.cross(a, b, dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1371" class="pln"><span class="n"><a href="#t1371">1371</a></span><span class="t"><span class="str">    tensor([[ 1.0844, -0.5281,  0.6120],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1372" class="pln"><span class="n"><a href="#t1372">1372</a></span><span class="t"><span class="str">            [-2.4490, -1.5687,  1.9792],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1373" class="pln"><span class="n"><a href="#t1373">1373</a></span><span class="t"><span class="str">            [-0.8304, -1.3037,  0.5650],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1374" class="pln"><span class="n"><a href="#t1374">1374</a></span><span class="t"><span class="str">            [-1.2329,  1.9883,  1.0551]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1375" class="pln"><span class="n"><a href="#t1375">1375</a></span><span class="t"><span class="str">    >>> torch.cross(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1376" class="pln"><span class="n"><a href="#t1376">1376</a></span><span class="t"><span class="str">    tensor([[ 1.0844, -0.5281,  0.6120],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1377" class="pln"><span class="n"><a href="#t1377">1377</a></span><span class="t"><span class="str">            [-2.4490, -1.5687,  1.9792],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1378" class="pln"><span class="n"><a href="#t1378">1378</a></span><span class="t"><span class="str">            [-0.8304, -1.3037,  0.5650],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1379" class="pln"><span class="n"><a href="#t1379">1379</a></span><span class="t"><span class="str">            [-1.2329,  1.9883,  1.0551]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1380" class="pln"><span class="n"><a href="#t1380">1380</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1381" class="pln"><span class="n"><a href="#t1381">1381</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1382" class="run"><span class="n"><a href="#t1382">1382</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cummax</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1383" class="pln"><span class="n"><a href="#t1383">1383</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1384" class="pln"><span class="n"><a href="#t1384">1384</a></span><span class="t"><span class="str">cummax(input, dim, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1385" class="pln"><span class="n"><a href="#t1385">1385</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values``is the cumulative maximum of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1386" class="pln"><span class="n"><a href="#t1386">1386</a></span><span class="t"><span class="str">elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1387" class="pln"><span class="n"><a href="#t1387">1387</a></span><span class="t"><span class="str">location of each maximum value found in the dimension :attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1388" class="pln"><span class="n"><a href="#t1388">1388</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1389" class="pln"><span class="n"><a href="#t1389">1389</a></span><span class="t"><span class="str">    y_i = max(x_1, x_2, x_3, \dots, x_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1390" class="pln"><span class="n"><a href="#t1390">1390</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1391" class="pln"><span class="n"><a href="#t1391">1391</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1392" class="pln"><span class="n"><a href="#t1392">1392</a></span><span class="t"><span class="str">    dim  (int): the dimension to do the operation over</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1393" class="pln"><span class="n"><a href="#t1393">1393</a></span><span class="t"><span class="str">    out (tuple, optional): the result tuple of two output tensors (values, indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1394" class="pln"><span class="n"><a href="#t1394">1394</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1395" class="pln"><span class="n"><a href="#t1395">1395</a></span><span class="t"><span class="str">    >>> a = torch.randn(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1396" class="pln"><span class="n"><a href="#t1396">1396</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1397" class="pln"><span class="n"><a href="#t1397">1397</a></span><span class="t"><span class="str">    tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1398" class="pln"><span class="n"><a href="#t1398">1398</a></span><span class="t"><span class="str">         1.9946, -0.8209])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1399" class="pln"><span class="n"><a href="#t1399">1399</a></span><span class="t"><span class="str">    >>> torch.cummax(a, dim=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1400" class="pln"><span class="n"><a href="#t1400">1400</a></span><span class="t"><span class="str">    torch.return_types.cummax(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1401" class="pln"><span class="n"><a href="#t1401">1401</a></span><span class="t"><span class="str">        values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1402" class="pln"><span class="n"><a href="#t1402">1402</a></span><span class="t"><span class="str">         1.9946,  1.9946]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1403" class="pln"><span class="n"><a href="#t1403">1403</a></span><span class="t"><span class="str">        indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1404" class="pln"><span class="n"><a href="#t1404">1404</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">reduceops_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1405" class="pln"><span class="n"><a href="#t1405">1405</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1406" class="run"><span class="n"><a href="#t1406">1406</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cummin</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1407" class="pln"><span class="n"><a href="#t1407">1407</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1408" class="pln"><span class="n"><a href="#t1408">1408</a></span><span class="t"><span class="str">cummin(input, dim, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1409" class="pln"><span class="n"><a href="#t1409">1409</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values``is the cumulative maximum of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1410" class="pln"><span class="n"><a href="#t1410">1410</a></span><span class="t"><span class="str">elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1411" class="pln"><span class="n"><a href="#t1411">1411</a></span><span class="t"><span class="str">location of each maximum value found in the dimension :attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1412" class="pln"><span class="n"><a href="#t1412">1412</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1413" class="pln"><span class="n"><a href="#t1413">1413</a></span><span class="t"><span class="str">    y_i = max(x_1, x_2, x_3, \dots, x_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1414" class="pln"><span class="n"><a href="#t1414">1414</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1415" class="pln"><span class="n"><a href="#t1415">1415</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1416" class="pln"><span class="n"><a href="#t1416">1416</a></span><span class="t"><span class="str">    dim  (int): the dimension to do the operation over</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1417" class="pln"><span class="n"><a href="#t1417">1417</a></span><span class="t"><span class="str">    out (tuple, optional): the result tuple of two output tensors (values, indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1418" class="pln"><span class="n"><a href="#t1418">1418</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1419" class="pln"><span class="n"><a href="#t1419">1419</a></span><span class="t"><span class="str">    >>> a = torch.randn(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1420" class="pln"><span class="n"><a href="#t1420">1420</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1421" class="pln"><span class="n"><a href="#t1421">1421</a></span><span class="t"><span class="str">    tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1422" class="pln"><span class="n"><a href="#t1422">1422</a></span><span class="t"><span class="str">         0.9165,  1.6684])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1423" class="pln"><span class="n"><a href="#t1423">1423</a></span><span class="t"><span class="str">    >>> torch.cummin(a, dim=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1424" class="pln"><span class="n"><a href="#t1424">1424</a></span><span class="t"><span class="str">    torch.return_types.cummin(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1425" class="pln"><span class="n"><a href="#t1425">1425</a></span><span class="t"><span class="str">        values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1426" class="pln"><span class="n"><a href="#t1426">1426</a></span><span class="t"><span class="str">        -1.3298, -1.3298]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1427" class="pln"><span class="n"><a href="#t1427">1427</a></span><span class="t"><span class="str">        indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1428" class="pln"><span class="n"><a href="#t1428">1428</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">reduceops_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1429" class="pln"><span class="n"><a href="#t1429">1429</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1430" class="run"><span class="n"><a href="#t1430">1430</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cumprod</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1431" class="pln"><span class="n"><a href="#t1431">1431</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1432" class="pln"><span class="n"><a href="#t1432">1432</a></span><span class="t"><span class="str">cumprod(input, dim, out=None, dtype=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1433" class="pln"><span class="n"><a href="#t1433">1433</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1434" class="pln"><span class="n"><a href="#t1434">1434</a></span><span class="t"><span class="str">Returns the cumulative product of elements of :attr:`input` in the dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1435" class="pln"><span class="n"><a href="#t1435">1435</a></span><span class="t"><span class="str">:attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1436" class="pln"><span class="n"><a href="#t1436">1436</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1437" class="pln"><span class="n"><a href="#t1437">1437</a></span><span class="t"><span class="str">For example, if :attr:`input` is a vector of size N, the result will also be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1438" class="pln"><span class="n"><a href="#t1438">1438</a></span><span class="t"><span class="str">a vector of size N, with elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1439" class="pln"><span class="n"><a href="#t1439">1439</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1440" class="pln"><span class="n"><a href="#t1440">1440</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1441" class="pln"><span class="n"><a href="#t1441">1441</a></span><span class="t"><span class="str">    y_i = x_1 \times x_2\times x_3\times \dots \times x_i</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1442" class="pln"><span class="n"><a href="#t1442">1442</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1443" class="pln"><span class="n"><a href="#t1443">1443</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1444" class="pln"><span class="n"><a href="#t1444">1444</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1445" class="pln"><span class="n"><a href="#t1445">1445</a></span><span class="t"><span class="str">    dim  (int): the dimension to do the operation over</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1446" class="pln"><span class="n"><a href="#t1446">1446</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1447" class="pln"><span class="n"><a href="#t1447">1447</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1448" class="pln"><span class="n"><a href="#t1448">1448</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1449" class="pln"><span class="n"><a href="#t1449">1449</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1450" class="pln"><span class="n"><a href="#t1450">1450</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1451" class="pln"><span class="n"><a href="#t1451">1451</a></span><span class="t"><span class="str">    >>> a = torch.randn(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1452" class="pln"><span class="n"><a href="#t1452">1452</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1453" class="pln"><span class="n"><a href="#t1453">1453</a></span><span class="t"><span class="str">    tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1454" class="pln"><span class="n"><a href="#t1454">1454</a></span><span class="t"><span class="str">            -0.2129, -0.4206,  0.1968])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1455" class="pln"><span class="n"><a href="#t1455">1455</a></span><span class="t"><span class="str">    >>> torch.cumprod(a, dim=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1456" class="pln"><span class="n"><a href="#t1456">1456</a></span><span class="t"><span class="str">    tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1457" class="pln"><span class="n"><a href="#t1457">1457</a></span><span class="t"><span class="str">             0.0014, -0.0006, -0.0001])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1458" class="pln"><span class="n"><a href="#t1458">1458</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1459" class="pln"><span class="n"><a href="#t1459">1459</a></span><span class="t"><span class="str">    >>> a[5] = 0.0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1460" class="pln"><span class="n"><a href="#t1460">1460</a></span><span class="t"><span class="str">    >>> torch.cumprod(a, dim=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1461" class="pln"><span class="n"><a href="#t1461">1461</a></span><span class="t"><span class="str">    tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1462" class="pln"><span class="n"><a href="#t1462">1462</a></span><span class="t"><span class="str">             0.0000, -0.0000, -0.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1463" class="pln"><span class="n"><a href="#t1463">1463</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">reduceops_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1464" class="pln"><span class="n"><a href="#t1464">1464</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1465" class="run"><span class="n"><a href="#t1465">1465</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">cumsum</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1466" class="pln"><span class="n"><a href="#t1466">1466</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1467" class="pln"><span class="n"><a href="#t1467">1467</a></span><span class="t"><span class="str">cumsum(input, dim, out=None, dtype=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1468" class="pln"><span class="n"><a href="#t1468">1468</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1469" class="pln"><span class="n"><a href="#t1469">1469</a></span><span class="t"><span class="str">Returns the cumulative sum of elements of :attr:`input` in the dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1470" class="pln"><span class="n"><a href="#t1470">1470</a></span><span class="t"><span class="str">:attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1471" class="pln"><span class="n"><a href="#t1471">1471</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1472" class="pln"><span class="n"><a href="#t1472">1472</a></span><span class="t"><span class="str">For example, if :attr:`input` is a vector of size N, the result will also be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1473" class="pln"><span class="n"><a href="#t1473">1473</a></span><span class="t"><span class="str">a vector of size N, with elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1474" class="pln"><span class="n"><a href="#t1474">1474</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1475" class="pln"><span class="n"><a href="#t1475">1475</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1476" class="pln"><span class="n"><a href="#t1476">1476</a></span><span class="t"><span class="str">    y_i = x_1 + x_2 + x_3 + \dots + x_i</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1477" class="pln"><span class="n"><a href="#t1477">1477</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1478" class="pln"><span class="n"><a href="#t1478">1478</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1479" class="pln"><span class="n"><a href="#t1479">1479</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1480" class="pln"><span class="n"><a href="#t1480">1480</a></span><span class="t"><span class="str">    dim  (int): the dimension to do the operation over</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1481" class="pln"><span class="n"><a href="#t1481">1481</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1482" class="pln"><span class="n"><a href="#t1482">1482</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1483" class="pln"><span class="n"><a href="#t1483">1483</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1484" class="pln"><span class="n"><a href="#t1484">1484</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1485" class="pln"><span class="n"><a href="#t1485">1485</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1486" class="pln"><span class="n"><a href="#t1486">1486</a></span><span class="t"><span class="str">    >>> a = torch.randn(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1487" class="pln"><span class="n"><a href="#t1487">1487</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1488" class="pln"><span class="n"><a href="#t1488">1488</a></span><span class="t"><span class="str">    tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1489" class="pln"><span class="n"><a href="#t1489">1489</a></span><span class="t"><span class="str">             0.1850, -1.1571, -0.4243])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1490" class="pln"><span class="n"><a href="#t1490">1490</a></span><span class="t"><span class="str">    >>> torch.cumsum(a, dim=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1491" class="pln"><span class="n"><a href="#t1491">1491</a></span><span class="t"><span class="str">    tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1492" class="pln"><span class="n"><a href="#t1492">1492</a></span><span class="t"><span class="str">            -1.8209, -2.9780, -3.4022])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1493" class="pln"><span class="n"><a href="#t1493">1493</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">reduceops_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1494" class="pln"><span class="n"><a href="#t1494">1494</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1495" class="run"><span class="n"><a href="#t1495">1495</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">diag</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1496" class="pln"><span class="n"><a href="#t1496">1496</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1497" class="pln"><span class="n"><a href="#t1497">1497</a></span><span class="t"><span class="str">diag(input, diagonal=0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1498" class="pln"><span class="n"><a href="#t1498">1498</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1499" class="pln"><span class="n"><a href="#t1499">1499</a></span><span class="t"><span class="str">- If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1500" class="pln"><span class="n"><a href="#t1500">1500</a></span><span class="t"><span class="str">  with the elements of :attr:`input` as the diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1501" class="pln"><span class="n"><a href="#t1501">1501</a></span><span class="t"><span class="str">- If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1502" class="pln"><span class="n"><a href="#t1502">1502</a></span><span class="t"><span class="str">  the diagonal elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1503" class="pln"><span class="n"><a href="#t1503">1503</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1504" class="pln"><span class="n"><a href="#t1504">1504</a></span><span class="t"><span class="str">The argument :attr:`diagonal` controls which diagonal to consider:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1505" class="pln"><span class="n"><a href="#t1505">1505</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1506" class="pln"><span class="n"><a href="#t1506">1506</a></span><span class="t"><span class="str">- If :attr:`diagonal` = 0, it is the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1507" class="pln"><span class="n"><a href="#t1507">1507</a></span><span class="t"><span class="str">- If :attr:`diagonal` > 0, it is above the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1508" class="pln"><span class="n"><a href="#t1508">1508</a></span><span class="t"><span class="str">- If :attr:`diagonal` &lt; 0, it is below the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1509" class="pln"><span class="n"><a href="#t1509">1509</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1510" class="pln"><span class="n"><a href="#t1510">1510</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1511" class="pln"><span class="n"><a href="#t1511">1511</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1512" class="pln"><span class="n"><a href="#t1512">1512</a></span><span class="t"><span class="str">    diagonal (int, optional): the diagonal to consider</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1513" class="pln"><span class="n"><a href="#t1513">1513</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1514" class="pln"><span class="n"><a href="#t1514">1514</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1515" class="pln"><span class="n"><a href="#t1515">1515</a></span><span class="t"><span class="str">.. seealso::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1516" class="pln"><span class="n"><a href="#t1516">1516</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1517" class="pln"><span class="n"><a href="#t1517">1517</a></span><span class="t"><span class="str">        :func:`torch.diagonal` always returns the diagonal of its input.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1518" class="pln"><span class="n"><a href="#t1518">1518</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1519" class="pln"><span class="n"><a href="#t1519">1519</a></span><span class="t"><span class="str">        :func:`torch.diagflat` always constructs a tensor with diagonal elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1520" class="pln"><span class="n"><a href="#t1520">1520</a></span><span class="t"><span class="str">        specified by the input.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1521" class="pln"><span class="n"><a href="#t1521">1521</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1522" class="pln"><span class="n"><a href="#t1522">1522</a></span><span class="t"><span class="str">Examples:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1523" class="pln"><span class="n"><a href="#t1523">1523</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1524" class="pln"><span class="n"><a href="#t1524">1524</a></span><span class="t"><span class="str">Get the square matrix where the input vector is the diagonal::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1525" class="pln"><span class="n"><a href="#t1525">1525</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1526" class="pln"><span class="n"><a href="#t1526">1526</a></span><span class="t"><span class="str">    >>> a = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1527" class="pln"><span class="n"><a href="#t1527">1527</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1528" class="pln"><span class="n"><a href="#t1528">1528</a></span><span class="t"><span class="str">    tensor([ 0.5950,-0.0872, 2.3298])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1529" class="pln"><span class="n"><a href="#t1529">1529</a></span><span class="t"><span class="str">    >>> torch.diag(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1530" class="pln"><span class="n"><a href="#t1530">1530</a></span><span class="t"><span class="str">    tensor([[ 0.5950, 0.0000, 0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1531" class="pln"><span class="n"><a href="#t1531">1531</a></span><span class="t"><span class="str">            [ 0.0000,-0.0872, 0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1532" class="pln"><span class="n"><a href="#t1532">1532</a></span><span class="t"><span class="str">            [ 0.0000, 0.0000, 2.3298]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1533" class="pln"><span class="n"><a href="#t1533">1533</a></span><span class="t"><span class="str">    >>> torch.diag(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1534" class="pln"><span class="n"><a href="#t1534">1534</a></span><span class="t"><span class="str">    tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1535" class="pln"><span class="n"><a href="#t1535">1535</a></span><span class="t"><span class="str">            [ 0.0000, 0.0000,-0.0872, 0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1536" class="pln"><span class="n"><a href="#t1536">1536</a></span><span class="t"><span class="str">            [ 0.0000, 0.0000, 0.0000, 2.3298],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1537" class="pln"><span class="n"><a href="#t1537">1537</a></span><span class="t"><span class="str">            [ 0.0000, 0.0000, 0.0000, 0.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1538" class="pln"><span class="n"><a href="#t1538">1538</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1539" class="pln"><span class="n"><a href="#t1539">1539</a></span><span class="t"><span class="str">Get the k-th diagonal of a given matrix::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1540" class="pln"><span class="n"><a href="#t1540">1540</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1541" class="pln"><span class="n"><a href="#t1541">1541</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1542" class="pln"><span class="n"><a href="#t1542">1542</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1543" class="pln"><span class="n"><a href="#t1543">1543</a></span><span class="t"><span class="str">    tensor([[-0.4264, 0.0255,-0.1064],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1544" class="pln"><span class="n"><a href="#t1544">1544</a></span><span class="t"><span class="str">            [ 0.8795,-0.2429, 0.1374],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1545" class="pln"><span class="n"><a href="#t1545">1545</a></span><span class="t"><span class="str">            [ 0.1029,-0.6482,-1.6300]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1546" class="pln"><span class="n"><a href="#t1546">1546</a></span><span class="t"><span class="str">    >>> torch.diag(a, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1547" class="pln"><span class="n"><a href="#t1547">1547</a></span><span class="t"><span class="str">    tensor([-0.4264,-0.2429,-1.6300])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1548" class="pln"><span class="n"><a href="#t1548">1548</a></span><span class="t"><span class="str">    >>> torch.diag(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1549" class="pln"><span class="n"><a href="#t1549">1549</a></span><span class="t"><span class="str">    tensor([ 0.0255, 0.1374])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1550" class="pln"><span class="n"><a href="#t1550">1550</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1551" class="pln"><span class="n"><a href="#t1551">1551</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1552" class="run"><span class="n"><a href="#t1552">1552</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">diag_embed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1553" class="pln"><span class="n"><a href="#t1553">1553</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1554" class="pln"><span class="n"><a href="#t1554">1554</a></span><span class="t"><span class="str">diag_embed(input, offset=0, dim1=-2, dim2=-1) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1555" class="pln"><span class="n"><a href="#t1555">1555</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1556" class="pln"><span class="n"><a href="#t1556">1556</a></span><span class="t"><span class="str">Creates a tensor whose diagonals of certain 2D planes (specified by</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1557" class="pln"><span class="n"><a href="#t1557">1557</a></span><span class="t"><span class="str">:attr:`dim1` and :attr:`dim2`) are filled by :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1558" class="pln"><span class="n"><a href="#t1558">1558</a></span><span class="t"><span class="str">To facilitate creating batched diagonal matrices, the 2D planes formed by</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1559" class="pln"><span class="n"><a href="#t1559">1559</a></span><span class="t"><span class="str">the last two dimensions of the returned tensor are chosen by default.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1560" class="pln"><span class="n"><a href="#t1560">1560</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1561" class="pln"><span class="n"><a href="#t1561">1561</a></span><span class="t"><span class="str">The argument :attr:`offset` controls which diagonal to consider:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1562" class="pln"><span class="n"><a href="#t1562">1562</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1563" class="pln"><span class="n"><a href="#t1563">1563</a></span><span class="t"><span class="str">- If :attr:`offset` = 0, it is the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1564" class="pln"><span class="n"><a href="#t1564">1564</a></span><span class="t"><span class="str">- If :attr:`offset` > 0, it is above the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1565" class="pln"><span class="n"><a href="#t1565">1565</a></span><span class="t"><span class="str">- If :attr:`offset` &lt; 0, it is below the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1566" class="pln"><span class="n"><a href="#t1566">1566</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1567" class="pln"><span class="n"><a href="#t1567">1567</a></span><span class="t"><span class="str">The size of the new matrix will be calculated to make the specified diagonal</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1568" class="pln"><span class="n"><a href="#t1568">1568</a></span><span class="t"><span class="str">of the size of the last input dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1569" class="pln"><span class="n"><a href="#t1569">1569</a></span><span class="t"><span class="str">Note that for :attr:`offset` other than :math:`0`, the order of :attr:`dim1`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1570" class="pln"><span class="n"><a href="#t1570">1570</a></span><span class="t"><span class="str">and :attr:`dim2` matters. Exchanging them is equivalent to changing the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1571" class="pln"><span class="n"><a href="#t1571">1571</a></span><span class="t"><span class="str">sign of :attr:`offset`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1572" class="pln"><span class="n"><a href="#t1572">1572</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1573" class="pln"><span class="n"><a href="#t1573">1573</a></span><span class="t"><span class="str">Applying :meth:`torch.diagonal` to the output of this function with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1574" class="pln"><span class="n"><a href="#t1574">1574</a></span><span class="t"><span class="str">the same arguments yields a matrix identical to input. However,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1575" class="pln"><span class="n"><a href="#t1575">1575</a></span><span class="t"><span class="str">:meth:`torch.diagonal` has different default dimensions, so those</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1576" class="pln"><span class="n"><a href="#t1576">1576</a></span><span class="t"><span class="str">need to be explicitly specified.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1577" class="pln"><span class="n"><a href="#t1577">1577</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1578" class="pln"><span class="n"><a href="#t1578">1578</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1579" class="pln"><span class="n"><a href="#t1579">1579</a></span><span class="t"><span class="str">    {input} Must be at least 1-dimensional.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1580" class="pln"><span class="n"><a href="#t1580">1580</a></span><span class="t"><span class="str">    offset (int, optional): which diagonal to consider. Default: 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1581" class="pln"><span class="n"><a href="#t1581">1581</a></span><span class="t"><span class="str">        (main diagonal).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1582" class="pln"><span class="n"><a href="#t1582">1582</a></span><span class="t"><span class="str">    dim1 (int, optional): first dimension with respect to which to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1583" class="pln"><span class="n"><a href="#t1583">1583</a></span><span class="t"><span class="str">        take diagonal. Default: -2.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1584" class="pln"><span class="n"><a href="#t1584">1584</a></span><span class="t"><span class="str">    dim2 (int, optional): second dimension with respect to which to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1585" class="pln"><span class="n"><a href="#t1585">1585</a></span><span class="t"><span class="str">        take diagonal. Default: -1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1586" class="pln"><span class="n"><a href="#t1586">1586</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1587" class="pln"><span class="n"><a href="#t1587">1587</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1588" class="pln"><span class="n"><a href="#t1588">1588</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1589" class="pln"><span class="n"><a href="#t1589">1589</a></span><span class="t"><span class="str">    >>> a = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1590" class="pln"><span class="n"><a href="#t1590">1590</a></span><span class="t"><span class="str">    >>> torch.diag_embed(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1591" class="pln"><span class="n"><a href="#t1591">1591</a></span><span class="t"><span class="str">    tensor([[[ 1.5410,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1592" class="pln"><span class="n"><a href="#t1592">1592</a></span><span class="t"><span class="str">             [ 0.0000, -0.2934,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1593" class="pln"><span class="n"><a href="#t1593">1593</a></span><span class="t"><span class="str">             [ 0.0000,  0.0000, -2.1788]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1594" class="pln"><span class="n"><a href="#t1594">1594</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1595" class="pln"><span class="n"><a href="#t1595">1595</a></span><span class="t"><span class="str">            [[ 0.5684,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1596" class="pln"><span class="n"><a href="#t1596">1596</a></span><span class="t"><span class="str">             [ 0.0000, -1.0845,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1597" class="pln"><span class="n"><a href="#t1597">1597</a></span><span class="t"><span class="str">             [ 0.0000,  0.0000, -1.3986]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1598" class="pln"><span class="n"><a href="#t1598">1598</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1599" class="pln"><span class="n"><a href="#t1599">1599</a></span><span class="t"><span class="str">    >>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1600" class="pln"><span class="n"><a href="#t1600">1600</a></span><span class="t"><span class="str">    tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1601" class="pln"><span class="n"><a href="#t1601">1601</a></span><span class="t"><span class="str">             [ 0.0000,  0.5684,  0.0000,  0.0000]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1602" class="pln"><span class="n"><a href="#t1602">1602</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1603" class="pln"><span class="n"><a href="#t1603">1603</a></span><span class="t"><span class="str">            [[ 0.0000,  0.0000, -0.2934,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1604" class="pln"><span class="n"><a href="#t1604">1604</a></span><span class="t"><span class="str">             [ 0.0000,  0.0000, -1.0845,  0.0000]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1605" class="pln"><span class="n"><a href="#t1605">1605</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1606" class="pln"><span class="n"><a href="#t1606">1606</a></span><span class="t"><span class="str">            [[ 0.0000,  0.0000,  0.0000, -2.1788],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1607" class="pln"><span class="n"><a href="#t1607">1607</a></span><span class="t"><span class="str">             [ 0.0000,  0.0000,  0.0000, -1.3986]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1608" class="pln"><span class="n"><a href="#t1608">1608</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1609" class="pln"><span class="n"><a href="#t1609">1609</a></span><span class="t"><span class="str">            [[ 0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1610" class="pln"><span class="n"><a href="#t1610">1610</a></span><span class="t"><span class="str">             [ 0.0000,  0.0000,  0.0000,  0.0000]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1611" class="pln"><span class="n"><a href="#t1611">1611</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1612" class="pln"><span class="n"><a href="#t1612">1612</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1613" class="pln"><span class="n"><a href="#t1613">1613</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1614" class="run"><span class="n"><a href="#t1614">1614</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">diagflat</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1615" class="pln"><span class="n"><a href="#t1615">1615</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1616" class="pln"><span class="n"><a href="#t1616">1616</a></span><span class="t"><span class="str">diagflat(input, offset=0) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1617" class="pln"><span class="n"><a href="#t1617">1617</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1618" class="pln"><span class="n"><a href="#t1618">1618</a></span><span class="t"><span class="str">- If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1619" class="pln"><span class="n"><a href="#t1619">1619</a></span><span class="t"><span class="str">  with the elements of :attr:`input` as the diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1620" class="pln"><span class="n"><a href="#t1620">1620</a></span><span class="t"><span class="str">- If :attr:`input` is a tensor with more than one dimension, then returns a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1621" class="pln"><span class="n"><a href="#t1621">1621</a></span><span class="t"><span class="str">  2-D tensor with diagonal elements equal to a flattened :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1622" class="pln"><span class="n"><a href="#t1622">1622</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1623" class="pln"><span class="n"><a href="#t1623">1623</a></span><span class="t"><span class="str">The argument :attr:`offset` controls which diagonal to consider:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1624" class="pln"><span class="n"><a href="#t1624">1624</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1625" class="pln"><span class="n"><a href="#t1625">1625</a></span><span class="t"><span class="str">- If :attr:`offset` = 0, it is the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1626" class="pln"><span class="n"><a href="#t1626">1626</a></span><span class="t"><span class="str">- If :attr:`offset` > 0, it is above the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1627" class="pln"><span class="n"><a href="#t1627">1627</a></span><span class="t"><span class="str">- If :attr:`offset` &lt; 0, it is below the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1628" class="pln"><span class="n"><a href="#t1628">1628</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1629" class="pln"><span class="n"><a href="#t1629">1629</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1630" class="pln"><span class="n"><a href="#t1630">1630</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1631" class="pln"><span class="n"><a href="#t1631">1631</a></span><span class="t"><span class="str">    offset (int, optional): the diagonal to consider. Default: 0 (main</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1632" class="pln"><span class="n"><a href="#t1632">1632</a></span><span class="t"><span class="str">        diagonal).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1633" class="pln"><span class="n"><a href="#t1633">1633</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1634" class="pln"><span class="n"><a href="#t1634">1634</a></span><span class="t"><span class="str">Examples::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1635" class="pln"><span class="n"><a href="#t1635">1635</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1636" class="pln"><span class="n"><a href="#t1636">1636</a></span><span class="t"><span class="str">    >>> a = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1637" class="pln"><span class="n"><a href="#t1637">1637</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1638" class="pln"><span class="n"><a href="#t1638">1638</a></span><span class="t"><span class="str">    tensor([-0.2956, -0.9068,  0.1695])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1639" class="pln"><span class="n"><a href="#t1639">1639</a></span><span class="t"><span class="str">    >>> torch.diagflat(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1640" class="pln"><span class="n"><a href="#t1640">1640</a></span><span class="t"><span class="str">    tensor([[-0.2956,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1641" class="pln"><span class="n"><a href="#t1641">1641</a></span><span class="t"><span class="str">            [ 0.0000, -0.9068,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1642" class="pln"><span class="n"><a href="#t1642">1642</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.1695]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1643" class="pln"><span class="n"><a href="#t1643">1643</a></span><span class="t"><span class="str">    >>> torch.diagflat(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1644" class="pln"><span class="n"><a href="#t1644">1644</a></span><span class="t"><span class="str">    tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1645" class="pln"><span class="n"><a href="#t1645">1645</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000, -0.9068,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1646" class="pln"><span class="n"><a href="#t1646">1646</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000,  0.1695],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1647" class="pln"><span class="n"><a href="#t1647">1647</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000,  0.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1648" class="pln"><span class="n"><a href="#t1648">1648</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1649" class="pln"><span class="n"><a href="#t1649">1649</a></span><span class="t"><span class="str">    >>> a = torch.randn(2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1650" class="pln"><span class="n"><a href="#t1650">1650</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1651" class="pln"><span class="n"><a href="#t1651">1651</a></span><span class="t"><span class="str">    tensor([[ 0.2094, -0.3018],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1652" class="pln"><span class="n"><a href="#t1652">1652</a></span><span class="t"><span class="str">            [-0.1516,  1.9342]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1653" class="pln"><span class="n"><a href="#t1653">1653</a></span><span class="t"><span class="str">    >>> torch.diagflat(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1654" class="pln"><span class="n"><a href="#t1654">1654</a></span><span class="t"><span class="str">    tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1655" class="pln"><span class="n"><a href="#t1655">1655</a></span><span class="t"><span class="str">            [ 0.0000, -0.3018,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1656" class="pln"><span class="n"><a href="#t1656">1656</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000, -0.1516,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1657" class="pln"><span class="n"><a href="#t1657">1657</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000,  1.9342]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1658" class="pln"><span class="n"><a href="#t1658">1658</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1659" class="pln"><span class="n"><a href="#t1659">1659</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1660" class="run"><span class="n"><a href="#t1660">1660</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">diagonal</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1661" class="pln"><span class="n"><a href="#t1661">1661</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1662" class="pln"><span class="n"><a href="#t1662">1662</a></span><span class="t"><span class="str">diagonal(input, offset=0, dim1=0, dim2=1) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1663" class="pln"><span class="n"><a href="#t1663">1663</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1664" class="pln"><span class="n"><a href="#t1664">1664</a></span><span class="t"><span class="str">Returns a partial view of :attr:`input` with the its diagonal elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1665" class="pln"><span class="n"><a href="#t1665">1665</a></span><span class="t"><span class="str">with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1666" class="pln"><span class="n"><a href="#t1666">1666</a></span><span class="t"><span class="str">at the end of the shape.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1667" class="pln"><span class="n"><a href="#t1667">1667</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1668" class="pln"><span class="n"><a href="#t1668">1668</a></span><span class="t"><span class="str">The argument :attr:`offset` controls which diagonal to consider:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1669" class="pln"><span class="n"><a href="#t1669">1669</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1670" class="pln"><span class="n"><a href="#t1670">1670</a></span><span class="t"><span class="str">- If :attr:`offset` = 0, it is the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1671" class="pln"><span class="n"><a href="#t1671">1671</a></span><span class="t"><span class="str">- If :attr:`offset` > 0, it is above the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1672" class="pln"><span class="n"><a href="#t1672">1672</a></span><span class="t"><span class="str">- If :attr:`offset` &lt; 0, it is below the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1673" class="pln"><span class="n"><a href="#t1673">1673</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1674" class="pln"><span class="n"><a href="#t1674">1674</a></span><span class="t"><span class="str">Applying :meth:`torch.diag_embed` to the output of this function with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1675" class="pln"><span class="n"><a href="#t1675">1675</a></span><span class="t"><span class="str">the same arguments yields a diagonal matrix with the diagonal entries</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1676" class="pln"><span class="n"><a href="#t1676">1676</a></span><span class="t"><span class="str">of the input. However, :meth:`torch.diag_embed` has different default</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1677" class="pln"><span class="n"><a href="#t1677">1677</a></span><span class="t"><span class="str">dimensions, so those need to be explicitly specified.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1678" class="pln"><span class="n"><a href="#t1678">1678</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1679" class="pln"><span class="n"><a href="#t1679">1679</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1680" class="pln"><span class="n"><a href="#t1680">1680</a></span><span class="t"><span class="str">    {input} Must be at least 2-dimensional.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1681" class="pln"><span class="n"><a href="#t1681">1681</a></span><span class="t"><span class="str">    offset (int, optional): which diagonal to consider. Default: 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1682" class="pln"><span class="n"><a href="#t1682">1682</a></span><span class="t"><span class="str">        (main diagonal).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1683" class="pln"><span class="n"><a href="#t1683">1683</a></span><span class="t"><span class="str">    dim1 (int, optional): first dimension with respect to which to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1684" class="pln"><span class="n"><a href="#t1684">1684</a></span><span class="t"><span class="str">        take diagonal. Default: 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1685" class="pln"><span class="n"><a href="#t1685">1685</a></span><span class="t"><span class="str">    dim2 (int, optional): second dimension with respect to which to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1686" class="pln"><span class="n"><a href="#t1686">1686</a></span><span class="t"><span class="str">        take diagonal. Default: 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1687" class="pln"><span class="n"><a href="#t1687">1687</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1688" class="pln"><span class="n"><a href="#t1688">1688</a></span><span class="t"><span class="str">.. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1689" class="pln"><span class="n"><a href="#t1689">1689</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1690" class="pln"><span class="n"><a href="#t1690">1690</a></span><span class="t"><span class="str">Examples::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1691" class="pln"><span class="n"><a href="#t1691">1691</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1692" class="pln"><span class="n"><a href="#t1692">1692</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1693" class="pln"><span class="n"><a href="#t1693">1693</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1694" class="pln"><span class="n"><a href="#t1694">1694</a></span><span class="t"><span class="str">    tensor([[-1.0854,  1.1431, -0.1752],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1695" class="pln"><span class="n"><a href="#t1695">1695</a></span><span class="t"><span class="str">            [ 0.8536, -0.0905,  0.0360],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1696" class="pln"><span class="n"><a href="#t1696">1696</a></span><span class="t"><span class="str">            [ 0.6927, -0.3735, -0.4945]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1697" class="pln"><span class="n"><a href="#t1697">1697</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1698" class="pln"><span class="n"><a href="#t1698">1698</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1699" class="pln"><span class="n"><a href="#t1699">1699</a></span><span class="t"><span class="str">    >>> torch.diagonal(a, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1700" class="pln"><span class="n"><a href="#t1700">1700</a></span><span class="t"><span class="str">    tensor([-1.0854, -0.0905, -0.4945])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1701" class="pln"><span class="n"><a href="#t1701">1701</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1702" class="pln"><span class="n"><a href="#t1702">1702</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1703" class="pln"><span class="n"><a href="#t1703">1703</a></span><span class="t"><span class="str">    >>> torch.diagonal(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1704" class="pln"><span class="n"><a href="#t1704">1704</a></span><span class="t"><span class="str">    tensor([ 1.1431,  0.0360])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1705" class="pln"><span class="n"><a href="#t1705">1705</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1706" class="pln"><span class="n"><a href="#t1706">1706</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1707" class="pln"><span class="n"><a href="#t1707">1707</a></span><span class="t"><span class="str">    >>> x = torch.randn(2, 5, 4, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1708" class="pln"><span class="n"><a href="#t1708">1708</a></span><span class="t"><span class="str">    >>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1709" class="pln"><span class="n"><a href="#t1709">1709</a></span><span class="t"><span class="str">    tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1710" class="pln"><span class="n"><a href="#t1710">1710</a></span><span class="t"><span class="str">             [-1.1065,  1.0401, -0.2235, -0.7938]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1711" class="pln"><span class="n"><a href="#t1711">1711</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1712" class="pln"><span class="n"><a href="#t1712">1712</a></span><span class="t"><span class="str">            [[-1.7325, -0.3081,  0.6166,  0.2335],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1713" class="pln"><span class="n"><a href="#t1713">1713</a></span><span class="t"><span class="str">             [ 1.0500,  0.7336, -0.3836, -1.1015]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1714" class="pln"><span class="n"><a href="#t1714">1714</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1715" class="pln"><span class="n"><a href="#t1715">1715</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1716" class="run"><span class="n"><a href="#t1716">1716</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">digamma</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1717" class="pln"><span class="n"><a href="#t1717">1717</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1718" class="pln"><span class="n"><a href="#t1718">1718</a></span><span class="t"><span class="str">digamma(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1719" class="pln"><span class="n"><a href="#t1719">1719</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1720" class="pln"><span class="n"><a href="#t1720">1720</a></span><span class="t"><span class="str">Computes the logarithmic derivative of the gamma function on `input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1721" class="pln"><span class="n"><a href="#t1721">1721</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1722" class="pln"><span class="n"><a href="#t1722">1722</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1723" class="pln"><span class="n"><a href="#t1723">1723</a></span><span class="t"><span class="str">    \psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1724" class="pln"><span class="n"><a href="#t1724">1724</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1725" class="pln"><span class="n"><a href="#t1725">1725</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1726" class="pln"><span class="n"><a href="#t1726">1726</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compute the digamma function on</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1727" class="pln"><span class="n"><a href="#t1727">1727</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1728" class="pln"><span class="n"><a href="#t1728">1728</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1729" class="pln"><span class="n"><a href="#t1729">1729</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1730" class="pln"><span class="n"><a href="#t1730">1730</a></span><span class="t"><span class="str">    >>> a = torch.tensor([1, 0.5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1731" class="pln"><span class="n"><a href="#t1731">1731</a></span><span class="t"><span class="str">    >>> torch.digamma(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1732" class="pln"><span class="n"><a href="#t1732">1732</a></span><span class="t"><span class="str">    tensor([-0.5772, -1.9635])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1733" class="pln"><span class="n"><a href="#t1733">1733</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1734" class="pln"><span class="n"><a href="#t1734">1734</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1735" class="pln"><span class="n"><a href="#t1735">1735</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1736" class="run"><span class="n"><a href="#t1736">1736</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">dist</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1737" class="pln"><span class="n"><a href="#t1737">1737</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1738" class="pln"><span class="n"><a href="#t1738">1738</a></span><span class="t"><span class="str">dist(input, other, p=2) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1739" class="pln"><span class="n"><a href="#t1739">1739</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1740" class="pln"><span class="n"><a href="#t1740">1740</a></span><span class="t"><span class="str">Returns the p-norm of (:attr:`input` - :attr:`other`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1741" class="pln"><span class="n"><a href="#t1741">1741</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1742" class="pln"><span class="n"><a href="#t1742">1742</a></span><span class="t"><span class="str">The shapes of :attr:`input` and :attr:`other` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1743" class="pln"><span class="n"><a href="#t1743">1743</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1744" class="pln"><span class="n"><a href="#t1744">1744</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1745" class="pln"><span class="n"><a href="#t1745">1745</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1746" class="pln"><span class="n"><a href="#t1746">1746</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1747" class="pln"><span class="n"><a href="#t1747">1747</a></span><span class="t"><span class="str">    other (Tensor): the Right-hand-side input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1748" class="pln"><span class="n"><a href="#t1748">1748</a></span><span class="t"><span class="str">    p (float, optional): the norm to be computed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1749" class="pln"><span class="n"><a href="#t1749">1749</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1750" class="pln"><span class="n"><a href="#t1750">1750</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1751" class="pln"><span class="n"><a href="#t1751">1751</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1752" class="pln"><span class="n"><a href="#t1752">1752</a></span><span class="t"><span class="str">    >>> x = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1753" class="pln"><span class="n"><a href="#t1753">1753</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1754" class="pln"><span class="n"><a href="#t1754">1754</a></span><span class="t"><span class="str">    tensor([-1.5393, -0.8675,  0.5916,  1.6321])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1755" class="pln"><span class="n"><a href="#t1755">1755</a></span><span class="t"><span class="str">    >>> y = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1756" class="pln"><span class="n"><a href="#t1756">1756</a></span><span class="t"><span class="str">    >>> y</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1757" class="pln"><span class="n"><a href="#t1757">1757</a></span><span class="t"><span class="str">    tensor([ 0.0967, -1.0511,  0.6295,  0.8360])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1758" class="pln"><span class="n"><a href="#t1758">1758</a></span><span class="t"><span class="str">    >>> torch.dist(x, y, 3.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1759" class="pln"><span class="n"><a href="#t1759">1759</a></span><span class="t"><span class="str">    tensor(1.6727)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1760" class="pln"><span class="n"><a href="#t1760">1760</a></span><span class="t"><span class="str">    >>> torch.dist(x, y, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1761" class="pln"><span class="n"><a href="#t1761">1761</a></span><span class="t"><span class="str">    tensor(1.6973)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1762" class="pln"><span class="n"><a href="#t1762">1762</a></span><span class="t"><span class="str">    >>> torch.dist(x, y, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1763" class="pln"><span class="n"><a href="#t1763">1763</a></span><span class="t"><span class="str">    tensor(inf)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1764" class="pln"><span class="n"><a href="#t1764">1764</a></span><span class="t"><span class="str">    >>> torch.dist(x, y, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1765" class="pln"><span class="n"><a href="#t1765">1765</a></span><span class="t"><span class="str">    tensor(2.6537)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1766" class="pln"><span class="n"><a href="#t1766">1766</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1767" class="pln"><span class="n"><a href="#t1767">1767</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1768" class="run"><span class="n"><a href="#t1768">1768</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">div</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1769" class="pln"><span class="n"><a href="#t1769">1769</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1770" class="pln"><span class="n"><a href="#t1770">1770</a></span><span class="t"><span class="str">.. function:: div(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1771" class="pln"><span class="n"><a href="#t1771">1771</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1772" class="pln"><span class="n"><a href="#t1772">1772</a></span><span class="t"><span class="str">Divides each element of the input ``input`` with the scalar ``other`` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1773" class="pln"><span class="n"><a href="#t1773">1773</a></span><span class="t"><span class="str">returns a new resulting tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1774" class="pln"><span class="n"><a href="#t1774">1774</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1775" class="pln"><span class="n"><a href="#t1775">1775</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1776" class="pln"><span class="n"><a href="#t1776">1776</a></span><span class="t"><span class="str">    \text{{out}}_i = \frac{{\text{{input}}_i}}{{\text{{other}}}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1777" class="pln"><span class="n"><a href="#t1777">1777</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1778" class="pln"><span class="n"><a href="#t1778">1778</a></span><span class="t"><span class="str">If the :class:`torch.dtype` of ``input`` and ``other`` differ, the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1779" class="pln"><span class="n"><a href="#t1779">1779</a></span><span class="t"><span class="str">:class:`torch.dtype` of the result tensor is determined following rules</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1780" class="pln"><span class="n"><a href="#t1780">1780</a></span><span class="t"><span class="str">described in the type promotion :ref:`documentation &lt;type-promotion-doc>`. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1781" class="pln"><span class="n"><a href="#t1781">1781</a></span><span class="t"><span class="str">``out`` is specified, the result must be :ref:`castable &lt;type-promotion-doc>`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1782" class="pln"><span class="n"><a href="#t1782">1782</a></span><span class="t"><span class="str">to the :class:`torch.dtype` of the specified output tensor. Integral division</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1783" class="pln"><span class="n"><a href="#t1783">1783</a></span><span class="t"><span class="str">by zero leads to undefined behavior.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1784" class="pln"><span class="n"><a href="#t1784">1784</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1785" class="pln"><span class="n"><a href="#t1785">1785</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1786" class="pln"><span class="n"><a href="#t1786">1786</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1787" class="pln"><span class="n"><a href="#t1787">1787</a></span><span class="t"><span class="str">    other (Number): the number to be divided to each element of ``input``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1788" class="pln"><span class="n"><a href="#t1788">1788</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1789" class="pln"><span class="n"><a href="#t1789">1789</a></span><span class="t"><span class="str">Keyword args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1790" class="pln"><span class="n"><a href="#t1790">1790</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1791" class="pln"><span class="n"><a href="#t1791">1791</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1792" class="pln"><span class="n"><a href="#t1792">1792</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1793" class="pln"><span class="n"><a href="#t1793">1793</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1794" class="pln"><span class="n"><a href="#t1794">1794</a></span><span class="t"><span class="str">    >>> a = torch.randn(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1795" class="pln"><span class="n"><a href="#t1795">1795</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1796" class="pln"><span class="n"><a href="#t1796">1796</a></span><span class="t"><span class="str">    tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1797" class="pln"><span class="n"><a href="#t1797">1797</a></span><span class="t"><span class="str">    >>> torch.div(a, 0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1798" class="pln"><span class="n"><a href="#t1798">1798</a></span><span class="t"><span class="str">    tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1799" class="pln"><span class="n"><a href="#t1799">1799</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1800" class="pln"><span class="n"><a href="#t1800">1800</a></span><span class="t"><span class="str">.. function:: div(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1801" class="pln"><span class="n"><a href="#t1801">1801</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1802" class="pln"><span class="n"><a href="#t1802">1802</a></span><span class="t"><span class="str">Each element of the tensor ``input`` is divided by each element of the tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1803" class="pln"><span class="n"><a href="#t1803">1803</a></span><span class="t"><span class="str">``other``. The resulting tensor is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1804" class="pln"><span class="n"><a href="#t1804">1804</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1805" class="pln"><span class="n"><a href="#t1805">1805</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1806" class="pln"><span class="n"><a href="#t1806">1806</a></span><span class="t"><span class="str">    \text{{out}}_i = \frac{{\text{{input}}_i}}{{\text{{other}}_i}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1807" class="pln"><span class="n"><a href="#t1807">1807</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1808" class="pln"><span class="n"><a href="#t1808">1808</a></span><span class="t"><span class="str">The shapes of ``input`` and ``other`` must be :ref:`broadcastable</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1809" class="pln"><span class="n"><a href="#t1809">1809</a></span><span class="t"><span class="str">&lt;broadcasting-semantics>`. If the :class:`torch.dtype` of ``input`` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1810" class="pln"><span class="n"><a href="#t1810">1810</a></span><span class="t"><span class="str">``other`` differ, the :class:`torch.dtype` of the result tensor is determined</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1811" class="pln"><span class="n"><a href="#t1811">1811</a></span><span class="t"><span class="str">following rules described in the type promotion :ref:`documentation</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1812" class="pln"><span class="n"><a href="#t1812">1812</a></span><span class="t"><span class="str">&lt;type-promotion-doc>`. If ``out`` is specified, the result must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1813" class="pln"><span class="n"><a href="#t1813">1813</a></span><span class="t"><span class="str">:ref:`castable &lt;type-promotion-doc>` to the :class:`torch.dtype` of the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1814" class="pln"><span class="n"><a href="#t1814">1814</a></span><span class="t"><span class="str">specified output tensor. Integral division by zero leads to undefined behavior.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1815" class="pln"><span class="n"><a href="#t1815">1815</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1816" class="pln"><span class="n"><a href="#t1816">1816</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1817" class="pln"><span class="n"><a href="#t1817">1817</a></span><span class="t"><span class="str">    input (Tensor): the numerator tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1818" class="pln"><span class="n"><a href="#t1818">1818</a></span><span class="t"><span class="str">    other (Tensor): the denominator tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1819" class="pln"><span class="n"><a href="#t1819">1819</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1820" class="pln"><span class="n"><a href="#t1820">1820</a></span><span class="t"><span class="str">Keyword args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1821" class="pln"><span class="n"><a href="#t1821">1821</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1822" class="pln"><span class="n"><a href="#t1822">1822</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1823" class="pln"><span class="n"><a href="#t1823">1823</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1824" class="pln"><span class="n"><a href="#t1824">1824</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1825" class="pln"><span class="n"><a href="#t1825">1825</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1826" class="pln"><span class="n"><a href="#t1826">1826</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1827" class="pln"><span class="n"><a href="#t1827">1827</a></span><span class="t"><span class="str">    tensor([[-0.3711, -1.9353, -0.4605, -0.2917],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1828" class="pln"><span class="n"><a href="#t1828">1828</a></span><span class="t"><span class="str">            [ 0.1815, -1.0111,  0.9805, -1.5923],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1829" class="pln"><span class="n"><a href="#t1829">1829</a></span><span class="t"><span class="str">            [ 0.1062,  1.4581,  0.7759, -1.2344],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1830" class="pln"><span class="n"><a href="#t1830">1830</a></span><span class="t"><span class="str">            [-0.1830, -0.0313,  1.1908, -1.4757]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1831" class="pln"><span class="n"><a href="#t1831">1831</a></span><span class="t"><span class="str">    >>> b = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1832" class="pln"><span class="n"><a href="#t1832">1832</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1833" class="pln"><span class="n"><a href="#t1833">1833</a></span><span class="t"><span class="str">    tensor([ 0.8032,  0.2930, -0.8113, -0.2308])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1834" class="pln"><span class="n"><a href="#t1834">1834</a></span><span class="t"><span class="str">    >>> torch.div(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1835" class="pln"><span class="n"><a href="#t1835">1835</a></span><span class="t"><span class="str">    tensor([[-0.4620, -6.6051,  0.5676,  1.2637],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1836" class="pln"><span class="n"><a href="#t1836">1836</a></span><span class="t"><span class="str">            [ 0.2260, -3.4507, -1.2086,  6.8988],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1837" class="pln"><span class="n"><a href="#t1837">1837</a></span><span class="t"><span class="str">            [ 0.1322,  4.9764, -0.9564,  5.3480],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1838" class="pln"><span class="n"><a href="#t1838">1838</a></span><span class="t"><span class="str">            [-0.2278, -0.1068, -1.4678,  6.3936]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1839" class="pln"><span class="n"><a href="#t1839">1839</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1840" class="pln"><span class="n"><a href="#t1840">1840</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1841" class="run"><span class="n"><a href="#t1841">1841</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">dot</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1842" class="pln"><span class="n"><a href="#t1842">1842</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1843" class="pln"><span class="n"><a href="#t1843">1843</a></span><span class="t"><span class="str">dot(input, tensor) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1844" class="pln"><span class="n"><a href="#t1844">1844</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1845" class="pln"><span class="n"><a href="#t1845">1845</a></span><span class="t"><span class="str">Computes the dot product (inner product) of two tensors.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1846" class="pln"><span class="n"><a href="#t1846">1846</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1847" class="pln"><span class="n"><a href="#t1847">1847</a></span><span class="t"><span class="str">.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1848" class="pln"><span class="n"><a href="#t1848">1848</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1849" class="pln"><span class="n"><a href="#t1849">1849</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1850" class="pln"><span class="n"><a href="#t1850">1850</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1851" class="pln"><span class="n"><a href="#t1851">1851</a></span><span class="t"><span class="str">    >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1852" class="pln"><span class="n"><a href="#t1852">1852</a></span><span class="t"><span class="str">    tensor(7)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1853" class="pln"><span class="n"><a href="#t1853">1853</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1854" class="pln"><span class="n"><a href="#t1854">1854</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1855" class="run"><span class="n"><a href="#t1855">1855</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">eig</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1856" class="pln"><span class="n"><a href="#t1856">1856</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1857" class="pln"><span class="n"><a href="#t1857">1857</a></span><span class="t"><span class="str">eig(input, eigenvectors=False, out=None) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1858" class="pln"><span class="n"><a href="#t1858">1858</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1859" class="pln"><span class="n"><a href="#t1859">1859</a></span><span class="t"><span class="str">Computes the eigenvalues and eigenvectors of a real square matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1860" class="pln"><span class="n"><a href="#t1860">1860</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1861" class="pln"><span class="n"><a href="#t1861">1861</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1862" class="pln"><span class="n"><a href="#t1862">1862</a></span><span class="t"><span class="str">    Since eigenvalues and eigenvectors might be complex, backward pass is supported only</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1863" class="pln"><span class="n"><a href="#t1863">1863</a></span><span class="t"><span class="str">    for :func:`torch.symeig`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1864" class="pln"><span class="n"><a href="#t1864">1864</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1865" class="pln"><span class="n"><a href="#t1865">1865</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1866" class="pln"><span class="n"><a href="#t1866">1866</a></span><span class="t"><span class="str">    input (Tensor): the square matrix of shape :math:`(n \times n)` for which the eigenvalues and eigenvectors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1867" class="pln"><span class="n"><a href="#t1867">1867</a></span><span class="t"><span class="str">        will be computed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1868" class="pln"><span class="n"><a href="#t1868">1868</a></span><span class="t"><span class="str">    eigenvectors (bool): ``True`` to compute both eigenvalues and eigenvectors;</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1869" class="pln"><span class="n"><a href="#t1869">1869</a></span><span class="t"><span class="str">        otherwise, only eigenvalues will be computed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1870" class="pln"><span class="n"><a href="#t1870">1870</a></span><span class="t"><span class="str">    out (tuple, optional): the output tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1871" class="pln"><span class="n"><a href="#t1871">1871</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1872" class="pln"><span class="n"><a href="#t1872">1872</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1873" class="pln"><span class="n"><a href="#t1873">1873</a></span><span class="t"><span class="str">    (Tensor, Tensor): A namedtuple (eigenvalues, eigenvectors) containing</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1874" class="pln"><span class="n"><a href="#t1874">1874</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1875" class="pln"><span class="n"><a href="#t1875">1875</a></span><span class="t"><span class="str">        - **eigenvalues** (*Tensor*): Shape :math:`(n \times 2)`. Each row is an eigenvalue of ``input``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1876" class="pln"><span class="n"><a href="#t1876">1876</a></span><span class="t"><span class="str">          where the first element is the real part and the second element is the imaginary part.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1877" class="pln"><span class="n"><a href="#t1877">1877</a></span><span class="t"><span class="str">          The eigenvalues are not necessarily ordered.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1878" class="pln"><span class="n"><a href="#t1878">1878</a></span><span class="t"><span class="str">        - **eigenvectors** (*Tensor*): If ``eigenvectors=False``, it's an empty tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1879" class="pln"><span class="n"><a href="#t1879">1879</a></span><span class="t"><span class="str">          Otherwise, this tensor of shape :math:`(n \times n)` can be used to compute normalized (unit length)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1880" class="pln"><span class="n"><a href="#t1880">1880</a></span><span class="t"><span class="str">          eigenvectors of corresponding eigenvalues as follows.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1881" class="pln"><span class="n"><a href="#t1881">1881</a></span><span class="t"><span class="str">          If the corresponding `eigenvalues[j]` is a real number, column `eigenvectors[:, j]` is the eigenvector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1882" class="pln"><span class="n"><a href="#t1882">1882</a></span><span class="t"><span class="str">          corresponding to `eigenvalues[j]`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1883" class="pln"><span class="n"><a href="#t1883">1883</a></span><span class="t"><span class="str">          If the corresponding `eigenvalues[j]` and `eigenvalues[j + 1]` form a complex conjugate pair, then the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1884" class="pln"><span class="n"><a href="#t1884">1884</a></span><span class="t"><span class="str">          true eigenvectors can be computed as</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1885" class="pln"><span class="n"><a href="#t1885">1885</a></span><span class="t"><span class="str">          :math:`\text{true eigenvector}[j] = eigenvectors[:, j] + i \times eigenvectors[:, j + 1]`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1886" class="pln"><span class="n"><a href="#t1886">1886</a></span><span class="t"><span class="str">          :math:`\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \times eigenvectors[:, j + 1]`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1887" class="pln"><span class="n"><a href="#t1887">1887</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1888" class="pln"><span class="n"><a href="#t1888">1888</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1889" class="run"><span class="n"><a href="#t1889">1889</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">eq</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1890" class="pln"><span class="n"><a href="#t1890">1890</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1891" class="pln"><span class="n"><a href="#t1891">1891</a></span><span class="t"><span class="str">eq(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1892" class="pln"><span class="n"><a href="#t1892">1892</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1893" class="pln"><span class="n"><a href="#t1893">1893</a></span><span class="t"><span class="str">Computes element-wise equality</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1894" class="pln"><span class="n"><a href="#t1894">1894</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1895" class="pln"><span class="n"><a href="#t1895">1895</a></span><span class="t"><span class="str">The second argument can be a number or a tensor whose shape is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1896" class="pln"><span class="n"><a href="#t1896">1896</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with the first argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1897" class="pln"><span class="n"><a href="#t1897">1897</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1898" class="pln"><span class="n"><a href="#t1898">1898</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1899" class="pln"><span class="n"><a href="#t1899">1899</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1900" class="pln"><span class="n"><a href="#t1900">1900</a></span><span class="t"><span class="str">    other (Tensor or float): the tensor or value to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1901" class="pln"><span class="n"><a href="#t1901">1901</a></span><span class="t"><span class="str">    {out} Must be a `ByteTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1902" class="pln"><span class="n"><a href="#t1902">1902</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1903" class="pln"><span class="n"><a href="#t1903">1903</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1904" class="pln"><span class="n"><a href="#t1904">1904</a></span><span class="t"><span class="str">    Tensor: A ``torch.BoolTensor`` containing a True at each location where comparison is true</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1905" class="pln"><span class="n"><a href="#t1905">1905</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1906" class="pln"><span class="n"><a href="#t1906">1906</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1907" class="pln"><span class="n"><a href="#t1907">1907</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1908" class="pln"><span class="n"><a href="#t1908">1908</a></span><span class="t"><span class="str">    >>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1909" class="pln"><span class="n"><a href="#t1909">1909</a></span><span class="t"><span class="str">    tensor([[ True, False],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1910" class="pln"><span class="n"><a href="#t1910">1910</a></span><span class="t"><span class="str">            [False, True]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1911" class="pln"><span class="n"><a href="#t1911">1911</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1912" class="pln"><span class="n"><a href="#t1912">1912</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1913" class="run"><span class="n"><a href="#t1913">1913</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">equal</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1914" class="pln"><span class="n"><a href="#t1914">1914</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1915" class="pln"><span class="n"><a href="#t1915">1915</a></span><span class="t"><span class="str">equal(input, other) -> bool</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1916" class="pln"><span class="n"><a href="#t1916">1916</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1917" class="pln"><span class="n"><a href="#t1917">1917</a></span><span class="t"><span class="str">``True`` if two tensors have the same size and elements, ``False`` otherwise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1918" class="pln"><span class="n"><a href="#t1918">1918</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1919" class="pln"><span class="n"><a href="#t1919">1919</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1920" class="pln"><span class="n"><a href="#t1920">1920</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1921" class="pln"><span class="n"><a href="#t1921">1921</a></span><span class="t"><span class="str">    >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1922" class="pln"><span class="n"><a href="#t1922">1922</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1923" class="pln"><span class="n"><a href="#t1923">1923</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1924" class="pln"><span class="n"><a href="#t1924">1924</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1925" class="run"><span class="n"><a href="#t1925">1925</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">erf</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1926" class="pln"><span class="n"><a href="#t1926">1926</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1927" class="pln"><span class="n"><a href="#t1927">1927</a></span><span class="t"><span class="str">erf(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1928" class="pln"><span class="n"><a href="#t1928">1928</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1929" class="pln"><span class="n"><a href="#t1929">1929</a></span><span class="t"><span class="str">Computes the error function of each element. The error function is defined as follows:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1930" class="pln"><span class="n"><a href="#t1930">1930</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1931" class="pln"><span class="n"><a href="#t1931">1931</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1932" class="pln"><span class="n"><a href="#t1932">1932</a></span><span class="t"><span class="str">    \mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1933" class="pln"><span class="n"><a href="#t1933">1933</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1934" class="pln"><span class="n"><a href="#t1934">1934</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1935" class="pln"><span class="n"><a href="#t1935">1935</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1936" class="pln"><span class="n"><a href="#t1936">1936</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1937" class="pln"><span class="n"><a href="#t1937">1937</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1938" class="pln"><span class="n"><a href="#t1938">1938</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1939" class="pln"><span class="n"><a href="#t1939">1939</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1940" class="pln"><span class="n"><a href="#t1940">1940</a></span><span class="t"><span class="str">    >>> torch.erf(torch.tensor([0, -1., 10.]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1941" class="pln"><span class="n"><a href="#t1941">1941</a></span><span class="t"><span class="str">    tensor([ 0.0000, -0.8427,  1.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1942" class="pln"><span class="n"><a href="#t1942">1942</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1943" class="pln"><span class="n"><a href="#t1943">1943</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1944" class="run"><span class="n"><a href="#t1944">1944</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">erfc</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1945" class="pln"><span class="n"><a href="#t1945">1945</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1946" class="pln"><span class="n"><a href="#t1946">1946</a></span><span class="t"><span class="str">erfc(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1947" class="pln"><span class="n"><a href="#t1947">1947</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1948" class="pln"><span class="n"><a href="#t1948">1948</a></span><span class="t"><span class="str">Computes the complementary error function of each element of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1949" class="pln"><span class="n"><a href="#t1949">1949</a></span><span class="t"><span class="str">The complementary error function is defined as follows:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1950" class="pln"><span class="n"><a href="#t1950">1950</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1951" class="pln"><span class="n"><a href="#t1951">1951</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1952" class="pln"><span class="n"><a href="#t1952">1952</a></span><span class="t"><span class="str">    \mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1953" class="pln"><span class="n"><a href="#t1953">1953</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1954" class="pln"><span class="n"><a href="#t1954">1954</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1955" class="pln"><span class="n"><a href="#t1955">1955</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1956" class="pln"><span class="n"><a href="#t1956">1956</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1957" class="pln"><span class="n"><a href="#t1957">1957</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1958" class="pln"><span class="n"><a href="#t1958">1958</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1959" class="pln"><span class="n"><a href="#t1959">1959</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1960" class="pln"><span class="n"><a href="#t1960">1960</a></span><span class="t"><span class="str">    >>> torch.erfc(torch.tensor([0, -1., 10.]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1961" class="pln"><span class="n"><a href="#t1961">1961</a></span><span class="t"><span class="str">    tensor([ 1.0000, 1.8427,  0.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1962" class="pln"><span class="n"><a href="#t1962">1962</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1963" class="pln"><span class="n"><a href="#t1963">1963</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1964" class="run"><span class="n"><a href="#t1964">1964</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">erfinv</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1965" class="pln"><span class="n"><a href="#t1965">1965</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1966" class="pln"><span class="n"><a href="#t1966">1966</a></span><span class="t"><span class="str">erfinv(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1967" class="pln"><span class="n"><a href="#t1967">1967</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1968" class="pln"><span class="n"><a href="#t1968">1968</a></span><span class="t"><span class="str">Computes the inverse error function of each element of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1969" class="pln"><span class="n"><a href="#t1969">1969</a></span><span class="t"><span class="str">The inverse error function is defined in the range :math:`(-1, 1)` as:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1970" class="pln"><span class="n"><a href="#t1970">1970</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1971" class="pln"><span class="n"><a href="#t1971">1971</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1972" class="pln"><span class="n"><a href="#t1972">1972</a></span><span class="t"><span class="str">    \mathrm{erfinv}(\mathrm{erf}(x)) = x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1973" class="pln"><span class="n"><a href="#t1973">1973</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1974" class="pln"><span class="n"><a href="#t1974">1974</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1975" class="pln"><span class="n"><a href="#t1975">1975</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1976" class="pln"><span class="n"><a href="#t1976">1976</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1977" class="pln"><span class="n"><a href="#t1977">1977</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1978" class="pln"><span class="n"><a href="#t1978">1978</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1979" class="pln"><span class="n"><a href="#t1979">1979</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1980" class="pln"><span class="n"><a href="#t1980">1980</a></span><span class="t"><span class="str">    >>> torch.erfinv(torch.tensor([0, 0.5, -1.]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1981" class="pln"><span class="n"><a href="#t1981">1981</a></span><span class="t"><span class="str">    tensor([ 0.0000,  0.4769,    -inf])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1982" class="pln"><span class="n"><a href="#t1982">1982</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1983" class="pln"><span class="n"><a href="#t1983">1983</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1984" class="run"><span class="n"><a href="#t1984">1984</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">exp</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1985" class="pln"><span class="n"><a href="#t1985">1985</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1986" class="pln"><span class="n"><a href="#t1986">1986</a></span><span class="t"><span class="str">exp(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1987" class="pln"><span class="n"><a href="#t1987">1987</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1988" class="pln"><span class="n"><a href="#t1988">1988</a></span><span class="t"><span class="str">Returns a new tensor with the exponential of the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1989" class="pln"><span class="n"><a href="#t1989">1989</a></span><span class="t"><span class="str">of the input tensor :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1990" class="pln"><span class="n"><a href="#t1990">1990</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1991" class="pln"><span class="n"><a href="#t1991">1991</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1992" class="pln"><span class="n"><a href="#t1992">1992</a></span><span class="t"><span class="str">    y_{i} = e^{x_{i}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1993" class="pln"><span class="n"><a href="#t1993">1993</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1994" class="pln"><span class="n"><a href="#t1994">1994</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1995" class="pln"><span class="n"><a href="#t1995">1995</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1996" class="pln"><span class="n"><a href="#t1996">1996</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1997" class="pln"><span class="n"><a href="#t1997">1997</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t1998" class="pln"><span class="n"><a href="#t1998">1998</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t1999" class="pln"><span class="n"><a href="#t1999">1999</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2000" class="pln"><span class="n"><a href="#t2000">2000</a></span><span class="t"><span class="str">    >>> torch.exp(torch.tensor([0, math.log(2.)]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2001" class="pln"><span class="n"><a href="#t2001">2001</a></span><span class="t"><span class="str">    tensor([ 1.,  2.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2002" class="pln"><span class="n"><a href="#t2002">2002</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2003" class="pln"><span class="n"><a href="#t2003">2003</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2004" class="run"><span class="n"><a href="#t2004">2004</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">expm1</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2005" class="pln"><span class="n"><a href="#t2005">2005</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2006" class="pln"><span class="n"><a href="#t2006">2006</a></span><span class="t"><span class="str">expm1(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2007" class="pln"><span class="n"><a href="#t2007">2007</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2008" class="pln"><span class="n"><a href="#t2008">2008</a></span><span class="t"><span class="str">Returns a new tensor with the exponential of the elements minus 1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2009" class="pln"><span class="n"><a href="#t2009">2009</a></span><span class="t"><span class="str">of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2010" class="pln"><span class="n"><a href="#t2010">2010</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2011" class="pln"><span class="n"><a href="#t2011">2011</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2012" class="pln"><span class="n"><a href="#t2012">2012</a></span><span class="t"><span class="str">    y_{i} = e^{x_{i}} - 1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2013" class="pln"><span class="n"><a href="#t2013">2013</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2014" class="pln"><span class="n"><a href="#t2014">2014</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2015" class="pln"><span class="n"><a href="#t2015">2015</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2016" class="pln"><span class="n"><a href="#t2016">2016</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2017" class="pln"><span class="n"><a href="#t2017">2017</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2018" class="pln"><span class="n"><a href="#t2018">2018</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2019" class="pln"><span class="n"><a href="#t2019">2019</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2020" class="pln"><span class="n"><a href="#t2020">2020</a></span><span class="t"><span class="str">    >>> torch.expm1(torch.tensor([0, math.log(2.)]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2021" class="pln"><span class="n"><a href="#t2021">2021</a></span><span class="t"><span class="str">    tensor([ 0.,  1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2022" class="pln"><span class="n"><a href="#t2022">2022</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2023" class="pln"><span class="n"><a href="#t2023">2023</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2024" class="run"><span class="n"><a href="#t2024">2024</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">eye</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2025" class="pln"><span class="n"><a href="#t2025">2025</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2026" class="pln"><span class="n"><a href="#t2026">2026</a></span><span class="t"><span class="str">eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2027" class="pln"><span class="n"><a href="#t2027">2027</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2028" class="pln"><span class="n"><a href="#t2028">2028</a></span><span class="t"><span class="str">Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2029" class="pln"><span class="n"><a href="#t2029">2029</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2030" class="pln"><span class="n"><a href="#t2030">2030</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2031" class="pln"><span class="n"><a href="#t2031">2031</a></span><span class="t"><span class="str">    n (int): the number of rows</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2032" class="pln"><span class="n"><a href="#t2032">2032</a></span><span class="t"><span class="str">    m (int, optional): the number of columns with default being :attr:`n`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2033" class="pln"><span class="n"><a href="#t2033">2033</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2034" class="pln"><span class="n"><a href="#t2034">2034</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2035" class="pln"><span class="n"><a href="#t2035">2035</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2036" class="pln"><span class="n"><a href="#t2036">2036</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2037" class="pln"><span class="n"><a href="#t2037">2037</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2038" class="pln"><span class="n"><a href="#t2038">2038</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2039" class="pln"><span class="n"><a href="#t2039">2039</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2040" class="pln"><span class="n"><a href="#t2040">2040</a></span><span class="t"><span class="str">    Tensor: A 2-D tensor with ones on the diagonal and zeros elsewhere</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2041" class="pln"><span class="n"><a href="#t2041">2041</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2042" class="pln"><span class="n"><a href="#t2042">2042</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2043" class="pln"><span class="n"><a href="#t2043">2043</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2044" class="pln"><span class="n"><a href="#t2044">2044</a></span><span class="t"><span class="str">    >>> torch.eye(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2045" class="pln"><span class="n"><a href="#t2045">2045</a></span><span class="t"><span class="str">    tensor([[ 1.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2046" class="pln"><span class="n"><a href="#t2046">2046</a></span><span class="t"><span class="str">            [ 0.,  1.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2047" class="pln"><span class="n"><a href="#t2047">2047</a></span><span class="t"><span class="str">            [ 0.,  0.,  1.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2048" class="pln"><span class="n"><a href="#t2048">2048</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2049" class="pln"><span class="n"><a href="#t2049">2049</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2050" class="run"><span class="n"><a href="#t2050">2050</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">floor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2051" class="pln"><span class="n"><a href="#t2051">2051</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2052" class="pln"><span class="n"><a href="#t2052">2052</a></span><span class="t"><span class="str">floor(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2053" class="pln"><span class="n"><a href="#t2053">2053</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2054" class="pln"><span class="n"><a href="#t2054">2054</a></span><span class="t"><span class="str">Returns a new tensor with the floor of the elements of :attr:`input`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2055" class="pln"><span class="n"><a href="#t2055">2055</a></span><span class="t"><span class="str">the largest integer less than or equal to each element.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2056" class="pln"><span class="n"><a href="#t2056">2056</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2057" class="pln"><span class="n"><a href="#t2057">2057</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2058" class="pln"><span class="n"><a href="#t2058">2058</a></span><span class="t"><span class="str">    \text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2059" class="pln"><span class="n"><a href="#t2059">2059</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2060" class="pln"><span class="n"><a href="#t2060">2060</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2061" class="pln"><span class="n"><a href="#t2061">2061</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2062" class="pln"><span class="n"><a href="#t2062">2062</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2063" class="pln"><span class="n"><a href="#t2063">2063</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2064" class="pln"><span class="n"><a href="#t2064">2064</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2065" class="pln"><span class="n"><a href="#t2065">2065</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2066" class="pln"><span class="n"><a href="#t2066">2066</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2067" class="pln"><span class="n"><a href="#t2067">2067</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2068" class="pln"><span class="n"><a href="#t2068">2068</a></span><span class="t"><span class="str">    tensor([-0.8166,  1.5308, -0.2530, -0.2091])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2069" class="pln"><span class="n"><a href="#t2069">2069</a></span><span class="t"><span class="str">    >>> torch.floor(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2070" class="pln"><span class="n"><a href="#t2070">2070</a></span><span class="t"><span class="str">    tensor([-1.,  1., -1., -1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2071" class="pln"><span class="n"><a href="#t2071">2071</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2072" class="pln"><span class="n"><a href="#t2072">2072</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2073" class="run"><span class="n"><a href="#t2073">2073</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">floor_divide</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2074" class="pln"><span class="n"><a href="#t2074">2074</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2075" class="pln"><span class="n"><a href="#t2075">2075</a></span><span class="t"><span class="str">floor_divide(input, other) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2076" class="pln"><span class="n"><a href="#t2076">2076</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2077" class="pln"><span class="n"><a href="#t2077">2077</a></span><span class="t"><span class="str">Return the division of the inputs rounded down to the nearest integer. See :func:`torch.div`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2078" class="pln"><span class="n"><a href="#t2078">2078</a></span><span class="t"><span class="str">for type promotion and broadcasting rules.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2079" class="pln"><span class="n"><a href="#t2079">2079</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2080" class="pln"><span class="n"><a href="#t2080">2080</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2081" class="pln"><span class="n"><a href="#t2081">2081</a></span><span class="t"><span class="str">    \text{{out}}_i = \left\lfloor \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right\rfloor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2082" class="pln"><span class="n"><a href="#t2082">2082</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2083" class="pln"><span class="n"><a href="#t2083">2083</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2084" class="pln"><span class="n"><a href="#t2084">2084</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2085" class="pln"><span class="n"><a href="#t2085">2085</a></span><span class="t"><span class="str">    input (Tensor): the numerator tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2086" class="pln"><span class="n"><a href="#t2086">2086</a></span><span class="t"><span class="str">    other (Tensor or Scalar): the denominator</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2087" class="pln"><span class="n"><a href="#t2087">2087</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2088" class="pln"><span class="n"><a href="#t2088">2088</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2089" class="pln"><span class="n"><a href="#t2089">2089</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2090" class="pln"><span class="n"><a href="#t2090">2090</a></span><span class="t"><span class="str">    >>> a = torch.tensor([4.0, 3.0])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2091" class="pln"><span class="n"><a href="#t2091">2091</a></span><span class="t"><span class="str">    >>> b = torch.tensor([2.0, 2.0])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2092" class="pln"><span class="n"><a href="#t2092">2092</a></span><span class="t"><span class="str">    >>> torch.floor_divide(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2093" class="pln"><span class="n"><a href="#t2093">2093</a></span><span class="t"><span class="str">    tensor([2.0, 1.0])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2094" class="pln"><span class="n"><a href="#t2094">2094</a></span><span class="t"><span class="str">    >>> torch.floor_divide(a, 1.4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2095" class="pln"><span class="n"><a href="#t2095">2095</a></span><span class="t"><span class="str">    tensor([2.0, 2.0])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2096" class="pln"><span class="n"><a href="#t2096">2096</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2097" class="pln"><span class="n"><a href="#t2097">2097</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2098" class="run"><span class="n"><a href="#t2098">2098</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">fmod</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2099" class="pln"><span class="n"><a href="#t2099">2099</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2100" class="pln"><span class="n"><a href="#t2100">2100</a></span><span class="t"><span class="str">fmod(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2101" class="pln"><span class="n"><a href="#t2101">2101</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2102" class="pln"><span class="n"><a href="#t2102">2102</a></span><span class="t"><span class="str">Computes the element-wise remainder of division.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2103" class="pln"><span class="n"><a href="#t2103">2103</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2104" class="pln"><span class="n"><a href="#t2104">2104</a></span><span class="t"><span class="str">The dividend and divisor may contain both for integer and floating point</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2105" class="pln"><span class="n"><a href="#t2105">2105</a></span><span class="t"><span class="str">numbers. The remainder has the same sign as the dividend :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2106" class="pln"><span class="n"><a href="#t2106">2106</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2107" class="pln"><span class="n"><a href="#t2107">2107</a></span><span class="t"><span class="str">When :attr:`other` is a tensor, the shapes of :attr:`input` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2108" class="pln"><span class="n"><a href="#t2108">2108</a></span><span class="t"><span class="str">:attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2109" class="pln"><span class="n"><a href="#t2109">2109</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2110" class="pln"><span class="n"><a href="#t2110">2110</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2111" class="pln"><span class="n"><a href="#t2111">2111</a></span><span class="t"><span class="str">    input (Tensor): the dividend</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2112" class="pln"><span class="n"><a href="#t2112">2112</a></span><span class="t"><span class="str">    other (Tensor or float): the divisor, which may be either a number or a tensor of the same shape as the dividend</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2113" class="pln"><span class="n"><a href="#t2113">2113</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2114" class="pln"><span class="n"><a href="#t2114">2114</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2115" class="pln"><span class="n"><a href="#t2115">2115</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2116" class="pln"><span class="n"><a href="#t2116">2116</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2117" class="pln"><span class="n"><a href="#t2117">2117</a></span><span class="t"><span class="str">    >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2118" class="pln"><span class="n"><a href="#t2118">2118</a></span><span class="t"><span class="str">    tensor([-1., -0., -1.,  1.,  0.,  1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2119" class="pln"><span class="n"><a href="#t2119">2119</a></span><span class="t"><span class="str">    >>> torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2120" class="pln"><span class="n"><a href="#t2120">2120</a></span><span class="t"><span class="str">    tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2121" class="pln"><span class="n"><a href="#t2121">2121</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2122" class="pln"><span class="n"><a href="#t2122">2122</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2123" class="pln"><span class="n"><a href="#t2123">2123</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2124" class="pln"><span class="n"><a href="#t2124">2124</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2125" class="run"><span class="n"><a href="#t2125">2125</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">frac</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2126" class="pln"><span class="n"><a href="#t2126">2126</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2127" class="pln"><span class="n"><a href="#t2127">2127</a></span><span class="t"><span class="str">frac(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2128" class="pln"><span class="n"><a href="#t2128">2128</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2129" class="pln"><span class="n"><a href="#t2129">2129</a></span><span class="t"><span class="str">Computes the fractional portion of each element in :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2130" class="pln"><span class="n"><a href="#t2130">2130</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2131" class="pln"><span class="n"><a href="#t2131">2131</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2132" class="pln"><span class="n"><a href="#t2132">2132</a></span><span class="t"><span class="str">    \text{out}_{i} = \text{input}_{i} - \left\lfloor |\text{input}_{i}| \right\rfloor * \operatorname{sgn}(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2133" class="pln"><span class="n"><a href="#t2133">2133</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2134" class="pln"><span class="n"><a href="#t2134">2134</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2135" class="pln"><span class="n"><a href="#t2135">2135</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2136" class="pln"><span class="n"><a href="#t2136">2136</a></span><span class="t"><span class="str">    >>> torch.frac(torch.tensor([1, 2.5, -3.2]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2137" class="pln"><span class="n"><a href="#t2137">2137</a></span><span class="t"><span class="str">    tensor([ 0.0000,  0.5000, -0.2000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2138" class="pln"><span class="n"><a href="#t2138">2138</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2139" class="pln"><span class="n"><a href="#t2139">2139</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2140" class="run"><span class="n"><a href="#t2140">2140</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">from_numpy</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2141" class="pln"><span class="n"><a href="#t2141">2141</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2142" class="pln"><span class="n"><a href="#t2142">2142</a></span><span class="t"><span class="str">from_numpy(ndarray) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2143" class="pln"><span class="n"><a href="#t2143">2143</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2144" class="pln"><span class="n"><a href="#t2144">2144</a></span><span class="t"><span class="str">Creates a :class:`Tensor` from a :class:`numpy.ndarray`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2145" class="pln"><span class="n"><a href="#t2145">2145</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2146" class="pln"><span class="n"><a href="#t2146">2146</a></span><span class="t"><span class="str">The returned tensor and :attr:`ndarray` share the same memory. Modifications to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2147" class="pln"><span class="n"><a href="#t2147">2147</a></span><span class="t"><span class="str">the tensor will be reflected in the :attr:`ndarray` and vice versa. The returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2148" class="pln"><span class="n"><a href="#t2148">2148</a></span><span class="t"><span class="str">tensor is not resizable.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2149" class="pln"><span class="n"><a href="#t2149">2149</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2150" class="pln"><span class="n"><a href="#t2150">2150</a></span><span class="t"><span class="str">It currently accepts :attr:`ndarray` with dtypes of ``numpy.float64``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2151" class="pln"><span class="n"><a href="#t2151">2151</a></span><span class="t"><span class="str">``numpy.float32``, ``numpy.float16``, ``numpy.int64``, ``numpy.int32``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2152" class="pln"><span class="n"><a href="#t2152">2152</a></span><span class="t"><span class="str">``numpy.int16``, ``numpy.int8``, ``numpy.uint8``, and ``numpy.bool``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2153" class="pln"><span class="n"><a href="#t2153">2153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2154" class="pln"><span class="n"><a href="#t2154">2154</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2155" class="pln"><span class="n"><a href="#t2155">2155</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2156" class="pln"><span class="n"><a href="#t2156">2156</a></span><span class="t"><span class="str">    >>> a = numpy.array([1, 2, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2157" class="pln"><span class="n"><a href="#t2157">2157</a></span><span class="t"><span class="str">    >>> t = torch.from_numpy(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2158" class="pln"><span class="n"><a href="#t2158">2158</a></span><span class="t"><span class="str">    >>> t</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2159" class="pln"><span class="n"><a href="#t2159">2159</a></span><span class="t"><span class="str">    tensor([ 1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2160" class="pln"><span class="n"><a href="#t2160">2160</a></span><span class="t"><span class="str">    >>> t[0] = -1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2161" class="pln"><span class="n"><a href="#t2161">2161</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2162" class="pln"><span class="n"><a href="#t2162">2162</a></span><span class="t"><span class="str">    array([-1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2163" class="pln"><span class="n"><a href="#t2163">2163</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2164" class="pln"><span class="n"><a href="#t2164">2164</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2165" class="run"><span class="n"><a href="#t2165">2165</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">flatten</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2166" class="pln"><span class="n"><a href="#t2166">2166</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2167" class="pln"><span class="n"><a href="#t2167">2167</a></span><span class="t"><span class="str">flatten(input, start_dim=0, end_dim=-1) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2168" class="pln"><span class="n"><a href="#t2168">2168</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2169" class="pln"><span class="n"><a href="#t2169">2169</a></span><span class="t"><span class="str">Flattens a contiguous range of dims in a tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2170" class="pln"><span class="n"><a href="#t2170">2170</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2171" class="pln"><span class="n"><a href="#t2171">2171</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2172" class="pln"><span class="n"><a href="#t2172">2172</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2173" class="pln"><span class="n"><a href="#t2173">2173</a></span><span class="t"><span class="str">    start_dim (int): the first dim to flatten</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2174" class="pln"><span class="n"><a href="#t2174">2174</a></span><span class="t"><span class="str">    end_dim (int): the last dim to flatten</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2175" class="pln"><span class="n"><a href="#t2175">2175</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2176" class="pln"><span class="n"><a href="#t2176">2176</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2177" class="pln"><span class="n"><a href="#t2177">2177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2178" class="pln"><span class="n"><a href="#t2178">2178</a></span><span class="t"><span class="str">    >>> t = torch.tensor([[[1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2179" class="pln"><span class="n"><a href="#t2179">2179</a></span><span class="t"><span class="str">                           [3, 4]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2180" class="pln"><span class="n"><a href="#t2180">2180</a></span><span class="t"><span class="str">                          [[5, 6],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2181" class="pln"><span class="n"><a href="#t2181">2181</a></span><span class="t"><span class="str">                           [7, 8]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2182" class="pln"><span class="n"><a href="#t2182">2182</a></span><span class="t"><span class="str">    >>> torch.flatten(t)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2183" class="pln"><span class="n"><a href="#t2183">2183</a></span><span class="t"><span class="str">    tensor([1, 2, 3, 4, 5, 6, 7, 8])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2184" class="pln"><span class="n"><a href="#t2184">2184</a></span><span class="t"><span class="str">    >>> torch.flatten(t, start_dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2185" class="pln"><span class="n"><a href="#t2185">2185</a></span><span class="t"><span class="str">    tensor([[1, 2, 3, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2186" class="pln"><span class="n"><a href="#t2186">2186</a></span><span class="t"><span class="str">            [5, 6, 7, 8]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2187" class="pln"><span class="n"><a href="#t2187">2187</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2188" class="pln"><span class="n"><a href="#t2188">2188</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2189" class="run"><span class="n"><a href="#t2189">2189</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">gather</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2190" class="pln"><span class="n"><a href="#t2190">2190</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2191" class="pln"><span class="n"><a href="#t2191">2191</a></span><span class="t"><span class="str">gather(input, dim, index, out=None, sparse_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2192" class="pln"><span class="n"><a href="#t2192">2192</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2193" class="pln"><span class="n"><a href="#t2193">2193</a></span><span class="t"><span class="str">Gathers values along an axis specified by `dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2194" class="pln"><span class="n"><a href="#t2194">2194</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2195" class="pln"><span class="n"><a href="#t2195">2195</a></span><span class="t"><span class="str">For a 3-D tensor the output is specified by::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2196" class="pln"><span class="n"><a href="#t2196">2196</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2197" class="pln"><span class="n"><a href="#t2197">2197</a></span><span class="t"><span class="str">    out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2198" class="pln"><span class="n"><a href="#t2198">2198</a></span><span class="t"><span class="str">    out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2199" class="pln"><span class="n"><a href="#t2199">2199</a></span><span class="t"><span class="str">    out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2200" class="pln"><span class="n"><a href="#t2200">2200</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2201" class="pln"><span class="n"><a href="#t2201">2201</a></span><span class="t"><span class="str">If :attr:`input` is an n-dimensional tensor with size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2202" class="pln"><span class="n"><a href="#t2202">2202</a></span><span class="t"><span class="str">:math:`(x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2203" class="pln"><span class="n"><a href="#t2203">2203</a></span><span class="t"><span class="str">and ``dim = i``, then :attr:`index` must be an :math:`n`-dimensional tensor with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2204" class="pln"><span class="n"><a href="#t2204">2204</a></span><span class="t"><span class="str">size :math:`(x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})` where :math:`y \geq 1`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2205" class="pln"><span class="n"><a href="#t2205">2205</a></span><span class="t"><span class="str">and :attr:`out` will have the same size as :attr:`index`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2206" class="pln"><span class="n"><a href="#t2206">2206</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2207" class="pln"><span class="n"><a href="#t2207">2207</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2208" class="pln"><span class="n"><a href="#t2208">2208</a></span><span class="t"><span class="str">    input (Tensor): the source tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2209" class="pln"><span class="n"><a href="#t2209">2209</a></span><span class="t"><span class="str">    dim (int): the axis along which to index</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2210" class="pln"><span class="n"><a href="#t2210">2210</a></span><span class="t"><span class="str">    index (LongTensor): the indices of elements to gather</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2211" class="pln"><span class="n"><a href="#t2211">2211</a></span><span class="t"><span class="str">    out (Tensor, optional): the destination tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2212" class="pln"><span class="n"><a href="#t2212">2212</a></span><span class="t"><span class="str">    sparse_grad(bool,optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2213" class="pln"><span class="n"><a href="#t2213">2213</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2214" class="pln"><span class="n"><a href="#t2214">2214</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2215" class="pln"><span class="n"><a href="#t2215">2215</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2216" class="pln"><span class="n"><a href="#t2216">2216</a></span><span class="t"><span class="str">    >>> t = torch.tensor([[1,2],[3,4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2217" class="pln"><span class="n"><a href="#t2217">2217</a></span><span class="t"><span class="str">    >>> torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2218" class="pln"><span class="n"><a href="#t2218">2218</a></span><span class="t"><span class="str">    tensor([[ 1,  1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2219" class="pln"><span class="n"><a href="#t2219">2219</a></span><span class="t"><span class="str">            [ 4,  3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2220" class="pln"><span class="n"><a href="#t2220">2220</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2221" class="pln"><span class="n"><a href="#t2221">2221</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2222" class="run"><span class="n"><a href="#t2222">2222</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ge</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2223" class="pln"><span class="n"><a href="#t2223">2223</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2224" class="pln"><span class="n"><a href="#t2224">2224</a></span><span class="t"><span class="str">ge(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2225" class="pln"><span class="n"><a href="#t2225">2225</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2226" class="pln"><span class="n"><a href="#t2226">2226</a></span><span class="t"><span class="str">Computes :math:`\text{input} \geq \text{other}` element-wise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2227" class="pln"><span class="n"><a href="#t2227">2227</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2228" class="pln"><span class="n"><a href="#t2228">2228</a></span><span class="t"><span class="str">The second argument can be a number or a tensor whose shape is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2229" class="pln"><span class="n"><a href="#t2229">2229</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with the first argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2230" class="pln"><span class="n"><a href="#t2230">2230</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2231" class="pln"><span class="n"><a href="#t2231">2231</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2232" class="pln"><span class="n"><a href="#t2232">2232</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2233" class="pln"><span class="n"><a href="#t2233">2233</a></span><span class="t"><span class="str">    other (Tensor or float): the tensor or value to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2234" class="pln"><span class="n"><a href="#t2234">2234</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor that must be a `BoolTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2235" class="pln"><span class="n"><a href="#t2235">2235</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2236" class="pln"><span class="n"><a href="#t2236">2236</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2237" class="pln"><span class="n"><a href="#t2237">2237</a></span><span class="t"><span class="str">    Tensor: A ``torch.BoolTensor`` containing a True at each location where comparison is true</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2238" class="pln"><span class="n"><a href="#t2238">2238</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2239" class="pln"><span class="n"><a href="#t2239">2239</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2240" class="pln"><span class="n"><a href="#t2240">2240</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2241" class="pln"><span class="n"><a href="#t2241">2241</a></span><span class="t"><span class="str">    >>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2242" class="pln"><span class="n"><a href="#t2242">2242</a></span><span class="t"><span class="str">    tensor([[True, True], [False, True]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2243" class="pln"><span class="n"><a href="#t2243">2243</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2244" class="pln"><span class="n"><a href="#t2244">2244</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2245" class="run"><span class="n"><a href="#t2245">2245</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">geqrf</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2246" class="pln"><span class="n"><a href="#t2246">2246</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2247" class="pln"><span class="n"><a href="#t2247">2247</a></span><span class="t"><span class="str">geqrf(input, out=None) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2248" class="pln"><span class="n"><a href="#t2248">2248</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2249" class="pln"><span class="n"><a href="#t2249">2249</a></span><span class="t"><span class="str">This is a low-level function for calling LAPACK directly. This function</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2250" class="pln"><span class="n"><a href="#t2250">2250</a></span><span class="t"><span class="str">returns a namedtuple (a, tau) as defined in `LAPACK documentation for geqrf`_ .</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2251" class="pln"><span class="n"><a href="#t2251">2251</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2252" class="pln"><span class="n"><a href="#t2252">2252</a></span><span class="t"><span class="str">You'll generally want to use :func:`torch.qr` instead.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2253" class="pln"><span class="n"><a href="#t2253">2253</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2254" class="pln"><span class="n"><a href="#t2254">2254</a></span><span class="t"><span class="str">Computes a QR decomposition of :attr:`input`, but without constructing</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2255" class="pln"><span class="n"><a href="#t2255">2255</a></span><span class="t"><span class="str">:math:`Q` and :math:`R` as explicit separate matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2256" class="pln"><span class="n"><a href="#t2256">2256</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2257" class="pln"><span class="n"><a href="#t2257">2257</a></span><span class="t"><span class="str">Rather, this directly calls the underlying LAPACK function `?geqrf`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2258" class="pln"><span class="n"><a href="#t2258">2258</a></span><span class="t"><span class="str">which produces a sequence of 'elementary reflectors'.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2259" class="pln"><span class="n"><a href="#t2259">2259</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2260" class="pln"><span class="n"><a href="#t2260">2260</a></span><span class="t"><span class="str">See `LAPACK documentation for geqrf`_ for further details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2261" class="pln"><span class="n"><a href="#t2261">2261</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2262" class="pln"><span class="n"><a href="#t2262">2262</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2263" class="pln"><span class="n"><a href="#t2263">2263</a></span><span class="t"><span class="str">    input (Tensor): the input matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2264" class="pln"><span class="n"><a href="#t2264">2264</a></span><span class="t"><span class="str">    out (tuple, optional): the output tuple of (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2265" class="pln"><span class="n"><a href="#t2265">2265</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2266" class="pln"><span class="n"><a href="#t2266">2266</a></span><span class="t"><span class="str">.. _LAPACK documentation for geqrf:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2267" class="pln"><span class="n"><a href="#t2267">2267</a></span><span class="t"><span class="str">    https://software.intel.com/en-us/node/521004</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2268" class="pln"><span class="n"><a href="#t2268">2268</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2269" class="pln"><span class="n"><a href="#t2269">2269</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2270" class="pln"><span class="n"><a href="#t2270">2270</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2271" class="run"><span class="n"><a href="#t2271">2271</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ger</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2272" class="pln"><span class="n"><a href="#t2272">2272</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2273" class="pln"><span class="n"><a href="#t2273">2273</a></span><span class="t"><span class="str">ger(input, vec2, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2274" class="pln"><span class="n"><a href="#t2274">2274</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2275" class="pln"><span class="n"><a href="#t2275">2275</a></span><span class="t"><span class="str">Outer product of :attr:`input` and :attr:`vec2`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2276" class="pln"><span class="n"><a href="#t2276">2276</a></span><span class="t"><span class="str">If :attr:`input` is a vector of size :math:`n` and :attr:`vec2` is a vector of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2277" class="pln"><span class="n"><a href="#t2277">2277</a></span><span class="t"><span class="str">size :math:`m`, then :attr:`out` must be a matrix of size :math:`(n \times m)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2278" class="pln"><span class="n"><a href="#t2278">2278</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2279" class="pln"><span class="n"><a href="#t2279">2279</a></span><span class="t"><span class="str">.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2280" class="pln"><span class="n"><a href="#t2280">2280</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2281" class="pln"><span class="n"><a href="#t2281">2281</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2282" class="pln"><span class="n"><a href="#t2282">2282</a></span><span class="t"><span class="str">    input (Tensor): 1-D input vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2283" class="pln"><span class="n"><a href="#t2283">2283</a></span><span class="t"><span class="str">    vec2 (Tensor): 1-D input vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2284" class="pln"><span class="n"><a href="#t2284">2284</a></span><span class="t"><span class="str">    out (Tensor, optional): optional output matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2285" class="pln"><span class="n"><a href="#t2285">2285</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2286" class="pln"><span class="n"><a href="#t2286">2286</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2287" class="pln"><span class="n"><a href="#t2287">2287</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2288" class="pln"><span class="n"><a href="#t2288">2288</a></span><span class="t"><span class="str">    >>> v1 = torch.arange(1., 5.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2289" class="pln"><span class="n"><a href="#t2289">2289</a></span><span class="t"><span class="str">    >>> v2 = torch.arange(1., 4.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2290" class="pln"><span class="n"><a href="#t2290">2290</a></span><span class="t"><span class="str">    >>> torch.ger(v1, v2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2291" class="pln"><span class="n"><a href="#t2291">2291</a></span><span class="t"><span class="str">    tensor([[  1.,   2.,   3.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2292" class="pln"><span class="n"><a href="#t2292">2292</a></span><span class="t"><span class="str">            [  2.,   4.,   6.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2293" class="pln"><span class="n"><a href="#t2293">2293</a></span><span class="t"><span class="str">            [  3.,   6.,   9.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2294" class="pln"><span class="n"><a href="#t2294">2294</a></span><span class="t"><span class="str">            [  4.,   8.,  12.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2295" class="pln"><span class="n"><a href="#t2295">2295</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2296" class="pln"><span class="n"><a href="#t2296">2296</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2297" class="run"><span class="n"><a href="#t2297">2297</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">solve</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2298" class="pln"><span class="n"><a href="#t2298">2298</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2299" class="pln"><span class="n"><a href="#t2299">2299</a></span><span class="t"><span class="str">torch.solve(input, A, out=None) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2300" class="pln"><span class="n"><a href="#t2300">2300</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2301" class="pln"><span class="n"><a href="#t2301">2301</a></span><span class="t"><span class="str">This function returns the solution to the system of linear</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2302" class="pln"><span class="n"><a href="#t2302">2302</a></span><span class="t"><span class="str">equations represented by :math:`AX = B` and the LU factorization of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2303" class="pln"><span class="n"><a href="#t2303">2303</a></span><span class="t"><span class="str">A, in order as a namedtuple `solution, LU`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2304" class="pln"><span class="n"><a href="#t2304">2304</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2305" class="pln"><span class="n"><a href="#t2305">2305</a></span><span class="t"><span class="str">`LU` contains `L` and `U` factors for LU factorization of `A`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2306" class="pln"><span class="n"><a href="#t2306">2306</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2307" class="pln"><span class="n"><a href="#t2307">2307</a></span><span class="t"><span class="str">`torch.solve(B, A)` can take in 2D inputs `B, A` or inputs that are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2308" class="pln"><span class="n"><a href="#t2308">2308</a></span><span class="t"><span class="str">batches of 2D matrices. If the inputs are batches, then returns</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2309" class="pln"><span class="n"><a href="#t2309">2309</a></span><span class="t"><span class="str">batched outputs `solution, LU`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2310" class="pln"><span class="n"><a href="#t2310">2310</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2311" class="pln"><span class="n"><a href="#t2311">2311</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2312" class="pln"><span class="n"><a href="#t2312">2312</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2313" class="pln"><span class="n"><a href="#t2313">2313</a></span><span class="t"><span class="str">    Irrespective of the original strides, the returned matrices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2314" class="pln"><span class="n"><a href="#t2314">2314</a></span><span class="t"><span class="str">    `solution` and `LU` will be transposed, i.e. with strides like</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2315" class="pln"><span class="n"><a href="#t2315">2315</a></span><span class="t"><span class="str">    `B.contiguous().transpose(-1, -2).stride()` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2316" class="pln"><span class="n"><a href="#t2316">2316</a></span><span class="t"><span class="str">    `A.contiguous().transpose(-1, -2).stride()` respectively.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2317" class="pln"><span class="n"><a href="#t2317">2317</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2318" class="pln"><span class="n"><a href="#t2318">2318</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2319" class="pln"><span class="n"><a href="#t2319">2319</a></span><span class="t"><span class="str">    input (Tensor): input matrix :math:`B` of size :math:`(*, m, k)` , where :math:`*`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2320" class="pln"><span class="n"><a href="#t2320">2320</a></span><span class="t"><span class="str">                is zero or more batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2321" class="pln"><span class="n"><a href="#t2321">2321</a></span><span class="t"><span class="str">    A (Tensor): input square matrix of size :math:`(*, m, m)`, where</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2322" class="pln"><span class="n"><a href="#t2322">2322</a></span><span class="t"><span class="str">                :math:`*` is zero or more batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2323" class="pln"><span class="n"><a href="#t2323">2323</a></span><span class="t"><span class="str">    out ((Tensor, Tensor), optional): optional output tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2324" class="pln"><span class="n"><a href="#t2324">2324</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2325" class="pln"><span class="n"><a href="#t2325">2325</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2326" class="pln"><span class="n"><a href="#t2326">2326</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2327" class="pln"><span class="n"><a href="#t2327">2327</a></span><span class="t"><span class="str">    >>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2328" class="pln"><span class="n"><a href="#t2328">2328</a></span><span class="t"><span class="str">                          [-6.05, -3.30,  5.36, -4.44,  1.08],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2329" class="pln"><span class="n"><a href="#t2329">2329</a></span><span class="t"><span class="str">                          [-0.45,  2.58, -2.70,  0.27,  9.04],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2330" class="pln"><span class="n"><a href="#t2330">2330</a></span><span class="t"><span class="str">                          [8.32,  2.71,  4.35,  -7.17,  2.14],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2331" class="pln"><span class="n"><a href="#t2331">2331</a></span><span class="t"><span class="str">                          [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2332" class="pln"><span class="n"><a href="#t2332">2332</a></span><span class="t"><span class="str">    >>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2333" class="pln"><span class="n"><a href="#t2333">2333</a></span><span class="t"><span class="str">                          [-1.56,  4.00, -8.67,  1.75,  2.86],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2334" class="pln"><span class="n"><a href="#t2334">2334</a></span><span class="t"><span class="str">                          [9.81, -4.09, -4.57, -8.61,  8.99]]).t()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2335" class="pln"><span class="n"><a href="#t2335">2335</a></span><span class="t"><span class="str">    >>> X, LU = torch.solve(B, A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2336" class="pln"><span class="n"><a href="#t2336">2336</a></span><span class="t"><span class="str">    >>> torch.dist(B, torch.mm(A, X))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2337" class="pln"><span class="n"><a href="#t2337">2337</a></span><span class="t"><span class="str">    tensor(1.00000e-06 *</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2338" class="pln"><span class="n"><a href="#t2338">2338</a></span><span class="t"><span class="str">           7.0977)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2339" class="pln"><span class="n"><a href="#t2339">2339</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2340" class="pln"><span class="n"><a href="#t2340">2340</a></span><span class="t"><span class="str">    >>> # Batched solver example</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2341" class="pln"><span class="n"><a href="#t2341">2341</a></span><span class="t"><span class="str">    >>> A = torch.randn(2, 3, 1, 4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2342" class="pln"><span class="n"><a href="#t2342">2342</a></span><span class="t"><span class="str">    >>> B = torch.randn(2, 3, 1, 4, 6)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2343" class="pln"><span class="n"><a href="#t2343">2343</a></span><span class="t"><span class="str">    >>> X, LU = torch.solve(B, A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2344" class="pln"><span class="n"><a href="#t2344">2344</a></span><span class="t"><span class="str">    >>> torch.dist(B, A.matmul(X))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2345" class="pln"><span class="n"><a href="#t2345">2345</a></span><span class="t"><span class="str">    tensor(1.00000e-06 *</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2346" class="pln"><span class="n"><a href="#t2346">2346</a></span><span class="t"><span class="str">       3.6386)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2347" class="pln"><span class="n"><a href="#t2347">2347</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2348" class="pln"><span class="n"><a href="#t2348">2348</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2349" class="pln"><span class="n"><a href="#t2349">2349</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2350" class="run"><span class="n"><a href="#t2350">2350</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">get_default_dtype</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2351" class="pln"><span class="n"><a href="#t2351">2351</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2352" class="pln"><span class="n"><a href="#t2352">2352</a></span><span class="t"><span class="str">get_default_dtype() -> torch.dtype</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2353" class="pln"><span class="n"><a href="#t2353">2353</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2354" class="pln"><span class="n"><a href="#t2354">2354</a></span><span class="t"><span class="str">Get the current default floating point :class:`torch.dtype`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2355" class="pln"><span class="n"><a href="#t2355">2355</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2356" class="pln"><span class="n"><a href="#t2356">2356</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2357" class="pln"><span class="n"><a href="#t2357">2357</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2358" class="pln"><span class="n"><a href="#t2358">2358</a></span><span class="t"><span class="str">    >>> torch.get_default_dtype()  # initial default for floating point is torch.float32</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2359" class="pln"><span class="n"><a href="#t2359">2359</a></span><span class="t"><span class="str">    torch.float32</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2360" class="pln"><span class="n"><a href="#t2360">2360</a></span><span class="t"><span class="str">    >>> torch.set_default_dtype(torch.float64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2361" class="pln"><span class="n"><a href="#t2361">2361</a></span><span class="t"><span class="str">    >>> torch.get_default_dtype()  # default is now changed to torch.float64</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2362" class="pln"><span class="n"><a href="#t2362">2362</a></span><span class="t"><span class="str">    torch.float64</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2363" class="pln"><span class="n"><a href="#t2363">2363</a></span><span class="t"><span class="str">    >>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2364" class="pln"><span class="n"><a href="#t2364">2364</a></span><span class="t"><span class="str">    >>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2365" class="pln"><span class="n"><a href="#t2365">2365</a></span><span class="t"><span class="str">    torch.float32</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2366" class="pln"><span class="n"><a href="#t2366">2366</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2367" class="pln"><span class="n"><a href="#t2367">2367</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2368" class="pln"><span class="n"><a href="#t2368">2368</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2369" class="run"><span class="n"><a href="#t2369">2369</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">get_num_threads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2370" class="pln"><span class="n"><a href="#t2370">2370</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2371" class="pln"><span class="n"><a href="#t2371">2371</a></span><span class="t"><span class="str">get_num_threads() -> int</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2372" class="pln"><span class="n"><a href="#t2372">2372</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2373" class="pln"><span class="n"><a href="#t2373">2373</a></span><span class="t"><span class="str">Returns the number of threads used for parallelizing CPU operations</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2374" class="pln"><span class="n"><a href="#t2374">2374</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2375" class="pln"><span class="n"><a href="#t2375">2375</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2376" class="run"><span class="n"><a href="#t2376">2376</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">get_num_interop_threads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2377" class="pln"><span class="n"><a href="#t2377">2377</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2378" class="pln"><span class="n"><a href="#t2378">2378</a></span><span class="t"><span class="str">get_num_interop_threads() -> int</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2379" class="pln"><span class="n"><a href="#t2379">2379</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2380" class="pln"><span class="n"><a href="#t2380">2380</a></span><span class="t"><span class="str">Returns the number of threads used for inter-op parallelism on CPU</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2381" class="pln"><span class="n"><a href="#t2381">2381</a></span><span class="t"><span class="str">(e.g. in JIT interpreter)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2382" class="pln"><span class="n"><a href="#t2382">2382</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2383" class="pln"><span class="n"><a href="#t2383">2383</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2384" class="run"><span class="n"><a href="#t2384">2384</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">gt</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2385" class="pln"><span class="n"><a href="#t2385">2385</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2386" class="pln"><span class="n"><a href="#t2386">2386</a></span><span class="t"><span class="str">gt(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2387" class="pln"><span class="n"><a href="#t2387">2387</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2388" class="pln"><span class="n"><a href="#t2388">2388</a></span><span class="t"><span class="str">Computes :math:`\text{input} > \text{other}` element-wise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2389" class="pln"><span class="n"><a href="#t2389">2389</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2390" class="pln"><span class="n"><a href="#t2390">2390</a></span><span class="t"><span class="str">The second argument can be a number or a tensor whose shape is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2391" class="pln"><span class="n"><a href="#t2391">2391</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with the first argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2392" class="pln"><span class="n"><a href="#t2392">2392</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2393" class="pln"><span class="n"><a href="#t2393">2393</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2394" class="pln"><span class="n"><a href="#t2394">2394</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2395" class="pln"><span class="n"><a href="#t2395">2395</a></span><span class="t"><span class="str">    other (Tensor or float): the tensor or value to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2396" class="pln"><span class="n"><a href="#t2396">2396</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor that must be a `BoolTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2397" class="pln"><span class="n"><a href="#t2397">2397</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2398" class="pln"><span class="n"><a href="#t2398">2398</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2399" class="pln"><span class="n"><a href="#t2399">2399</a></span><span class="t"><span class="str">    Tensor: A ``torch.BoolTensor`` containing a True at each location where comparison is true</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2400" class="pln"><span class="n"><a href="#t2400">2400</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2401" class="pln"><span class="n"><a href="#t2401">2401</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2402" class="pln"><span class="n"><a href="#t2402">2402</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2403" class="pln"><span class="n"><a href="#t2403">2403</a></span><span class="t"><span class="str">    >>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2404" class="pln"><span class="n"><a href="#t2404">2404</a></span><span class="t"><span class="str">    tensor([[False, True], [False, False]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2405" class="pln"><span class="n"><a href="#t2405">2405</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2406" class="pln"><span class="n"><a href="#t2406">2406</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2407" class="run"><span class="n"><a href="#t2407">2407</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">histc</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2408" class="pln"><span class="n"><a href="#t2408">2408</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2409" class="pln"><span class="n"><a href="#t2409">2409</a></span><span class="t"><span class="str">histc(input, bins=100, min=0, max=0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2410" class="pln"><span class="n"><a href="#t2410">2410</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2411" class="pln"><span class="n"><a href="#t2411">2411</a></span><span class="t"><span class="str">Computes the histogram of a tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2412" class="pln"><span class="n"><a href="#t2412">2412</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2413" class="pln"><span class="n"><a href="#t2413">2413</a></span><span class="t"><span class="str">The elements are sorted into equal width bins between :attr:`min` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2414" class="pln"><span class="n"><a href="#t2414">2414</a></span><span class="t"><span class="str">:attr:`max`. If :attr:`min` and :attr:`max` are both zero, the minimum and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2415" class="pln"><span class="n"><a href="#t2415">2415</a></span><span class="t"><span class="str">maximum values of the data are used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2416" class="pln"><span class="n"><a href="#t2416">2416</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2417" class="pln"><span class="n"><a href="#t2417">2417</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2418" class="pln"><span class="n"><a href="#t2418">2418</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2419" class="pln"><span class="n"><a href="#t2419">2419</a></span><span class="t"><span class="str">    bins (int): number of histogram bins</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2420" class="pln"><span class="n"><a href="#t2420">2420</a></span><span class="t"><span class="str">    min (int): lower end of the range (inclusive)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2421" class="pln"><span class="n"><a href="#t2421">2421</a></span><span class="t"><span class="str">    max (int): upper end of the range (inclusive)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2422" class="pln"><span class="n"><a href="#t2422">2422</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2423" class="pln"><span class="n"><a href="#t2423">2423</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2424" class="pln"><span class="n"><a href="#t2424">2424</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2425" class="pln"><span class="n"><a href="#t2425">2425</a></span><span class="t"><span class="str">    Tensor: Histogram represented as a tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2426" class="pln"><span class="n"><a href="#t2426">2426</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2427" class="pln"><span class="n"><a href="#t2427">2427</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2428" class="pln"><span class="n"><a href="#t2428">2428</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2429" class="pln"><span class="n"><a href="#t2429">2429</a></span><span class="t"><span class="str">    >>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2430" class="pln"><span class="n"><a href="#t2430">2430</a></span><span class="t"><span class="str">    tensor([ 0.,  2.,  1.,  0.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2431" class="pln"><span class="n"><a href="#t2431">2431</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2432" class="pln"><span class="n"><a href="#t2432">2432</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2433" class="run"><span class="n"><a href="#t2433">2433</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">imag</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2434" class="pln"><span class="n"><a href="#t2434">2434</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2435" class="pln"><span class="n"><a href="#t2435">2435</a></span><span class="t"><span class="str">imag(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2436" class="pln"><span class="n"><a href="#t2436">2436</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2437" class="pln"><span class="n"><a href="#t2437">2437</a></span><span class="t"><span class="str">Computes the element-wise imag value of the given :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2438" class="pln"><span class="n"><a href="#t2438">2438</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2439" class="pln"><span class="n"><a href="#t2439">2439</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2440" class="pln"><span class="n"><a href="#t2440">2440</a></span><span class="t"><span class="str">    \text{out}_{i} = imag(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2441" class="pln"><span class="n"><a href="#t2441">2441</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2442" class="pln"><span class="n"><a href="#t2442">2442</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2443" class="pln"><span class="n"><a href="#t2443">2443</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2444" class="pln"><span class="n"><a href="#t2444">2444</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2445" class="pln"><span class="n"><a href="#t2445">2445</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2446" class="pln"><span class="n"><a href="#t2446">2446</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2447" class="pln"><span class="n"><a href="#t2447">2447</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2448" class="pln"><span class="n"><a href="#t2448">2448</a></span><span class="t"><span class="str">    >>> torch.imag(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2449" class="pln"><span class="n"><a href="#t2449">2449</a></span><span class="t"><span class="str">    tensor([ 1,  2,  -3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2450" class="pln"><span class="n"><a href="#t2450">2450</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2451" class="pln"><span class="n"><a href="#t2451">2451</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2452" class="run"><span class="n"><a href="#t2452">2452</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">index_select</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2453" class="pln"><span class="n"><a href="#t2453">2453</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2454" class="pln"><span class="n"><a href="#t2454">2454</a></span><span class="t"><span class="str">index_select(input, dim, index, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2455" class="pln"><span class="n"><a href="#t2455">2455</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2456" class="pln"><span class="n"><a href="#t2456">2456</a></span><span class="t"><span class="str">Returns a new tensor which indexes the :attr:`input` tensor along dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2457" class="pln"><span class="n"><a href="#t2457">2457</a></span><span class="t"><span class="str">:attr:`dim` using the entries in :attr:`index` which is a `LongTensor`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2458" class="pln"><span class="n"><a href="#t2458">2458</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2459" class="pln"><span class="n"><a href="#t2459">2459</a></span><span class="t"><span class="str">The returned tensor has the same number of dimensions as the original tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2460" class="pln"><span class="n"><a href="#t2460">2460</a></span><span class="t"><span class="str">(:attr:`input`).  The :attr:`dim`\ th dimension has the same size as the length</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2461" class="pln"><span class="n"><a href="#t2461">2461</a></span><span class="t"><span class="str">of :attr:`index`; other dimensions have the same size as in the original tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2462" class="pln"><span class="n"><a href="#t2462">2462</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2463" class="pln"><span class="n"><a href="#t2463">2463</a></span><span class="t"><span class="str">.. note:: The returned tensor does **not** use the same storage as the original</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2464" class="pln"><span class="n"><a href="#t2464">2464</a></span><span class="t"><span class="str">          tensor.  If :attr:`out` has a different shape than expected, we</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2465" class="pln"><span class="n"><a href="#t2465">2465</a></span><span class="t"><span class="str">          silently change it to the correct shape, reallocating the underlying</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2466" class="pln"><span class="n"><a href="#t2466">2466</a></span><span class="t"><span class="str">          storage if necessary.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2467" class="pln"><span class="n"><a href="#t2467">2467</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2468" class="pln"><span class="n"><a href="#t2468">2468</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2469" class="pln"><span class="n"><a href="#t2469">2469</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2470" class="pln"><span class="n"><a href="#t2470">2470</a></span><span class="t"><span class="str">    dim (int): the dimension in which we index</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2471" class="pln"><span class="n"><a href="#t2471">2471</a></span><span class="t"><span class="str">    index (LongTensor): the 1-D tensor containing the indices to index</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2472" class="pln"><span class="n"><a href="#t2472">2472</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2473" class="pln"><span class="n"><a href="#t2473">2473</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2474" class="pln"><span class="n"><a href="#t2474">2474</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2475" class="pln"><span class="n"><a href="#t2475">2475</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2476" class="pln"><span class="n"><a href="#t2476">2476</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2477" class="pln"><span class="n"><a href="#t2477">2477</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2478" class="pln"><span class="n"><a href="#t2478">2478</a></span><span class="t"><span class="str">    tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2479" class="pln"><span class="n"><a href="#t2479">2479</a></span><span class="t"><span class="str">            [-0.4664,  0.2647, -0.1228, -1.1068],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2480" class="pln"><span class="n"><a href="#t2480">2480</a></span><span class="t"><span class="str">            [-1.1734, -0.6571,  0.7230, -0.6004]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2481" class="pln"><span class="n"><a href="#t2481">2481</a></span><span class="t"><span class="str">    >>> indices = torch.tensor([0, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2482" class="pln"><span class="n"><a href="#t2482">2482</a></span><span class="t"><span class="str">    >>> torch.index_select(x, 0, indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2483" class="pln"><span class="n"><a href="#t2483">2483</a></span><span class="t"><span class="str">    tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2484" class="pln"><span class="n"><a href="#t2484">2484</a></span><span class="t"><span class="str">            [-1.1734, -0.6571,  0.7230, -0.6004]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2485" class="pln"><span class="n"><a href="#t2485">2485</a></span><span class="t"><span class="str">    >>> torch.index_select(x, 1, indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2486" class="pln"><span class="n"><a href="#t2486">2486</a></span><span class="t"><span class="str">    tensor([[ 0.1427, -0.5414],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2487" class="pln"><span class="n"><a href="#t2487">2487</a></span><span class="t"><span class="str">            [-0.4664, -0.1228],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2488" class="pln"><span class="n"><a href="#t2488">2488</a></span><span class="t"><span class="str">            [-1.1734,  0.7230]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2489" class="pln"><span class="n"><a href="#t2489">2489</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2490" class="pln"><span class="n"><a href="#t2490">2490</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2491" class="run"><span class="n"><a href="#t2491">2491</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">inverse</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2492" class="pln"><span class="n"><a href="#t2492">2492</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2493" class="pln"><span class="n"><a href="#t2493">2493</a></span><span class="t"><span class="str">inverse(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2494" class="pln"><span class="n"><a href="#t2494">2494</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2495" class="pln"><span class="n"><a href="#t2495">2495</a></span><span class="t"><span class="str">Takes the inverse of the square matrix :attr:`input`. :attr:`input` can be batches</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2496" class="pln"><span class="n"><a href="#t2496">2496</a></span><span class="t"><span class="str">of 2D square tensors, in which case this function would return a tensor composed of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2497" class="pln"><span class="n"><a href="#t2497">2497</a></span><span class="t"><span class="str">individual inverses.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2498" class="pln"><span class="n"><a href="#t2498">2498</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2499" class="pln"><span class="n"><a href="#t2499">2499</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2500" class="pln"><span class="n"><a href="#t2500">2500</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2501" class="pln"><span class="n"><a href="#t2501">2501</a></span><span class="t"><span class="str">    Irrespective of the original strides, the returned tensors will be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2502" class="pln"><span class="n"><a href="#t2502">2502</a></span><span class="t"><span class="str">    transposed, i.e. with strides like `input.contiguous().transpose(-2, -1).stride()`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2503" class="pln"><span class="n"><a href="#t2503">2503</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2504" class="pln"><span class="n"><a href="#t2504">2504</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2505" class="pln"><span class="n"><a href="#t2505">2505</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size :math:`(*, n, n)` where `*` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2506" class="pln"><span class="n"><a href="#t2506">2506</a></span><span class="t"><span class="str">                    batch dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2507" class="pln"><span class="n"><a href="#t2507">2507</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2508" class="pln"><span class="n"><a href="#t2508">2508</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2509" class="pln"><span class="n"><a href="#t2509">2509</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2510" class="pln"><span class="n"><a href="#t2510">2510</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2511" class="pln"><span class="n"><a href="#t2511">2511</a></span><span class="t"><span class="str">    >>> x = torch.rand(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2512" class="pln"><span class="n"><a href="#t2512">2512</a></span><span class="t"><span class="str">    >>> y = torch.inverse(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2513" class="pln"><span class="n"><a href="#t2513">2513</a></span><span class="t"><span class="str">    >>> z = torch.mm(x, y)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2514" class="pln"><span class="n"><a href="#t2514">2514</a></span><span class="t"><span class="str">    >>> z</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2515" class="pln"><span class="n"><a href="#t2515">2515</a></span><span class="t"><span class="str">    tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2516" class="pln"><span class="n"><a href="#t2516">2516</a></span><span class="t"><span class="str">            [ 0.0000,  1.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2517" class="pln"><span class="n"><a href="#t2517">2517</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  1.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2518" class="pln"><span class="n"><a href="#t2518">2518</a></span><span class="t"><span class="str">            [ 0.0000, -0.0000, -0.0000,  1.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2519" class="pln"><span class="n"><a href="#t2519">2519</a></span><span class="t"><span class="str">    >>> torch.max(torch.abs(z - torch.eye(4))) # Max non-zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2520" class="pln"><span class="n"><a href="#t2520">2520</a></span><span class="t"><span class="str">    tensor(1.1921e-07)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2521" class="pln"><span class="n"><a href="#t2521">2521</a></span><span class="t"><span class="str">    >>> # Batched inverse example</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2522" class="pln"><span class="n"><a href="#t2522">2522</a></span><span class="t"><span class="str">    >>> x = torch.randn(2, 3, 4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2523" class="pln"><span class="n"><a href="#t2523">2523</a></span><span class="t"><span class="str">    >>> y = torch.inverse(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2524" class="pln"><span class="n"><a href="#t2524">2524</a></span><span class="t"><span class="str">    >>> z = torch.matmul(x, y)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2525" class="pln"><span class="n"><a href="#t2525">2525</a></span><span class="t"><span class="str">    >>> torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2526" class="pln"><span class="n"><a href="#t2526">2526</a></span><span class="t"><span class="str">    tensor(1.9073e-06)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2527" class="pln"><span class="n"><a href="#t2527">2527</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2528" class="pln"><span class="n"><a href="#t2528">2528</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2529" class="run"><span class="n"><a href="#t2529">2529</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">isinf</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2530" class="pln"><span class="n"><a href="#t2530">2530</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2531" class="pln"><span class="n"><a href="#t2531">2531</a></span><span class="t"><span class="str">Returns a new tensor with boolean elements representing if each element is `+/-INF` or not.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2532" class="pln"><span class="n"><a href="#t2532">2532</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2533" class="pln"><span class="n"><a href="#t2533">2533</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2534" class="pln"><span class="n"><a href="#t2534">2534</a></span><span class="t"><span class="str">        tensor (Tensor): A tensor to check</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2535" class="pln"><span class="n"><a href="#t2535">2535</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2536" class="pln"><span class="n"><a href="#t2536">2536</a></span><span class="t"><span class="str">    Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2537" class="pln"><span class="n"><a href="#t2537">2537</a></span><span class="t"><span class="str">        Tensor: ``A torch.Tensor with dtype torch.bool`` containing a True at each location of `+/-INF` elements and False otherwise</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2538" class="pln"><span class="n"><a href="#t2538">2538</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2539" class="pln"><span class="n"><a href="#t2539">2539</a></span><span class="t"><span class="str">    Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2540" class="pln"><span class="n"><a href="#t2540">2540</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2541" class="pln"><span class="n"><a href="#t2541">2541</a></span><span class="t"><span class="str">        >>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2542" class="pln"><span class="n"><a href="#t2542">2542</a></span><span class="t"><span class="str">        tensor([False,  True,  False,  True,  False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2543" class="pln"><span class="n"><a href="#t2543">2543</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2544" class="pln"><span class="n"><a href="#t2544">2544</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2545" class="run"><span class="n"><a href="#t2545">2545</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">isfinite</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2546" class="pln"><span class="n"><a href="#t2546">2546</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2547" class="pln"><span class="n"><a href="#t2547">2547</a></span><span class="t"><span class="str">Returns a new tensor with boolean elements representing if each element is `Finite` or not.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2548" class="pln"><span class="n"><a href="#t2548">2548</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2549" class="pln"><span class="n"><a href="#t2549">2549</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2550" class="pln"><span class="n"><a href="#t2550">2550</a></span><span class="t"><span class="str">        tensor (Tensor): A tensor to check</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2551" class="pln"><span class="n"><a href="#t2551">2551</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2552" class="pln"><span class="n"><a href="#t2552">2552</a></span><span class="t"><span class="str">    Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2553" class="pln"><span class="n"><a href="#t2553">2553</a></span><span class="t"><span class="str">        Tensor: ``A torch.Tensor with dtype torch.bool`` containing a True at each location of finite elements and False otherwise</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2554" class="pln"><span class="n"><a href="#t2554">2554</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2555" class="pln"><span class="n"><a href="#t2555">2555</a></span><span class="t"><span class="str">    Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2556" class="pln"><span class="n"><a href="#t2556">2556</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2557" class="pln"><span class="n"><a href="#t2557">2557</a></span><span class="t"><span class="str">        >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2558" class="pln"><span class="n"><a href="#t2558">2558</a></span><span class="t"><span class="str">        tensor([True,  False,  True,  False,  False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2559" class="pln"><span class="n"><a href="#t2559">2559</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2560" class="pln"><span class="n"><a href="#t2560">2560</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2561" class="run"><span class="n"><a href="#t2561">2561</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">isnan</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2562" class="pln"><span class="n"><a href="#t2562">2562</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2563" class="pln"><span class="n"><a href="#t2563">2563</a></span><span class="t"><span class="str">Returns a new tensor with boolean elements representing if each element is `NaN` or not.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2564" class="pln"><span class="n"><a href="#t2564">2564</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2565" class="pln"><span class="n"><a href="#t2565">2565</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2566" class="pln"><span class="n"><a href="#t2566">2566</a></span><span class="t"><span class="str">    input (Tensor): A tensor to check</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2567" class="pln"><span class="n"><a href="#t2567">2567</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2568" class="pln"><span class="n"><a href="#t2568">2568</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2569" class="pln"><span class="n"><a href="#t2569">2569</a></span><span class="t"><span class="str">    Tensor: A ``torch.BoolTensor`` containing a True at each location of `NaN` elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2570" class="pln"><span class="n"><a href="#t2570">2570</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2571" class="pln"><span class="n"><a href="#t2571">2571</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2572" class="pln"><span class="n"><a href="#t2572">2572</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2573" class="pln"><span class="n"><a href="#t2573">2573</a></span><span class="t"><span class="str">    >>> torch.isnan(torch.tensor([1, float('nan'), 2]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2574" class="pln"><span class="n"><a href="#t2574">2574</a></span><span class="t"><span class="str">    tensor([False, True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2575" class="pln"><span class="n"><a href="#t2575">2575</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2576" class="pln"><span class="n"><a href="#t2576">2576</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2577" class="run"><span class="n"><a href="#t2577">2577</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">is_floating_point</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2578" class="pln"><span class="n"><a href="#t2578">2578</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2579" class="pln"><span class="n"><a href="#t2579">2579</a></span><span class="t"><span class="str">is_floating_point(input) -> (bool)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2580" class="pln"><span class="n"><a href="#t2580">2580</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2581" class="pln"><span class="n"><a href="#t2581">2581</a></span><span class="t"><span class="str">Returns True if the data type of :attr:`input` is a floating point data type i.e.,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2582" class="pln"><span class="n"><a href="#t2582">2582</a></span><span class="t"><span class="str">one of ``torch.float64``, ``torch.float32`` and ``torch.float16``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2583" class="pln"><span class="n"><a href="#t2583">2583</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2584" class="pln"><span class="n"><a href="#t2584">2584</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2585" class="pln"><span class="n"><a href="#t2585">2585</a></span><span class="t"><span class="str">    input (Tensor): the PyTorch tensor to test</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2586" class="pln"><span class="n"><a href="#t2586">2586</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2587" class="pln"><span class="n"><a href="#t2587">2587</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2588" class="run"><span class="n"><a href="#t2588">2588</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">kthvalue</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2589" class="pln"><span class="n"><a href="#t2589">2589</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2590" class="pln"><span class="n"><a href="#t2590">2590</a></span><span class="t"><span class="str">kthvalue(input, k, dim=None, keepdim=False, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2591" class="pln"><span class="n"><a href="#t2591">2591</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2592" class="pln"><span class="n"><a href="#t2592">2592</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values`` is the :attr:`k` th</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2593" class="pln"><span class="n"><a href="#t2593">2593</a></span><span class="t"><span class="str">smallest element of each row of the :attr:`input` tensor in the given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2594" class="pln"><span class="n"><a href="#t2594">2594</a></span><span class="t"><span class="str">:attr:`dim`. And ``indices`` is the index location of each element found.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2595" class="pln"><span class="n"><a href="#t2595">2595</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2596" class="pln"><span class="n"><a href="#t2596">2596</a></span><span class="t"><span class="str">If :attr:`dim` is not given, the last dimension of the `input` is chosen.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2597" class="pln"><span class="n"><a href="#t2597">2597</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2598" class="pln"><span class="n"><a href="#t2598">2598</a></span><span class="t"><span class="str">If :attr:`keepdim` is ``True``, both the :attr:`values` and :attr:`indices` tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2599" class="pln"><span class="n"><a href="#t2599">2599</a></span><span class="t"><span class="str">are the same size as :attr:`input`, except in the dimension :attr:`dim` where</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2600" class="pln"><span class="n"><a href="#t2600">2600</a></span><span class="t"><span class="str">they are of size 1. Otherwise, :attr:`dim` is squeezed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2601" class="pln"><span class="n"><a href="#t2601">2601</a></span><span class="t"><span class="str">(see :func:`torch.squeeze`), resulting in both the :attr:`values` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2602" class="pln"><span class="n"><a href="#t2602">2602</a></span><span class="t"><span class="str">:attr:`indices` tensors having 1 fewer dimension than the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2603" class="pln"><span class="n"><a href="#t2603">2603</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2604" class="pln"><span class="n"><a href="#t2604">2604</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2605" class="pln"><span class="n"><a href="#t2605">2605</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2606" class="pln"><span class="n"><a href="#t2606">2606</a></span><span class="t"><span class="str">    k (int): k for the k-th smallest element</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2607" class="pln"><span class="n"><a href="#t2607">2607</a></span><span class="t"><span class="str">    dim (int, optional): the dimension to find the kth value along</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2608" class="pln"><span class="n"><a href="#t2608">2608</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2609" class="pln"><span class="n"><a href="#t2609">2609</a></span><span class="t"><span class="str">    out (tuple, optional): the output tuple of (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2610" class="pln"><span class="n"><a href="#t2610">2610</a></span><span class="t"><span class="str">                           can be optionally given to be used as output buffers</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2611" class="pln"><span class="n"><a href="#t2611">2611</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2612" class="pln"><span class="n"><a href="#t2612">2612</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2613" class="pln"><span class="n"><a href="#t2613">2613</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2614" class="pln"><span class="n"><a href="#t2614">2614</a></span><span class="t"><span class="str">    >>> x = torch.arange(1., 6.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2615" class="pln"><span class="n"><a href="#t2615">2615</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2616" class="pln"><span class="n"><a href="#t2616">2616</a></span><span class="t"><span class="str">    tensor([ 1.,  2.,  3.,  4.,  5.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2617" class="pln"><span class="n"><a href="#t2617">2617</a></span><span class="t"><span class="str">    >>> torch.kthvalue(x, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2618" class="pln"><span class="n"><a href="#t2618">2618</a></span><span class="t"><span class="str">    torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2619" class="pln"><span class="n"><a href="#t2619">2619</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2620" class="pln"><span class="n"><a href="#t2620">2620</a></span><span class="t"><span class="str">    >>> x=torch.arange(1.,7.).resize_(2,3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2621" class="pln"><span class="n"><a href="#t2621">2621</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2622" class="pln"><span class="n"><a href="#t2622">2622</a></span><span class="t"><span class="str">    tensor([[ 1.,  2.,  3.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2623" class="pln"><span class="n"><a href="#t2623">2623</a></span><span class="t"><span class="str">            [ 4.,  5.,  6.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2624" class="pln"><span class="n"><a href="#t2624">2624</a></span><span class="t"><span class="str">    >>> torch.kthvalue(x, 2, 0, True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2625" class="pln"><span class="n"><a href="#t2625">2625</a></span><span class="t"><span class="str">    torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2626" class="pln"><span class="n"><a href="#t2626">2626</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2627" class="pln"><span class="n"><a href="#t2627">2627</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2628" class="run"><span class="n"><a href="#t2628">2628</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">le</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2629" class="pln"><span class="n"><a href="#t2629">2629</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2630" class="pln"><span class="n"><a href="#t2630">2630</a></span><span class="t"><span class="str">le(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2631" class="pln"><span class="n"><a href="#t2631">2631</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2632" class="pln"><span class="n"><a href="#t2632">2632</a></span><span class="t"><span class="str">Computes :math:`\text{input} \leq \text{other}` element-wise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2633" class="pln"><span class="n"><a href="#t2633">2633</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2634" class="pln"><span class="n"><a href="#t2634">2634</a></span><span class="t"><span class="str">The second argument can be a number or a tensor whose shape is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2635" class="pln"><span class="n"><a href="#t2635">2635</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with the first argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2636" class="pln"><span class="n"><a href="#t2636">2636</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2637" class="pln"><span class="n"><a href="#t2637">2637</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2638" class="pln"><span class="n"><a href="#t2638">2638</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2639" class="pln"><span class="n"><a href="#t2639">2639</a></span><span class="t"><span class="str">    other (Tensor or float): the tensor or value to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2640" class="pln"><span class="n"><a href="#t2640">2640</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor that must be a `BoolTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2641" class="pln"><span class="n"><a href="#t2641">2641</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2642" class="pln"><span class="n"><a href="#t2642">2642</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2643" class="pln"><span class="n"><a href="#t2643">2643</a></span><span class="t"><span class="str">    Tensor: A ``torch.BoolTensor`` containing a True at each location where comparison is true</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2644" class="pln"><span class="n"><a href="#t2644">2644</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2645" class="pln"><span class="n"><a href="#t2645">2645</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2646" class="pln"><span class="n"><a href="#t2646">2646</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2647" class="pln"><span class="n"><a href="#t2647">2647</a></span><span class="t"><span class="str">    >>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2648" class="pln"><span class="n"><a href="#t2648">2648</a></span><span class="t"><span class="str">    tensor([[True, False], [True, True]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2649" class="pln"><span class="n"><a href="#t2649">2649</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2650" class="pln"><span class="n"><a href="#t2650">2650</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2651" class="run"><span class="n"><a href="#t2651">2651</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">lerp</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2652" class="pln"><span class="n"><a href="#t2652">2652</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2653" class="pln"><span class="n"><a href="#t2653">2653</a></span><span class="t"><span class="str">lerp(input, end, weight, out=None)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2654" class="pln"><span class="n"><a href="#t2654">2654</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2655" class="pln"><span class="n"><a href="#t2655">2655</a></span><span class="t"><span class="str">Does a linear interpolation of two tensors :attr:`start` (given by :attr:`input`) and :attr:`end` based</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2656" class="pln"><span class="n"><a href="#t2656">2656</a></span><span class="t"><span class="str">on a scalar or tensor :attr:`weight` and returns the resulting :attr:`out` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2657" class="pln"><span class="n"><a href="#t2657">2657</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2658" class="pln"><span class="n"><a href="#t2658">2658</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2659" class="pln"><span class="n"><a href="#t2659">2659</a></span><span class="t"><span class="str">    \text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2660" class="pln"><span class="n"><a href="#t2660">2660</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2661" class="pln"><span class="n"><a href="#t2661">2661</a></span><span class="t"><span class="str">The shapes of :attr:`start` and :attr:`end` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2662" class="pln"><span class="n"><a href="#t2662">2662</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`. If :attr:`weight` is a tensor, then</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2663" class="pln"><span class="n"><a href="#t2663">2663</a></span><span class="t"><span class="str">the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2664" class="pln"><span class="n"><a href="#t2664">2664</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2665" class="pln"><span class="n"><a href="#t2665">2665</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2666" class="pln"><span class="n"><a href="#t2666">2666</a></span><span class="t"><span class="str">    input (Tensor): the tensor with the starting points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2667" class="pln"><span class="n"><a href="#t2667">2667</a></span><span class="t"><span class="str">    end (Tensor): the tensor with the ending points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2668" class="pln"><span class="n"><a href="#t2668">2668</a></span><span class="t"><span class="str">    weight (float or tensor): the weight for the interpolation formula</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2669" class="pln"><span class="n"><a href="#t2669">2669</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2670" class="pln"><span class="n"><a href="#t2670">2670</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2671" class="pln"><span class="n"><a href="#t2671">2671</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2672" class="pln"><span class="n"><a href="#t2672">2672</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2673" class="pln"><span class="n"><a href="#t2673">2673</a></span><span class="t"><span class="str">    >>> start = torch.arange(1., 5.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2674" class="pln"><span class="n"><a href="#t2674">2674</a></span><span class="t"><span class="str">    >>> end = torch.empty(4).fill_(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2675" class="pln"><span class="n"><a href="#t2675">2675</a></span><span class="t"><span class="str">    >>> start</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2676" class="pln"><span class="n"><a href="#t2676">2676</a></span><span class="t"><span class="str">    tensor([ 1.,  2.,  3.,  4.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2677" class="pln"><span class="n"><a href="#t2677">2677</a></span><span class="t"><span class="str">    >>> end</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2678" class="pln"><span class="n"><a href="#t2678">2678</a></span><span class="t"><span class="str">    tensor([ 10.,  10.,  10.,  10.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2679" class="pln"><span class="n"><a href="#t2679">2679</a></span><span class="t"><span class="str">    >>> torch.lerp(start, end, 0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2680" class="pln"><span class="n"><a href="#t2680">2680</a></span><span class="t"><span class="str">    tensor([ 5.5000,  6.0000,  6.5000,  7.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2681" class="pln"><span class="n"><a href="#t2681">2681</a></span><span class="t"><span class="str">    >>> torch.lerp(start, end, torch.full_like(start, 0.5))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2682" class="pln"><span class="n"><a href="#t2682">2682</a></span><span class="t"><span class="str">    tensor([ 5.5000,  6.0000,  6.5000,  7.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2683" class="pln"><span class="n"><a href="#t2683">2683</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2684" class="pln"><span class="n"><a href="#t2684">2684</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2685" class="run"><span class="n"><a href="#t2685">2685</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">lgamma</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2686" class="pln"><span class="n"><a href="#t2686">2686</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2687" class="pln"><span class="n"><a href="#t2687">2687</a></span><span class="t"><span class="str">lgamma(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2688" class="pln"><span class="n"><a href="#t2688">2688</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2689" class="pln"><span class="n"><a href="#t2689">2689</a></span><span class="t"><span class="str">Computes the logarithm of the gamma function on :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2690" class="pln"><span class="n"><a href="#t2690">2690</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2691" class="pln"><span class="n"><a href="#t2691">2691</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2692" class="pln"><span class="n"><a href="#t2692">2692</a></span><span class="t"><span class="str">    \text{out}_{i} = \log \Gamma(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2693" class="pln"><span class="n"><a href="#t2693">2693</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2694" class="pln"><span class="n"><a href="#t2694">2694</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2695" class="pln"><span class="n"><a href="#t2695">2695</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2696" class="pln"><span class="n"><a href="#t2696">2696</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2697" class="pln"><span class="n"><a href="#t2697">2697</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2698" class="pln"><span class="n"><a href="#t2698">2698</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2699" class="pln"><span class="n"><a href="#t2699">2699</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2700" class="pln"><span class="n"><a href="#t2700">2700</a></span><span class="t"><span class="str">    >>> a = torch.arange(0.5, 2, 0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2701" class="pln"><span class="n"><a href="#t2701">2701</a></span><span class="t"><span class="str">    >>> torch.lgamma(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2702" class="pln"><span class="n"><a href="#t2702">2702</a></span><span class="t"><span class="str">    tensor([ 0.5724,  0.0000, -0.1208])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2703" class="pln"><span class="n"><a href="#t2703">2703</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2704" class="pln"><span class="n"><a href="#t2704">2704</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2705" class="run"><span class="n"><a href="#t2705">2705</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">linspace</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2706" class="pln"><span class="n"><a href="#t2706">2706</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2707" class="pln"><span class="n"><a href="#t2707">2707</a></span><span class="t"><span class="str">linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2708" class="pln"><span class="n"><a href="#t2708">2708</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2709" class="pln"><span class="n"><a href="#t2709">2709</a></span><span class="t"><span class="str">Returns a one-dimensional tensor of :attr:`steps`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2710" class="pln"><span class="n"><a href="#t2710">2710</a></span><span class="t"><span class="str">equally spaced points between :attr:`start` and :attr:`end`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2711" class="pln"><span class="n"><a href="#t2711">2711</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2712" class="pln"><span class="n"><a href="#t2712">2712</a></span><span class="t"><span class="str">The output tensor is 1-D of size :attr:`steps`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2713" class="pln"><span class="n"><a href="#t2713">2713</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2714" class="pln"><span class="n"><a href="#t2714">2714</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2715" class="pln"><span class="n"><a href="#t2715">2715</a></span><span class="t"><span class="str">    start (float): the starting value for the set of points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2716" class="pln"><span class="n"><a href="#t2716">2716</a></span><span class="t"><span class="str">    end (float): the ending value for the set of points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2717" class="pln"><span class="n"><a href="#t2717">2717</a></span><span class="t"><span class="str">    steps (int): number of points to sample between :attr:`start`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2718" class="pln"><span class="n"><a href="#t2718">2718</a></span><span class="t"><span class="str">        and :attr:`end`. Default: ``100``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2719" class="pln"><span class="n"><a href="#t2719">2719</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2720" class="pln"><span class="n"><a href="#t2720">2720</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2721" class="pln"><span class="n"><a href="#t2721">2721</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2722" class="pln"><span class="n"><a href="#t2722">2722</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2723" class="pln"><span class="n"><a href="#t2723">2723</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2724" class="pln"><span class="n"><a href="#t2724">2724</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2725" class="pln"><span class="n"><a href="#t2725">2725</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2726" class="pln"><span class="n"><a href="#t2726">2726</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2727" class="pln"><span class="n"><a href="#t2727">2727</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2728" class="pln"><span class="n"><a href="#t2728">2728</a></span><span class="t"><span class="str">    >>> torch.linspace(3, 10, steps=5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2729" class="pln"><span class="n"><a href="#t2729">2729</a></span><span class="t"><span class="str">    tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2730" class="pln"><span class="n"><a href="#t2730">2730</a></span><span class="t"><span class="str">    >>> torch.linspace(-10, 10, steps=5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2731" class="pln"><span class="n"><a href="#t2731">2731</a></span><span class="t"><span class="str">    tensor([-10.,  -5.,   0.,   5.,  10.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2732" class="pln"><span class="n"><a href="#t2732">2732</a></span><span class="t"><span class="str">    >>> torch.linspace(start=-10, end=10, steps=5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2733" class="pln"><span class="n"><a href="#t2733">2733</a></span><span class="t"><span class="str">    tensor([-10.,  -5.,   0.,   5.,  10.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2734" class="pln"><span class="n"><a href="#t2734">2734</a></span><span class="t"><span class="str">    >>> torch.linspace(start=-10, end=10, steps=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2735" class="pln"><span class="n"><a href="#t2735">2735</a></span><span class="t"><span class="str">    tensor([-10.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2736" class="pln"><span class="n"><a href="#t2736">2736</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2737" class="pln"><span class="n"><a href="#t2737">2737</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2738" class="run"><span class="n"><a href="#t2738">2738</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">log</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2739" class="pln"><span class="n"><a href="#t2739">2739</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2740" class="pln"><span class="n"><a href="#t2740">2740</a></span><span class="t"><span class="str">log(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2741" class="pln"><span class="n"><a href="#t2741">2741</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2742" class="pln"><span class="n"><a href="#t2742">2742</a></span><span class="t"><span class="str">Returns a new tensor with the natural logarithm of the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2743" class="pln"><span class="n"><a href="#t2743">2743</a></span><span class="t"><span class="str">of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2744" class="pln"><span class="n"><a href="#t2744">2744</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2745" class="pln"><span class="n"><a href="#t2745">2745</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2746" class="pln"><span class="n"><a href="#t2746">2746</a></span><span class="t"><span class="str">    y_{i} = \log_{e} (x_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2747" class="pln"><span class="n"><a href="#t2747">2747</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2748" class="pln"><span class="n"><a href="#t2748">2748</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2749" class="pln"><span class="n"><a href="#t2749">2749</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2750" class="pln"><span class="n"><a href="#t2750">2750</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2751" class="pln"><span class="n"><a href="#t2751">2751</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2752" class="pln"><span class="n"><a href="#t2752">2752</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2753" class="pln"><span class="n"><a href="#t2753">2753</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2754" class="pln"><span class="n"><a href="#t2754">2754</a></span><span class="t"><span class="str">    >>> a = torch.randn(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2755" class="pln"><span class="n"><a href="#t2755">2755</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2756" class="pln"><span class="n"><a href="#t2756">2756</a></span><span class="t"><span class="str">    tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2757" class="pln"><span class="n"><a href="#t2757">2757</a></span><span class="t"><span class="str">    >>> torch.log(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2758" class="pln"><span class="n"><a href="#t2758">2758</a></span><span class="t"><span class="str">    tensor([ nan,  nan,  nan,  nan,  nan])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2759" class="pln"><span class="n"><a href="#t2759">2759</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2760" class="pln"><span class="n"><a href="#t2760">2760</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2761" class="run"><span class="n"><a href="#t2761">2761</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">log10</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2762" class="pln"><span class="n"><a href="#t2762">2762</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2763" class="pln"><span class="n"><a href="#t2763">2763</a></span><span class="t"><span class="str">log10(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2764" class="pln"><span class="n"><a href="#t2764">2764</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2765" class="pln"><span class="n"><a href="#t2765">2765</a></span><span class="t"><span class="str">Returns a new tensor with the logarithm to the base 10 of the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2766" class="pln"><span class="n"><a href="#t2766">2766</a></span><span class="t"><span class="str">of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2767" class="pln"><span class="n"><a href="#t2767">2767</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2768" class="pln"><span class="n"><a href="#t2768">2768</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2769" class="pln"><span class="n"><a href="#t2769">2769</a></span><span class="t"><span class="str">    y_{i} = \log_{10} (x_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2770" class="pln"><span class="n"><a href="#t2770">2770</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2771" class="pln"><span class="n"><a href="#t2771">2771</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2772" class="pln"><span class="n"><a href="#t2772">2772</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2773" class="pln"><span class="n"><a href="#t2773">2773</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2774" class="pln"><span class="n"><a href="#t2774">2774</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2775" class="pln"><span class="n"><a href="#t2775">2775</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2776" class="pln"><span class="n"><a href="#t2776">2776</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2777" class="pln"><span class="n"><a href="#t2777">2777</a></span><span class="t"><span class="str">    >>> a = torch.rand(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2778" class="pln"><span class="n"><a href="#t2778">2778</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2779" class="pln"><span class="n"><a href="#t2779">2779</a></span><span class="t"><span class="str">    tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2780" class="pln"><span class="n"><a href="#t2780">2780</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2781" class="pln"><span class="n"><a href="#t2781">2781</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2782" class="pln"><span class="n"><a href="#t2782">2782</a></span><span class="t"><span class="str">    >>> torch.log10(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2783" class="pln"><span class="n"><a href="#t2783">2783</a></span><span class="t"><span class="str">    tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2784" class="pln"><span class="n"><a href="#t2784">2784</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2785" class="pln"><span class="n"><a href="#t2785">2785</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2786" class="pln"><span class="n"><a href="#t2786">2786</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2787" class="run"><span class="n"><a href="#t2787">2787</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">log1p</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2788" class="pln"><span class="n"><a href="#t2788">2788</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2789" class="pln"><span class="n"><a href="#t2789">2789</a></span><span class="t"><span class="str">log1p(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2790" class="pln"><span class="n"><a href="#t2790">2790</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2791" class="pln"><span class="n"><a href="#t2791">2791</a></span><span class="t"><span class="str">Returns a new tensor with the natural logarithm of (1 + :attr:`input`).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2792" class="pln"><span class="n"><a href="#t2792">2792</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2793" class="pln"><span class="n"><a href="#t2793">2793</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2794" class="pln"><span class="n"><a href="#t2794">2794</a></span><span class="t"><span class="str">    y_i = \log_{e} (x_i + 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2795" class="pln"><span class="n"><a href="#t2795">2795</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2796" class="pln"><span class="n"><a href="#t2796">2796</a></span><span class="t"><span class="str">.. note:: This function is more accurate than :func:`torch.log` for small</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2797" class="pln"><span class="n"><a href="#t2797">2797</a></span><span class="t"><span class="str">          values of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2798" class="pln"><span class="n"><a href="#t2798">2798</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2799" class="pln"><span class="n"><a href="#t2799">2799</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2800" class="pln"><span class="n"><a href="#t2800">2800</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2801" class="pln"><span class="n"><a href="#t2801">2801</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2802" class="pln"><span class="n"><a href="#t2802">2802</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2803" class="pln"><span class="n"><a href="#t2803">2803</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2804" class="pln"><span class="n"><a href="#t2804">2804</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2805" class="pln"><span class="n"><a href="#t2805">2805</a></span><span class="t"><span class="str">    >>> a = torch.randn(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2806" class="pln"><span class="n"><a href="#t2806">2806</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2807" class="pln"><span class="n"><a href="#t2807">2807</a></span><span class="t"><span class="str">    tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2808" class="pln"><span class="n"><a href="#t2808">2808</a></span><span class="t"><span class="str">    >>> torch.log1p(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2809" class="pln"><span class="n"><a href="#t2809">2809</a></span><span class="t"><span class="str">    tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2810" class="pln"><span class="n"><a href="#t2810">2810</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2811" class="pln"><span class="n"><a href="#t2811">2811</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2812" class="run"><span class="n"><a href="#t2812">2812</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">log2</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2813" class="pln"><span class="n"><a href="#t2813">2813</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2814" class="pln"><span class="n"><a href="#t2814">2814</a></span><span class="t"><span class="str">log2(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2815" class="pln"><span class="n"><a href="#t2815">2815</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2816" class="pln"><span class="n"><a href="#t2816">2816</a></span><span class="t"><span class="str">Returns a new tensor with the logarithm to the base 2 of the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2817" class="pln"><span class="n"><a href="#t2817">2817</a></span><span class="t"><span class="str">of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2818" class="pln"><span class="n"><a href="#t2818">2818</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2819" class="pln"><span class="n"><a href="#t2819">2819</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2820" class="pln"><span class="n"><a href="#t2820">2820</a></span><span class="t"><span class="str">    y_{i} = \log_{2} (x_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2821" class="pln"><span class="n"><a href="#t2821">2821</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2822" class="pln"><span class="n"><a href="#t2822">2822</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2823" class="pln"><span class="n"><a href="#t2823">2823</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2824" class="pln"><span class="n"><a href="#t2824">2824</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2825" class="pln"><span class="n"><a href="#t2825">2825</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2826" class="pln"><span class="n"><a href="#t2826">2826</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2827" class="pln"><span class="n"><a href="#t2827">2827</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2828" class="pln"><span class="n"><a href="#t2828">2828</a></span><span class="t"><span class="str">    >>> a = torch.rand(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2829" class="pln"><span class="n"><a href="#t2829">2829</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2830" class="pln"><span class="n"><a href="#t2830">2830</a></span><span class="t"><span class="str">    tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2831" class="pln"><span class="n"><a href="#t2831">2831</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2832" class="pln"><span class="n"><a href="#t2832">2832</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2833" class="pln"><span class="n"><a href="#t2833">2833</a></span><span class="t"><span class="str">    >>> torch.log2(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2834" class="pln"><span class="n"><a href="#t2834">2834</a></span><span class="t"><span class="str">    tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2835" class="pln"><span class="n"><a href="#t2835">2835</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2836" class="pln"><span class="n"><a href="#t2836">2836</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2837" class="pln"><span class="n"><a href="#t2837">2837</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2838" class="run"><span class="n"><a href="#t2838">2838</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logical_and</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2839" class="pln"><span class="n"><a href="#t2839">2839</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2840" class="pln"><span class="n"><a href="#t2840">2840</a></span><span class="t"><span class="str">logical_and(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2841" class="pln"><span class="n"><a href="#t2841">2841</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2842" class="pln"><span class="n"><a href="#t2842">2842</a></span><span class="t"><span class="str">Computes the element-wise logical AND of the given input tensors. Zeros are treated as ``False`` and nonzeros are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2843" class="pln"><span class="n"><a href="#t2843">2843</a></span><span class="t"><span class="str">treated as ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2844" class="pln"><span class="n"><a href="#t2844">2844</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2845" class="pln"><span class="n"><a href="#t2845">2845</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2846" class="pln"><span class="n"><a href="#t2846">2846</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2847" class="pln"><span class="n"><a href="#t2847">2847</a></span><span class="t"><span class="str">    other (Tensor): the tensor to compute AND with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2848" class="pln"><span class="n"><a href="#t2848">2848</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2849" class="pln"><span class="n"><a href="#t2849">2849</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2850" class="pln"><span class="n"><a href="#t2850">2850</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2851" class="pln"><span class="n"><a href="#t2851">2851</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2852" class="pln"><span class="n"><a href="#t2852">2852</a></span><span class="t"><span class="str">    >>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2853" class="pln"><span class="n"><a href="#t2853">2853</a></span><span class="t"><span class="str">    tensor([ True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2854" class="pln"><span class="n"><a href="#t2854">2854</a></span><span class="t"><span class="str">    >>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2855" class="pln"><span class="n"><a href="#t2855">2855</a></span><span class="t"><span class="str">    >>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2856" class="pln"><span class="n"><a href="#t2856">2856</a></span><span class="t"><span class="str">    >>> torch.logical_and(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2857" class="pln"><span class="n"><a href="#t2857">2857</a></span><span class="t"><span class="str">    tensor([False, False,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2858" class="pln"><span class="n"><a href="#t2858">2858</a></span><span class="t"><span class="str">    >>> torch.logical_and(a.double(), b.double())</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2859" class="pln"><span class="n"><a href="#t2859">2859</a></span><span class="t"><span class="str">    tensor([False, False,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2860" class="pln"><span class="n"><a href="#t2860">2860</a></span><span class="t"><span class="str">    >>> torch.logical_and(a.double(), b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2861" class="pln"><span class="n"><a href="#t2861">2861</a></span><span class="t"><span class="str">    tensor([False, False,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2862" class="pln"><span class="n"><a href="#t2862">2862</a></span><span class="t"><span class="str">    >>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2863" class="pln"><span class="n"><a href="#t2863">2863</a></span><span class="t"><span class="str">    tensor([False, False,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2864" class="pln"><span class="n"><a href="#t2864">2864</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2865" class="pln"><span class="n"><a href="#t2865">2865</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2866" class="run"><span class="n"><a href="#t2866">2866</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logical_not</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2867" class="pln"><span class="n"><a href="#t2867">2867</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2868" class="pln"><span class="n"><a href="#t2868">2868</a></span><span class="t"><span class="str">logical_not(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2869" class="pln"><span class="n"><a href="#t2869">2869</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2870" class="pln"><span class="n"><a href="#t2870">2870</a></span><span class="t"><span class="str">Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2871" class="pln"><span class="n"><a href="#t2871">2871</a></span><span class="t"><span class="str">dtype. If the input tensor is not a bool tensor, zeros are treated as ``False`` and non-zeros are treated as ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2872" class="pln"><span class="n"><a href="#t2872">2872</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2873" class="pln"><span class="n"><a href="#t2873">2873</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2874" class="pln"><span class="n"><a href="#t2874">2874</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2875" class="pln"><span class="n"><a href="#t2875">2875</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2876" class="pln"><span class="n"><a href="#t2876">2876</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2877" class="pln"><span class="n"><a href="#t2877">2877</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2878" class="pln"><span class="n"><a href="#t2878">2878</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2879" class="pln"><span class="n"><a href="#t2879">2879</a></span><span class="t"><span class="str">    >>> torch.logical_not(torch.tensor([True, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2880" class="pln"><span class="n"><a href="#t2880">2880</a></span><span class="t"><span class="str">    tensor([False,  True])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2881" class="pln"><span class="n"><a href="#t2881">2881</a></span><span class="t"><span class="str">    >>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2882" class="pln"><span class="n"><a href="#t2882">2882</a></span><span class="t"><span class="str">    tensor([ True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2883" class="pln"><span class="n"><a href="#t2883">2883</a></span><span class="t"><span class="str">    >>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2884" class="pln"><span class="n"><a href="#t2884">2884</a></span><span class="t"><span class="str">    tensor([ True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2885" class="pln"><span class="n"><a href="#t2885">2885</a></span><span class="t"><span class="str">    >>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2886" class="pln"><span class="n"><a href="#t2886">2886</a></span><span class="t"><span class="str">    tensor([1, 0, 0], dtype=torch.int16)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2887" class="pln"><span class="n"><a href="#t2887">2887</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2888" class="pln"><span class="n"><a href="#t2888">2888</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2889" class="run"><span class="n"><a href="#t2889">2889</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logical_or</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2890" class="pln"><span class="n"><a href="#t2890">2890</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2891" class="pln"><span class="n"><a href="#t2891">2891</a></span><span class="t"><span class="str">logical_or(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2892" class="pln"><span class="n"><a href="#t2892">2892</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2893" class="pln"><span class="n"><a href="#t2893">2893</a></span><span class="t"><span class="str">Computes the element-wise logical OR of the given input tensors. Zeros are treated as ``False`` and nonzeros are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2894" class="pln"><span class="n"><a href="#t2894">2894</a></span><span class="t"><span class="str">treated as ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2895" class="pln"><span class="n"><a href="#t2895">2895</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2896" class="pln"><span class="n"><a href="#t2896">2896</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2897" class="pln"><span class="n"><a href="#t2897">2897</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2898" class="pln"><span class="n"><a href="#t2898">2898</a></span><span class="t"><span class="str">    other (Tensor): the tensor to compute OR with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2899" class="pln"><span class="n"><a href="#t2899">2899</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2900" class="pln"><span class="n"><a href="#t2900">2900</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2901" class="pln"><span class="n"><a href="#t2901">2901</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2902" class="pln"><span class="n"><a href="#t2902">2902</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2903" class="pln"><span class="n"><a href="#t2903">2903</a></span><span class="t"><span class="str">    >>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2904" class="pln"><span class="n"><a href="#t2904">2904</a></span><span class="t"><span class="str">    tensor([ True, False,  True])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2905" class="pln"><span class="n"><a href="#t2905">2905</a></span><span class="t"><span class="str">    >>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2906" class="pln"><span class="n"><a href="#t2906">2906</a></span><span class="t"><span class="str">    >>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2907" class="pln"><span class="n"><a href="#t2907">2907</a></span><span class="t"><span class="str">    >>> torch.logical_or(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2908" class="pln"><span class="n"><a href="#t2908">2908</a></span><span class="t"><span class="str">    tensor([ True,  True,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2909" class="pln"><span class="n"><a href="#t2909">2909</a></span><span class="t"><span class="str">    >>> torch.logical_or(a.double(), b.double())</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2910" class="pln"><span class="n"><a href="#t2910">2910</a></span><span class="t"><span class="str">    tensor([ True,  True,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2911" class="pln"><span class="n"><a href="#t2911">2911</a></span><span class="t"><span class="str">    >>> torch.logical_or(a.double(), b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2912" class="pln"><span class="n"><a href="#t2912">2912</a></span><span class="t"><span class="str">    tensor([ True,  True,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2913" class="pln"><span class="n"><a href="#t2913">2913</a></span><span class="t"><span class="str">    >>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2914" class="pln"><span class="n"><a href="#t2914">2914</a></span><span class="t"><span class="str">    tensor([ True,  True,  True, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2915" class="pln"><span class="n"><a href="#t2915">2915</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2916" class="pln"><span class="n"><a href="#t2916">2916</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2917" class="run"><span class="n"><a href="#t2917">2917</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logical_xor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2918" class="pln"><span class="n"><a href="#t2918">2918</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2919" class="pln"><span class="n"><a href="#t2919">2919</a></span><span class="t"><span class="str">logical_xor(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2920" class="pln"><span class="n"><a href="#t2920">2920</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2921" class="pln"><span class="n"><a href="#t2921">2921</a></span><span class="t"><span class="str">Computes the element-wise logical XOR of the given input tensors. Zeros are treated as ``False`` and nonzeros are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2922" class="pln"><span class="n"><a href="#t2922">2922</a></span><span class="t"><span class="str">treated as ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2923" class="pln"><span class="n"><a href="#t2923">2923</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2924" class="pln"><span class="n"><a href="#t2924">2924</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2925" class="pln"><span class="n"><a href="#t2925">2925</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2926" class="pln"><span class="n"><a href="#t2926">2926</a></span><span class="t"><span class="str">    other (Tensor): the tensor to compute XOR with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2927" class="pln"><span class="n"><a href="#t2927">2927</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2928" class="pln"><span class="n"><a href="#t2928">2928</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2929" class="pln"><span class="n"><a href="#t2929">2929</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2930" class="pln"><span class="n"><a href="#t2930">2930</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2931" class="pln"><span class="n"><a href="#t2931">2931</a></span><span class="t"><span class="str">    >>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2932" class="pln"><span class="n"><a href="#t2932">2932</a></span><span class="t"><span class="str">    tensor([False, False,  True])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2933" class="pln"><span class="n"><a href="#t2933">2933</a></span><span class="t"><span class="str">    >>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2934" class="pln"><span class="n"><a href="#t2934">2934</a></span><span class="t"><span class="str">    >>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2935" class="pln"><span class="n"><a href="#t2935">2935</a></span><span class="t"><span class="str">    >>> torch.logical_xor(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2936" class="pln"><span class="n"><a href="#t2936">2936</a></span><span class="t"><span class="str">    tensor([ True,  True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2937" class="pln"><span class="n"><a href="#t2937">2937</a></span><span class="t"><span class="str">    >>> torch.logical_xor(a.double(), b.double())</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2938" class="pln"><span class="n"><a href="#t2938">2938</a></span><span class="t"><span class="str">    tensor([ True,  True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2939" class="pln"><span class="n"><a href="#t2939">2939</a></span><span class="t"><span class="str">    >>> torch.logical_xor(a.double(), b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2940" class="pln"><span class="n"><a href="#t2940">2940</a></span><span class="t"><span class="str">    tensor([ True,  True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2941" class="pln"><span class="n"><a href="#t2941">2941</a></span><span class="t"><span class="str">    >>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2942" class="pln"><span class="n"><a href="#t2942">2942</a></span><span class="t"><span class="str">    tensor([ True,  True, False, False])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2943" class="pln"><span class="n"><a href="#t2943">2943</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2944" class="pln"><span class="n"><a href="#t2944">2944</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2945" class="run"><span class="n"><a href="#t2945">2945</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logspace</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2946" class="pln"><span class="n"><a href="#t2946">2946</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2947" class="pln"><span class="n"><a href="#t2947">2947</a></span><span class="t"><span class="str">logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2948" class="pln"><span class="n"><a href="#t2948">2948</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2949" class="pln"><span class="n"><a href="#t2949">2949</a></span><span class="t"><span class="str">Returns a one-dimensional tensor of :attr:`steps` points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2950" class="pln"><span class="n"><a href="#t2950">2950</a></span><span class="t"><span class="str">logarithmically spaced with base :attr:`base` between</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2951" class="pln"><span class="n"><a href="#t2951">2951</a></span><span class="t"><span class="str">:math:`{{\text{{base}}}}^{{\text{{start}}}}` and :math:`{{\text{{base}}}}^{{\text{{end}}}}`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2952" class="pln"><span class="n"><a href="#t2952">2952</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2953" class="pln"><span class="n"><a href="#t2953">2953</a></span><span class="t"><span class="str">The output tensor is 1-D of size :attr:`steps`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2954" class="pln"><span class="n"><a href="#t2954">2954</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2955" class="pln"><span class="n"><a href="#t2955">2955</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2956" class="pln"><span class="n"><a href="#t2956">2956</a></span><span class="t"><span class="str">    start (float): the starting value for the set of points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2957" class="pln"><span class="n"><a href="#t2957">2957</a></span><span class="t"><span class="str">    end (float): the ending value for the set of points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2958" class="pln"><span class="n"><a href="#t2958">2958</a></span><span class="t"><span class="str">    steps (int): number of points to sample between :attr:`start`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2959" class="pln"><span class="n"><a href="#t2959">2959</a></span><span class="t"><span class="str">        and :attr:`end`. Default: ``100``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2960" class="pln"><span class="n"><a href="#t2960">2960</a></span><span class="t"><span class="str">    base (float): base of the logarithm function. Default: ``10.0``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2961" class="pln"><span class="n"><a href="#t2961">2961</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2962" class="pln"><span class="n"><a href="#t2962">2962</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2963" class="pln"><span class="n"><a href="#t2963">2963</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2964" class="pln"><span class="n"><a href="#t2964">2964</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2965" class="pln"><span class="n"><a href="#t2965">2965</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2966" class="pln"><span class="n"><a href="#t2966">2966</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2967" class="pln"><span class="n"><a href="#t2967">2967</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2968" class="pln"><span class="n"><a href="#t2968">2968</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2969" class="pln"><span class="n"><a href="#t2969">2969</a></span><span class="t"><span class="str">    >>> torch.logspace(start=-10, end=10, steps=5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2970" class="pln"><span class="n"><a href="#t2970">2970</a></span><span class="t"><span class="str">    tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2971" class="pln"><span class="n"><a href="#t2971">2971</a></span><span class="t"><span class="str">    >>> torch.logspace(start=0.1, end=1.0, steps=5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2972" class="pln"><span class="n"><a href="#t2972">2972</a></span><span class="t"><span class="str">    tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2973" class="pln"><span class="n"><a href="#t2973">2973</a></span><span class="t"><span class="str">    >>> torch.logspace(start=0.1, end=1.0, steps=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2974" class="pln"><span class="n"><a href="#t2974">2974</a></span><span class="t"><span class="str">    tensor([1.2589])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2975" class="pln"><span class="n"><a href="#t2975">2975</a></span><span class="t"><span class="str">    >>> torch.logspace(start=2, end=2, steps=1, base=2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2976" class="pln"><span class="n"><a href="#t2976">2976</a></span><span class="t"><span class="str">    tensor([4.0])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2977" class="pln"><span class="n"><a href="#t2977">2977</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2978" class="pln"><span class="n"><a href="#t2978">2978</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2979" class="run"><span class="n"><a href="#t2979">2979</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logsumexp</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2980" class="pln"><span class="n"><a href="#t2980">2980</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2981" class="pln"><span class="n"><a href="#t2981">2981</a></span><span class="t"><span class="str">logsumexp(input, dim, keepdim=False, out=None)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2982" class="pln"><span class="n"><a href="#t2982">2982</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2983" class="pln"><span class="n"><a href="#t2983">2983</a></span><span class="t"><span class="str">Returns the log of summed exponentials of each row of the :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2984" class="pln"><span class="n"><a href="#t2984">2984</a></span><span class="t"><span class="str">tensor in the given dimension :attr:`dim`. The computation is numerically</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2985" class="pln"><span class="n"><a href="#t2985">2985</a></span><span class="t"><span class="str">stabilized.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2986" class="pln"><span class="n"><a href="#t2986">2986</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2987" class="pln"><span class="n"><a href="#t2987">2987</a></span><span class="t"><span class="str">For summation index :math:`j` given by `dim` and other indices :math:`i`, the result is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2988" class="pln"><span class="n"><a href="#t2988">2988</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2989" class="pln"><span class="n"><a href="#t2989">2989</a></span><span class="t"><span class="str">    .. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2990" class="pln"><span class="n"><a href="#t2990">2990</a></span><span class="t"><span class="str">        \text{{logsumexp}}(x)_{{i}} = \log \sum_j \exp(x_{{ij}})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2991" class="pln"><span class="n"><a href="#t2991">2991</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2992" class="pln"><span class="n"><a href="#t2992">2992</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2993" class="pln"><span class="n"><a href="#t2993">2993</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t2994" class="pln"><span class="n"><a href="#t2994">2994</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2995" class="pln"><span class="n"><a href="#t2995">2995</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2996" class="pln"><span class="n"><a href="#t2996">2996</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2997" class="pln"><span class="n"><a href="#t2997">2997</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2998" class="pln"><span class="n"><a href="#t2998">2998</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2999" class="pln"><span class="n"><a href="#t2999">2999</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3000" class="pln"><span class="n"><a href="#t3000">3000</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3001" class="pln"><span class="n"><a href="#t3001">3001</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3002" class="pln"><span class="n"><a href="#t3002">3002</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3003" class="pln"><span class="n"><a href="#t3003">3003</a></span><span class="t"><span class="str">    >>> torch.logsumexp(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3004" class="pln"><span class="n"><a href="#t3004">3004</a></span><span class="t"><span class="str">    tensor([ 0.8442,  1.4322,  0.8711])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3005" class="pln"><span class="n"><a href="#t3005">3005</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3006" class="pln"><span class="n"><a href="#t3006">3006</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3007" class="run"><span class="n"><a href="#t3007">3007</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">lstsq</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3008" class="pln"><span class="n"><a href="#t3008">3008</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3009" class="pln"><span class="n"><a href="#t3009">3009</a></span><span class="t"><span class="str">lstsq(input, A, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3010" class="pln"><span class="n"><a href="#t3010">3010</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3011" class="pln"><span class="n"><a href="#t3011">3011</a></span><span class="t"><span class="str">Computes the solution to the least squares and least norm problems for a full</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3012" class="pln"><span class="n"><a href="#t3012">3012</a></span><span class="t"><span class="str">rank matrix :math:`A` of size :math:`(m \times n)` and a matrix :math:`B` of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3013" class="pln"><span class="n"><a href="#t3013">3013</a></span><span class="t"><span class="str">size :math:`(m \times k)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3014" class="pln"><span class="n"><a href="#t3014">3014</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3015" class="pln"><span class="n"><a href="#t3015">3015</a></span><span class="t"><span class="str">If :math:`m \geq n`, :func:`lstsq` solves the least-squares problem:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3016" class="pln"><span class="n"><a href="#t3016">3016</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3017" class="pln"><span class="n"><a href="#t3017">3017</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3018" class="pln"><span class="n"><a href="#t3018">3018</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3019" class="pln"><span class="n"><a href="#t3019">3019</a></span><span class="t"><span class="str">   \begin{array}{ll}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3020" class="pln"><span class="n"><a href="#t3020">3020</a></span><span class="t"><span class="str">   \min_X &amp; \|AX-B\|_2.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3021" class="pln"><span class="n"><a href="#t3021">3021</a></span><span class="t"><span class="str">   \end{array}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3022" class="pln"><span class="n"><a href="#t3022">3022</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3023" class="pln"><span class="n"><a href="#t3023">3023</a></span><span class="t"><span class="str">If :math:`m &lt; n`, :func:`lstsq` solves the least-norm problem:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3024" class="pln"><span class="n"><a href="#t3024">3024</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3025" class="pln"><span class="n"><a href="#t3025">3025</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3026" class="pln"><span class="n"><a href="#t3026">3026</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3027" class="pln"><span class="n"><a href="#t3027">3027</a></span><span class="t"><span class="str">   \begin{array}{ll}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3028" class="pln"><span class="n"><a href="#t3028">3028</a></span><span class="t"><span class="str">   \min_X &amp; \|X\|_2 &amp; \text{subject to} &amp; AX = B.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3029" class="pln"><span class="n"><a href="#t3029">3029</a></span><span class="t"><span class="str">   \end{array}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3030" class="pln"><span class="n"><a href="#t3030">3030</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3031" class="pln"><span class="n"><a href="#t3031">3031</a></span><span class="t"><span class="str">Returned tensor :math:`X` has shape :math:`(\max(m, n) \times k)`. The first :math:`n`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3032" class="pln"><span class="n"><a href="#t3032">3032</a></span><span class="t"><span class="str">rows of :math:`X` contains the solution. If :math:`m \geq n`, the residual sum of squares</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3033" class="pln"><span class="n"><a href="#t3033">3033</a></span><span class="t"><span class="str">for the solution in each column is given by the sum of squares of elements in the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3034" class="pln"><span class="n"><a href="#t3034">3034</a></span><span class="t"><span class="str">remaining :math:`m - n` rows of that column.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3035" class="pln"><span class="n"><a href="#t3035">3035</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3036" class="pln"><span class="n"><a href="#t3036">3036</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3037" class="pln"><span class="n"><a href="#t3037">3037</a></span><span class="t"><span class="str">    The case when :math:`m &lt; n` is not supported on the GPU.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3038" class="pln"><span class="n"><a href="#t3038">3038</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3039" class="pln"><span class="n"><a href="#t3039">3039</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3040" class="pln"><span class="n"><a href="#t3040">3040</a></span><span class="t"><span class="str">    input (Tensor): the matrix :math:`B`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3041" class="pln"><span class="n"><a href="#t3041">3041</a></span><span class="t"><span class="str">    A (Tensor): the :math:`m` by :math:`n` matrix :math:`A`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3042" class="pln"><span class="n"><a href="#t3042">3042</a></span><span class="t"><span class="str">    out (tuple, optional): the optional destination tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3043" class="pln"><span class="n"><a href="#t3043">3043</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3044" class="pln"><span class="n"><a href="#t3044">3044</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3045" class="pln"><span class="n"><a href="#t3045">3045</a></span><span class="t"><span class="str">    (Tensor, Tensor): A namedtuple (solution, QR) containing:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3046" class="pln"><span class="n"><a href="#t3046">3046</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3047" class="pln"><span class="n"><a href="#t3047">3047</a></span><span class="t"><span class="str">        - **solution** (*Tensor*): the least squares solution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3048" class="pln"><span class="n"><a href="#t3048">3048</a></span><span class="t"><span class="str">        - **QR** (*Tensor*): the details of the QR factorization</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3049" class="pln"><span class="n"><a href="#t3049">3049</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3050" class="pln"><span class="n"><a href="#t3050">3050</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3051" class="pln"><span class="n"><a href="#t3051">3051</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3052" class="pln"><span class="n"><a href="#t3052">3052</a></span><span class="t"><span class="str">    The returned matrices will always be transposed, irrespective of the strides</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3053" class="pln"><span class="n"><a href="#t3053">3053</a></span><span class="t"><span class="str">    of the input matrices. That is, they will have stride `(1, m)` instead of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3054" class="pln"><span class="n"><a href="#t3054">3054</a></span><span class="t"><span class="str">    `(m, 1)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3055" class="pln"><span class="n"><a href="#t3055">3055</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3056" class="pln"><span class="n"><a href="#t3056">3056</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3057" class="pln"><span class="n"><a href="#t3057">3057</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3058" class="pln"><span class="n"><a href="#t3058">3058</a></span><span class="t"><span class="str">    >>> A = torch.tensor([[1., 1, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3059" class="pln"><span class="n"><a href="#t3059">3059</a></span><span class="t"><span class="str">                          [2, 3, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3060" class="pln"><span class="n"><a href="#t3060">3060</a></span><span class="t"><span class="str">                          [3, 5, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3061" class="pln"><span class="n"><a href="#t3061">3061</a></span><span class="t"><span class="str">                          [4, 2, 5],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3062" class="pln"><span class="n"><a href="#t3062">3062</a></span><span class="t"><span class="str">                          [5, 4, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3063" class="pln"><span class="n"><a href="#t3063">3063</a></span><span class="t"><span class="str">    >>> B = torch.tensor([[-10., -3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3064" class="pln"><span class="n"><a href="#t3064">3064</a></span><span class="t"><span class="str">                          [ 12, 14],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3065" class="pln"><span class="n"><a href="#t3065">3065</a></span><span class="t"><span class="str">                          [ 14, 12],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3066" class="pln"><span class="n"><a href="#t3066">3066</a></span><span class="t"><span class="str">                          [ 16, 16],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3067" class="pln"><span class="n"><a href="#t3067">3067</a></span><span class="t"><span class="str">                          [ 18, 16]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3068" class="pln"><span class="n"><a href="#t3068">3068</a></span><span class="t"><span class="str">    >>> X, _ = torch.lstsq(B, A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3069" class="pln"><span class="n"><a href="#t3069">3069</a></span><span class="t"><span class="str">    >>> X</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3070" class="pln"><span class="n"><a href="#t3070">3070</a></span><span class="t"><span class="str">    tensor([[  2.0000,   1.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3071" class="pln"><span class="n"><a href="#t3071">3071</a></span><span class="t"><span class="str">            [  1.0000,   1.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3072" class="pln"><span class="n"><a href="#t3072">3072</a></span><span class="t"><span class="str">            [  1.0000,   2.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3073" class="pln"><span class="n"><a href="#t3073">3073</a></span><span class="t"><span class="str">            [ 10.9635,   4.8501],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3074" class="pln"><span class="n"><a href="#t3074">3074</a></span><span class="t"><span class="str">            [  8.9332,   5.2418]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3075" class="pln"><span class="n"><a href="#t3075">3075</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3076" class="pln"><span class="n"><a href="#t3076">3076</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3077" class="run"><span class="n"><a href="#t3077">3077</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">lt</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3078" class="pln"><span class="n"><a href="#t3078">3078</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3079" class="pln"><span class="n"><a href="#t3079">3079</a></span><span class="t"><span class="str">lt(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3080" class="pln"><span class="n"><a href="#t3080">3080</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3081" class="pln"><span class="n"><a href="#t3081">3081</a></span><span class="t"><span class="str">Computes :math:`\text{input} &lt; \text{other}` element-wise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3082" class="pln"><span class="n"><a href="#t3082">3082</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3083" class="pln"><span class="n"><a href="#t3083">3083</a></span><span class="t"><span class="str">The second argument can be a number or a tensor whose shape is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3084" class="pln"><span class="n"><a href="#t3084">3084</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with the first argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3085" class="pln"><span class="n"><a href="#t3085">3085</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3086" class="pln"><span class="n"><a href="#t3086">3086</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3087" class="pln"><span class="n"><a href="#t3087">3087</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3088" class="pln"><span class="n"><a href="#t3088">3088</a></span><span class="t"><span class="str">    other (Tensor or float): the tensor or value to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3089" class="pln"><span class="n"><a href="#t3089">3089</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor that must be a `BoolTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3090" class="pln"><span class="n"><a href="#t3090">3090</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3091" class="pln"><span class="n"><a href="#t3091">3091</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3092" class="pln"><span class="n"><a href="#t3092">3092</a></span><span class="t"><span class="str">    Tensor: A `torch.BoolTensor` containing a True at each location where comparison is true</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3093" class="pln"><span class="n"><a href="#t3093">3093</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3094" class="pln"><span class="n"><a href="#t3094">3094</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3095" class="pln"><span class="n"><a href="#t3095">3095</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3096" class="pln"><span class="n"><a href="#t3096">3096</a></span><span class="t"><span class="str">    >>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3097" class="pln"><span class="n"><a href="#t3097">3097</a></span><span class="t"><span class="str">    tensor([[False, False], [True, False]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3098" class="pln"><span class="n"><a href="#t3098">3098</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3099" class="pln"><span class="n"><a href="#t3099">3099</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3100" class="run"><span class="n"><a href="#t3100">3100</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">lu_solve</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3101" class="pln"><span class="n"><a href="#t3101">3101</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3102" class="pln"><span class="n"><a href="#t3102">3102</a></span><span class="t"><span class="str">lu_solve(input, LU_data, LU_pivots, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3103" class="pln"><span class="n"><a href="#t3103">3103</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3104" class="pln"><span class="n"><a href="#t3104">3104</a></span><span class="t"><span class="str">Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3105" class="pln"><span class="n"><a href="#t3105">3105</a></span><span class="t"><span class="str">LU factorization of A from :meth:`torch.lu`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3106" class="pln"><span class="n"><a href="#t3106">3106</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3107" class="pln"><span class="n"><a href="#t3107">3107</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3108" class="pln"><span class="n"><a href="#t3108">3108</a></span><span class="t"><span class="str">    b (Tensor): the RHS tensor of size :math:`(*, m, k)`, where :math:`*`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3109" class="pln"><span class="n"><a href="#t3109">3109</a></span><span class="t"><span class="str">                is zero or more batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3110" class="pln"><span class="n"><a href="#t3110">3110</a></span><span class="t"><span class="str">    LU_data (Tensor): the pivoted LU factorization of A from :meth:`torch.lu` of size :math:`(*, m, m)`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3111" class="pln"><span class="n"><a href="#t3111">3111</a></span><span class="t"><span class="str">                       where :math:`*` is zero or more batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3112" class="pln"><span class="n"><a href="#t3112">3112</a></span><span class="t"><span class="str">    LU_pivots (IntTensor): the pivots of the LU factorization from :meth:`torch.lu` of size :math:`(*, m)`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3113" class="pln"><span class="n"><a href="#t3113">3113</a></span><span class="t"><span class="str">                           where :math:`*` is zero or more batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3114" class="pln"><span class="n"><a href="#t3114">3114</a></span><span class="t"><span class="str">                           The batch dimensions of :attr:`LU_pivots` must be equal to the batch dimensions of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3115" class="pln"><span class="n"><a href="#t3115">3115</a></span><span class="t"><span class="str">                           :attr:`LU_data`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3116" class="pln"><span class="n"><a href="#t3116">3116</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3117" class="pln"><span class="n"><a href="#t3117">3117</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3118" class="pln"><span class="n"><a href="#t3118">3118</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3119" class="pln"><span class="n"><a href="#t3119">3119</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3120" class="pln"><span class="n"><a href="#t3120">3120</a></span><span class="t"><span class="str">    >>> A = torch.randn(2, 3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3121" class="pln"><span class="n"><a href="#t3121">3121</a></span><span class="t"><span class="str">    >>> b = torch.randn(2, 3, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3122" class="pln"><span class="n"><a href="#t3122">3122</a></span><span class="t"><span class="str">    >>> A_LU = torch.lu(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3123" class="pln"><span class="n"><a href="#t3123">3123</a></span><span class="t"><span class="str">    >>> x = torch.lu_solve(b, *A_LU)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3124" class="pln"><span class="n"><a href="#t3124">3124</a></span><span class="t"><span class="str">    >>> torch.norm(torch.bmm(A, x) - b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3125" class="pln"><span class="n"><a href="#t3125">3125</a></span><span class="t"><span class="str">    tensor(1.00000e-07 *</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3126" class="pln"><span class="n"><a href="#t3126">3126</a></span><span class="t"><span class="str">           2.8312)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3127" class="pln"><span class="n"><a href="#t3127">3127</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3128" class="pln"><span class="n"><a href="#t3128">3128</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3129" class="run"><span class="n"><a href="#t3129">3129</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">masked_select</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3130" class="pln"><span class="n"><a href="#t3130">3130</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3131" class="pln"><span class="n"><a href="#t3131">3131</a></span><span class="t"><span class="str">masked_select(input, mask, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3132" class="pln"><span class="n"><a href="#t3132">3132</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3133" class="pln"><span class="n"><a href="#t3133">3133</a></span><span class="t"><span class="str">Returns a new 1-D tensor which indexes the :attr:`input` tensor according to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3134" class="pln"><span class="n"><a href="#t3134">3134</a></span><span class="t"><span class="str">the boolean mask :attr:`mask` which is a `BoolTensor`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3135" class="pln"><span class="n"><a href="#t3135">3135</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3136" class="pln"><span class="n"><a href="#t3136">3136</a></span><span class="t"><span class="str">The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3137" class="pln"><span class="n"><a href="#t3137">3137</a></span><span class="t"><span class="str">to match, but they must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3138" class="pln"><span class="n"><a href="#t3138">3138</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3139" class="pln"><span class="n"><a href="#t3139">3139</a></span><span class="t"><span class="str">.. note:: The returned tensor does **not** use the same storage</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3140" class="pln"><span class="n"><a href="#t3140">3140</a></span><span class="t"><span class="str">          as the original tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3141" class="pln"><span class="n"><a href="#t3141">3141</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3142" class="pln"><span class="n"><a href="#t3142">3142</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3143" class="pln"><span class="n"><a href="#t3143">3143</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3144" class="pln"><span class="n"><a href="#t3144">3144</a></span><span class="t"><span class="str">    mask  (BoolTensor): the tensor containing the binary mask to index with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3145" class="pln"><span class="n"><a href="#t3145">3145</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3146" class="pln"><span class="n"><a href="#t3146">3146</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3147" class="pln"><span class="n"><a href="#t3147">3147</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3148" class="pln"><span class="n"><a href="#t3148">3148</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3149" class="pln"><span class="n"><a href="#t3149">3149</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3150" class="pln"><span class="n"><a href="#t3150">3150</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3151" class="pln"><span class="n"><a href="#t3151">3151</a></span><span class="t"><span class="str">    tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3152" class="pln"><span class="n"><a href="#t3152">3152</a></span><span class="t"><span class="str">            [-1.2035,  1.2252,  0.5002,  0.6248],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3153" class="pln"><span class="n"><a href="#t3153">3153</a></span><span class="t"><span class="str">            [ 0.1307, -2.0608,  0.1244,  2.0139]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3154" class="pln"><span class="n"><a href="#t3154">3154</a></span><span class="t"><span class="str">    >>> mask = x.ge(0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3155" class="pln"><span class="n"><a href="#t3155">3155</a></span><span class="t"><span class="str">    >>> mask</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3156" class="pln"><span class="n"><a href="#t3156">3156</a></span><span class="t"><span class="str">    tensor([[False, False, False, False],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3157" class="pln"><span class="n"><a href="#t3157">3157</a></span><span class="t"><span class="str">            [False, True, True, True],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3158" class="pln"><span class="n"><a href="#t3158">3158</a></span><span class="t"><span class="str">            [False, False, False, True]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3159" class="pln"><span class="n"><a href="#t3159">3159</a></span><span class="t"><span class="str">    >>> torch.masked_select(x, mask)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3160" class="pln"><span class="n"><a href="#t3160">3160</a></span><span class="t"><span class="str">    tensor([ 1.2252,  0.5002,  0.6248,  2.0139])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3161" class="pln"><span class="n"><a href="#t3161">3161</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3162" class="pln"><span class="n"><a href="#t3162">3162</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3163" class="run"><span class="n"><a href="#t3163">3163</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">matrix_rank</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3164" class="pln"><span class="n"><a href="#t3164">3164</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3165" class="pln"><span class="n"><a href="#t3165">3165</a></span><span class="t"><span class="str">matrix_rank(input, tol=None, symmetric=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3166" class="pln"><span class="n"><a href="#t3166">3166</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3167" class="pln"><span class="n"><a href="#t3167">3167</a></span><span class="t"><span class="str">Returns the numerical rank of a 2-D tensor. The method to compute the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3168" class="pln"><span class="n"><a href="#t3168">3168</a></span><span class="t"><span class="str">matrix rank is done using SVD by default. If :attr:`symmetric` is ``True``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3169" class="pln"><span class="n"><a href="#t3169">3169</a></span><span class="t"><span class="str">then :attr:`input` is assumed to be symmetric, and the computation of the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3170" class="pln"><span class="n"><a href="#t3170">3170</a></span><span class="t"><span class="str">rank is done by obtaining the eigenvalues.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3171" class="pln"><span class="n"><a href="#t3171">3171</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3172" class="pln"><span class="n"><a href="#t3172">3172</a></span><span class="t"><span class="str">:attr:`tol` is the threshold below which the singular values (or the eigenvalues</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3173" class="pln"><span class="n"><a href="#t3173">3173</a></span><span class="t"><span class="str">when :attr:`symmetric` is ``True``) are considered to be 0. If :attr:`tol` is not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3174" class="pln"><span class="n"><a href="#t3174">3174</a></span><span class="t"><span class="str">specified, :attr:`tol` is set to ``S.max() * max(S.size()) * eps`` where `S` is the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3175" class="pln"><span class="n"><a href="#t3175">3175</a></span><span class="t"><span class="str">singular values (or the eigenvalues when :attr:`symmetric` is ``True``), and ``eps``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3176" class="pln"><span class="n"><a href="#t3176">3176</a></span><span class="t"><span class="str">is the epsilon value for the datatype of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3177" class="pln"><span class="n"><a href="#t3177">3177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3178" class="pln"><span class="n"><a href="#t3178">3178</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3179" class="pln"><span class="n"><a href="#t3179">3179</a></span><span class="t"><span class="str">    input (Tensor): the input 2-D tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3180" class="pln"><span class="n"><a href="#t3180">3180</a></span><span class="t"><span class="str">    tol (float, optional): the tolerance value. Default: ``None``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3181" class="pln"><span class="n"><a href="#t3181">3181</a></span><span class="t"><span class="str">    symmetric(bool, optional): indicates whether :attr:`input` is symmetric.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3182" class="pln"><span class="n"><a href="#t3182">3182</a></span><span class="t"><span class="str">                               Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3183" class="pln"><span class="n"><a href="#t3183">3183</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3184" class="pln"><span class="n"><a href="#t3184">3184</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3185" class="pln"><span class="n"><a href="#t3185">3185</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3186" class="pln"><span class="n"><a href="#t3186">3186</a></span><span class="t"><span class="str">    >>> a = torch.eye(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3187" class="pln"><span class="n"><a href="#t3187">3187</a></span><span class="t"><span class="str">    >>> torch.matrix_rank(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3188" class="pln"><span class="n"><a href="#t3188">3188</a></span><span class="t"><span class="str">    tensor(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3189" class="pln"><span class="n"><a href="#t3189">3189</a></span><span class="t"><span class="str">    >>> b = torch.eye(10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3190" class="pln"><span class="n"><a href="#t3190">3190</a></span><span class="t"><span class="str">    >>> b[0, 0] = 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3191" class="pln"><span class="n"><a href="#t3191">3191</a></span><span class="t"><span class="str">    >>> torch.matrix_rank(b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3192" class="pln"><span class="n"><a href="#t3192">3192</a></span><span class="t"><span class="str">    tensor(9)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3193" class="pln"><span class="n"><a href="#t3193">3193</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3194" class="pln"><span class="n"><a href="#t3194">3194</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3195" class="run"><span class="n"><a href="#t3195">3195</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">matrix_power</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3196" class="pln"><span class="n"><a href="#t3196">3196</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3197" class="pln"><span class="n"><a href="#t3197">3197</a></span><span class="t"><span class="str">matrix_power(input, n) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3198" class="pln"><span class="n"><a href="#t3198">3198</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3199" class="pln"><span class="n"><a href="#t3199">3199</a></span><span class="t"><span class="str">Returns the matrix raised to the power :attr:`n` for square matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3200" class="pln"><span class="n"><a href="#t3200">3200</a></span><span class="t"><span class="str">For batch of matrices, each individual matrix is raised to the power :attr:`n`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3201" class="pln"><span class="n"><a href="#t3201">3201</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3202" class="pln"><span class="n"><a href="#t3202">3202</a></span><span class="t"><span class="str">If :attr:`n` is negative, then the inverse of the matrix (if invertible) is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3203" class="pln"><span class="n"><a href="#t3203">3203</a></span><span class="t"><span class="str">raised to the power :attr:`n`.  For a batch of matrices, the batched inverse</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3204" class="pln"><span class="n"><a href="#t3204">3204</a></span><span class="t"><span class="str">(if invertible) is raised to the power :attr:`n`. If :attr:`n` is 0, then an identity matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3205" class="pln"><span class="n"><a href="#t3205">3205</a></span><span class="t"><span class="str">is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3206" class="pln"><span class="n"><a href="#t3206">3206</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3207" class="pln"><span class="n"><a href="#t3207">3207</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3208" class="pln"><span class="n"><a href="#t3208">3208</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3209" class="pln"><span class="n"><a href="#t3209">3209</a></span><span class="t"><span class="str">    n (int): the power to raise the matrix to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3210" class="pln"><span class="n"><a href="#t3210">3210</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3211" class="pln"><span class="n"><a href="#t3211">3211</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3212" class="pln"><span class="n"><a href="#t3212">3212</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3213" class="pln"><span class="n"><a href="#t3213">3213</a></span><span class="t"><span class="str">    >>> a = torch.randn(2, 2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3214" class="pln"><span class="n"><a href="#t3214">3214</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3215" class="pln"><span class="n"><a href="#t3215">3215</a></span><span class="t"><span class="str">    tensor([[[-1.9975, -1.9610],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3216" class="pln"><span class="n"><a href="#t3216">3216</a></span><span class="t"><span class="str">             [ 0.9592, -2.3364]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3217" class="pln"><span class="n"><a href="#t3217">3217</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3218" class="pln"><span class="n"><a href="#t3218">3218</a></span><span class="t"><span class="str">            [[-1.2534, -1.3429],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3219" class="pln"><span class="n"><a href="#t3219">3219</a></span><span class="t"><span class="str">             [ 0.4153, -1.4664]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3220" class="pln"><span class="n"><a href="#t3220">3220</a></span><span class="t"><span class="str">    >>> torch.matrix_power(a, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3221" class="pln"><span class="n"><a href="#t3221">3221</a></span><span class="t"><span class="str">    tensor([[[  3.9392, -23.9916],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3222" class="pln"><span class="n"><a href="#t3222">3222</a></span><span class="t"><span class="str">             [ 11.7357,  -0.2070]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3223" class="pln"><span class="n"><a href="#t3223">3223</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3224" class="pln"><span class="n"><a href="#t3224">3224</a></span><span class="t"><span class="str">            [[  0.2468,  -6.7168],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3225" class="pln"><span class="n"><a href="#t3225">3225</a></span><span class="t"><span class="str">             [  2.0774,  -0.8187]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3226" class="pln"><span class="n"><a href="#t3226">3226</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3227" class="pln"><span class="n"><a href="#t3227">3227</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3228" class="run"><span class="n"><a href="#t3228">3228</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">max</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3229" class="pln"><span class="n"><a href="#t3229">3229</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3230" class="pln"><span class="n"><a href="#t3230">3230</a></span><span class="t"><span class="str">.. function:: max(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3231" class="pln"><span class="n"><a href="#t3231">3231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3232" class="pln"><span class="n"><a href="#t3232">3232</a></span><span class="t"><span class="str">Returns the maximum value of all elements in the ``input`` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3233" class="pln"><span class="n"><a href="#t3233">3233</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3234" class="pln"><span class="n"><a href="#t3234">3234</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3235" class="pln"><span class="n"><a href="#t3235">3235</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3236" class="pln"><span class="n"><a href="#t3236">3236</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3237" class="pln"><span class="n"><a href="#t3237">3237</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3238" class="pln"><span class="n"><a href="#t3238">3238</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3239" class="pln"><span class="n"><a href="#t3239">3239</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3240" class="pln"><span class="n"><a href="#t3240">3240</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3241" class="pln"><span class="n"><a href="#t3241">3241</a></span><span class="t"><span class="str">    tensor([[ 0.6763,  0.7445, -2.2369]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3242" class="pln"><span class="n"><a href="#t3242">3242</a></span><span class="t"><span class="str">    >>> torch.max(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3243" class="pln"><span class="n"><a href="#t3243">3243</a></span><span class="t"><span class="str">    tensor(0.7445)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3244" class="pln"><span class="n"><a href="#t3244">3244</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3245" class="pln"><span class="n"><a href="#t3245">3245</a></span><span class="t"><span class="str">.. function:: max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3246" class="pln"><span class="n"><a href="#t3246">3246</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3247" class="pln"><span class="n"><a href="#t3247">3247</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3248" class="pln"><span class="n"><a href="#t3248">3248</a></span><span class="t"><span class="str">value of each row of the :attr:`input` tensor in the given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3249" class="pln"><span class="n"><a href="#t3249">3249</a></span><span class="t"><span class="str">:attr:`dim`. And ``indices`` is the index location of each maximum value found</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3250" class="pln"><span class="n"><a href="#t3250">3250</a></span><span class="t"><span class="str">(argmax).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3251" class="pln"><span class="n"><a href="#t3251">3251</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3252" class="pln"><span class="n"><a href="#t3252">3252</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3253" class="pln"><span class="n"><a href="#t3253">3253</a></span><span class="t"><span class="str">    ``indices`` does not necessarily contain the first occurrence of each</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3254" class="pln"><span class="n"><a href="#t3254">3254</a></span><span class="t"><span class="str">    maximal value found, unless it is unique.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3255" class="pln"><span class="n"><a href="#t3255">3255</a></span><span class="t"><span class="str">    The exact implementation details are device-specific.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3256" class="pln"><span class="n"><a href="#t3256">3256</a></span><span class="t"><span class="str">    Do not expect the same result when run on CPU and GPU in general.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3257" class="pln"><span class="n"><a href="#t3257">3257</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3258" class="pln"><span class="n"><a href="#t3258">3258</a></span><span class="t"><span class="str">If ``keepdim`` is ``True``, the output tensors are of the same size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3259" class="pln"><span class="n"><a href="#t3259">3259</a></span><span class="t"><span class="str">as ``input`` except in the dimension ``dim`` where they are of size 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3260" class="pln"><span class="n"><a href="#t3260">3260</a></span><span class="t"><span class="str">Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3261" class="pln"><span class="n"><a href="#t3261">3261</a></span><span class="t"><span class="str">in the output tensors having 1 fewer dimension than ``input``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3262" class="pln"><span class="n"><a href="#t3262">3262</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3263" class="pln"><span class="n"><a href="#t3263">3263</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3264" class="pln"><span class="n"><a href="#t3264">3264</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3265" class="pln"><span class="n"><a href="#t3265">3265</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3266" class="pln"><span class="n"><a href="#t3266">3266</a></span><span class="t"><span class="str">    {keepdim} Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3267" class="pln"><span class="n"><a href="#t3267">3267</a></span><span class="t"><span class="str">    out (tuple, optional): the result tuple of two output tensors (max, max_indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3268" class="pln"><span class="n"><a href="#t3268">3268</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3269" class="pln"><span class="n"><a href="#t3269">3269</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3270" class="pln"><span class="n"><a href="#t3270">3270</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3271" class="pln"><span class="n"><a href="#t3271">3271</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3272" class="pln"><span class="n"><a href="#t3272">3272</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3273" class="pln"><span class="n"><a href="#t3273">3273</a></span><span class="t"><span class="str">    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3274" class="pln"><span class="n"><a href="#t3274">3274</a></span><span class="t"><span class="str">            [ 1.1949, -1.1127, -2.2379, -0.6702],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3275" class="pln"><span class="n"><a href="#t3275">3275</a></span><span class="t"><span class="str">            [ 1.5717, -0.9207,  0.1297, -1.8768],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3276" class="pln"><span class="n"><a href="#t3276">3276</a></span><span class="t"><span class="str">            [-0.6172,  1.0036, -0.6060, -0.2432]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3277" class="pln"><span class="n"><a href="#t3277">3277</a></span><span class="t"><span class="str">    >>> torch.max(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3278" class="pln"><span class="n"><a href="#t3278">3278</a></span><span class="t"><span class="str">    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3279" class="pln"><span class="n"><a href="#t3279">3279</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3280" class="pln"><span class="n"><a href="#t3280">3280</a></span><span class="t"><span class="str">.. function:: max(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3281" class="pln"><span class="n"><a href="#t3281">3281</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3282" class="pln"><span class="n"><a href="#t3282">3282</a></span><span class="t"><span class="str">Each element of the tensor ``input`` is compared with the corresponding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3283" class="pln"><span class="n"><a href="#t3283">3283</a></span><span class="t"><span class="str">element of the tensor ``other`` and an element-wise maximum is taken.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3284" class="pln"><span class="n"><a href="#t3284">3284</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3285" class="pln"><span class="n"><a href="#t3285">3285</a></span><span class="t"><span class="str">The shapes of ``input`` and ``other`` don't need to match,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3286" class="pln"><span class="n"><a href="#t3286">3286</a></span><span class="t"><span class="str">but they must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3287" class="pln"><span class="n"><a href="#t3287">3287</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3288" class="pln"><span class="n"><a href="#t3288">3288</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3289" class="pln"><span class="n"><a href="#t3289">3289</a></span><span class="t"><span class="str">    \text{{out}}_i = \max(\text{{tensor}}_i, \text{{other}}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3290" class="pln"><span class="n"><a href="#t3290">3290</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3291" class="pln"><span class="n"><a href="#t3291">3291</a></span><span class="t"><span class="str">.. note:: When the shapes do not match, the shape of the returned output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3292" class="pln"><span class="n"><a href="#t3292">3292</a></span><span class="t"><span class="str">          follows the :ref:`broadcasting rules &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3293" class="pln"><span class="n"><a href="#t3293">3293</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3294" class="pln"><span class="n"><a href="#t3294">3294</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3295" class="pln"><span class="n"><a href="#t3295">3295</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3296" class="pln"><span class="n"><a href="#t3296">3296</a></span><span class="t"><span class="str">    other (Tensor): the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3297" class="pln"><span class="n"><a href="#t3297">3297</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3298" class="pln"><span class="n"><a href="#t3298">3298</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3299" class="pln"><span class="n"><a href="#t3299">3299</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3300" class="pln"><span class="n"><a href="#t3300">3300</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3301" class="pln"><span class="n"><a href="#t3301">3301</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3302" class="pln"><span class="n"><a href="#t3302">3302</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3303" class="pln"><span class="n"><a href="#t3303">3303</a></span><span class="t"><span class="str">    tensor([ 0.2942, -0.7416,  0.2653, -0.1584])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3304" class="pln"><span class="n"><a href="#t3304">3304</a></span><span class="t"><span class="str">    >>> b = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3305" class="pln"><span class="n"><a href="#t3305">3305</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3306" class="pln"><span class="n"><a href="#t3306">3306</a></span><span class="t"><span class="str">    tensor([ 0.8722, -1.7421, -0.4141, -0.5055])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3307" class="pln"><span class="n"><a href="#t3307">3307</a></span><span class="t"><span class="str">    >>> torch.max(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3308" class="pln"><span class="n"><a href="#t3308">3308</a></span><span class="t"><span class="str">    tensor([ 0.8722, -0.7416,  0.2653, -0.1584])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3309" class="pln"><span class="n"><a href="#t3309">3309</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3310" class="pln"><span class="n"><a href="#t3310">3310</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3311" class="run"><span class="n"><a href="#t3311">3311</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">argmax</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3312" class="pln"><span class="n"><a href="#t3312">3312</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3313" class="pln"><span class="n"><a href="#t3313">3313</a></span><span class="t"><span class="str">.. function:: argmax(input) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3314" class="pln"><span class="n"><a href="#t3314">3314</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3315" class="pln"><span class="n"><a href="#t3315">3315</a></span><span class="t"><span class="str">Returns the indices of the maximum value of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3316" class="pln"><span class="n"><a href="#t3316">3316</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3317" class="pln"><span class="n"><a href="#t3317">3317</a></span><span class="t"><span class="str">This is the second value returned by :meth:`torch.max`. See its</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3318" class="pln"><span class="n"><a href="#t3318">3318</a></span><span class="t"><span class="str">documentation for the exact semantics of this method.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3319" class="pln"><span class="n"><a href="#t3319">3319</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3320" class="pln"><span class="n"><a href="#t3320">3320</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3321" class="pln"><span class="n"><a href="#t3321">3321</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3322" class="pln"><span class="n"><a href="#t3322">3322</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3323" class="pln"><span class="n"><a href="#t3323">3323</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3324" class="pln"><span class="n"><a href="#t3324">3324</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3325" class="pln"><span class="n"><a href="#t3325">3325</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3326" class="pln"><span class="n"><a href="#t3326">3326</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3327" class="pln"><span class="n"><a href="#t3327">3327</a></span><span class="t"><span class="str">    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3328" class="pln"><span class="n"><a href="#t3328">3328</a></span><span class="t"><span class="str">            [-0.7401, -0.8805, -0.3402, -1.1936],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3329" class="pln"><span class="n"><a href="#t3329">3329</a></span><span class="t"><span class="str">            [ 0.4907, -1.3948, -1.0691, -0.3132],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3330" class="pln"><span class="n"><a href="#t3330">3330</a></span><span class="t"><span class="str">            [-1.6092,  0.5419, -0.2993,  0.3195]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3331" class="pln"><span class="n"><a href="#t3331">3331</a></span><span class="t"><span class="str">    >>> torch.argmax(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3332" class="pln"><span class="n"><a href="#t3332">3332</a></span><span class="t"><span class="str">    tensor(0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3333" class="pln"><span class="n"><a href="#t3333">3333</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3334" class="pln"><span class="n"><a href="#t3334">3334</a></span><span class="t"><span class="str">.. function:: argmax(input, dim, keepdim=False) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3335" class="pln"><span class="n"><a href="#t3335">3335</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3336" class="pln"><span class="n"><a href="#t3336">3336</a></span><span class="t"><span class="str">Returns the indices of the maximum values of a tensor across a dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3337" class="pln"><span class="n"><a href="#t3337">3337</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3338" class="pln"><span class="n"><a href="#t3338">3338</a></span><span class="t"><span class="str">This is the second value returned by :meth:`torch.max`. See its</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3339" class="pln"><span class="n"><a href="#t3339">3339</a></span><span class="t"><span class="str">documentation for the exact semantics of this method.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3340" class="pln"><span class="n"><a href="#t3340">3340</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3341" class="pln"><span class="n"><a href="#t3341">3341</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3342" class="pln"><span class="n"><a href="#t3342">3342</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3343" class="pln"><span class="n"><a href="#t3343">3343</a></span><span class="t"><span class="str">    {dim} If ``None``, the argmax of the flattened input is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3344" class="pln"><span class="n"><a href="#t3344">3344</a></span><span class="t"><span class="str">    {keepdim} Ignored if ``dim=None``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3345" class="pln"><span class="n"><a href="#t3345">3345</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3346" class="pln"><span class="n"><a href="#t3346">3346</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3347" class="pln"><span class="n"><a href="#t3347">3347</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3348" class="pln"><span class="n"><a href="#t3348">3348</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3349" class="pln"><span class="n"><a href="#t3349">3349</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3350" class="pln"><span class="n"><a href="#t3350">3350</a></span><span class="t"><span class="str">    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3351" class="pln"><span class="n"><a href="#t3351">3351</a></span><span class="t"><span class="str">            [-0.7401, -0.8805, -0.3402, -1.1936],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3352" class="pln"><span class="n"><a href="#t3352">3352</a></span><span class="t"><span class="str">            [ 0.4907, -1.3948, -1.0691, -0.3132],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3353" class="pln"><span class="n"><a href="#t3353">3353</a></span><span class="t"><span class="str">            [-1.6092,  0.5419, -0.2993,  0.3195]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3354" class="pln"><span class="n"><a href="#t3354">3354</a></span><span class="t"><span class="str">    >>> torch.argmax(a, dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3355" class="pln"><span class="n"><a href="#t3355">3355</a></span><span class="t"><span class="str">    tensor([ 0,  2,  0,  1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3356" class="pln"><span class="n"><a href="#t3356">3356</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3357" class="pln"><span class="n"><a href="#t3357">3357</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3358" class="run"><span class="n"><a href="#t3358">3358</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">mean</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3359" class="pln"><span class="n"><a href="#t3359">3359</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3360" class="pln"><span class="n"><a href="#t3360">3360</a></span><span class="t"><span class="str">.. function:: mean(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3361" class="pln"><span class="n"><a href="#t3361">3361</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3362" class="pln"><span class="n"><a href="#t3362">3362</a></span><span class="t"><span class="str">Returns the mean value of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3363" class="pln"><span class="n"><a href="#t3363">3363</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3364" class="pln"><span class="n"><a href="#t3364">3364</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3365" class="pln"><span class="n"><a href="#t3365">3365</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3366" class="pln"><span class="n"><a href="#t3366">3366</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3367" class="pln"><span class="n"><a href="#t3367">3367</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3368" class="pln"><span class="n"><a href="#t3368">3368</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3369" class="pln"><span class="n"><a href="#t3369">3369</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3370" class="pln"><span class="n"><a href="#t3370">3370</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3371" class="pln"><span class="n"><a href="#t3371">3371</a></span><span class="t"><span class="str">    tensor([[ 0.2294, -0.5481,  1.3288]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3372" class="pln"><span class="n"><a href="#t3372">3372</a></span><span class="t"><span class="str">    >>> torch.mean(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3373" class="pln"><span class="n"><a href="#t3373">3373</a></span><span class="t"><span class="str">    tensor(0.3367)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3374" class="pln"><span class="n"><a href="#t3374">3374</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3375" class="pln"><span class="n"><a href="#t3375">3375</a></span><span class="t"><span class="str">.. function:: mean(input, dim, keepdim=False, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3376" class="pln"><span class="n"><a href="#t3376">3376</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3377" class="pln"><span class="n"><a href="#t3377">3377</a></span><span class="t"><span class="str">Returns the mean value of each row of the :attr:`input` tensor in the given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3378" class="pln"><span class="n"><a href="#t3378">3378</a></span><span class="t"><span class="str">dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3379" class="pln"><span class="n"><a href="#t3379">3379</a></span><span class="t"><span class="str">reduce over all of them.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3380" class="pln"><span class="n"><a href="#t3380">3380</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3381" class="pln"><span class="n"><a href="#t3381">3381</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3382" class="pln"><span class="n"><a href="#t3382">3382</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3383" class="pln"><span class="n"><a href="#t3383">3383</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3384" class="pln"><span class="n"><a href="#t3384">3384</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3385" class="pln"><span class="n"><a href="#t3385">3385</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3386" class="pln"><span class="n"><a href="#t3386">3386</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3387" class="pln"><span class="n"><a href="#t3387">3387</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3388" class="pln"><span class="n"><a href="#t3388">3388</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3389" class="pln"><span class="n"><a href="#t3389">3389</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3390" class="pln"><span class="n"><a href="#t3390">3390</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3391" class="pln"><span class="n"><a href="#t3391">3391</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3392" class="pln"><span class="n"><a href="#t3392">3392</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3393" class="pln"><span class="n"><a href="#t3393">3393</a></span><span class="t"><span class="str">    tensor([[-0.3841,  0.6320,  0.4254, -0.7384],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3394" class="pln"><span class="n"><a href="#t3394">3394</a></span><span class="t"><span class="str">            [-0.9644,  1.0131, -0.6549, -1.4279],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3395" class="pln"><span class="n"><a href="#t3395">3395</a></span><span class="t"><span class="str">            [-0.2951, -1.3350, -0.7694,  0.5600],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3396" class="pln"><span class="n"><a href="#t3396">3396</a></span><span class="t"><span class="str">            [ 1.0842, -0.9580,  0.3623,  0.2343]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3397" class="pln"><span class="n"><a href="#t3397">3397</a></span><span class="t"><span class="str">    >>> torch.mean(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3398" class="pln"><span class="n"><a href="#t3398">3398</a></span><span class="t"><span class="str">    tensor([-0.0163, -0.5085, -0.4599,  0.1807])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3399" class="pln"><span class="n"><a href="#t3399">3399</a></span><span class="t"><span class="str">    >>> torch.mean(a, 1, True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3400" class="pln"><span class="n"><a href="#t3400">3400</a></span><span class="t"><span class="str">    tensor([[-0.0163],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3401" class="pln"><span class="n"><a href="#t3401">3401</a></span><span class="t"><span class="str">            [-0.5085],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3402" class="pln"><span class="n"><a href="#t3402">3402</a></span><span class="t"><span class="str">            [-0.4599],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3403" class="pln"><span class="n"><a href="#t3403">3403</a></span><span class="t"><span class="str">            [ 0.1807]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3404" class="pln"><span class="n"><a href="#t3404">3404</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3405" class="pln"><span class="n"><a href="#t3405">3405</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3406" class="run"><span class="n"><a href="#t3406">3406</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">median</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3407" class="pln"><span class="n"><a href="#t3407">3407</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3408" class="pln"><span class="n"><a href="#t3408">3408</a></span><span class="t"><span class="str">.. function:: median(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3409" class="pln"><span class="n"><a href="#t3409">3409</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3410" class="pln"><span class="n"><a href="#t3410">3410</a></span><span class="t"><span class="str">Returns the median value of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3411" class="pln"><span class="n"><a href="#t3411">3411</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3412" class="pln"><span class="n"><a href="#t3412">3412</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3413" class="pln"><span class="n"><a href="#t3413">3413</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3414" class="pln"><span class="n"><a href="#t3414">3414</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3415" class="pln"><span class="n"><a href="#t3415">3415</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3416" class="pln"><span class="n"><a href="#t3416">3416</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3417" class="pln"><span class="n"><a href="#t3417">3417</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3418" class="pln"><span class="n"><a href="#t3418">3418</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3419" class="pln"><span class="n"><a href="#t3419">3419</a></span><span class="t"><span class="str">    tensor([[ 1.5219, -1.5212,  0.2202]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3420" class="pln"><span class="n"><a href="#t3420">3420</a></span><span class="t"><span class="str">    >>> torch.median(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3421" class="pln"><span class="n"><a href="#t3421">3421</a></span><span class="t"><span class="str">    tensor(0.2202)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3422" class="pln"><span class="n"><a href="#t3422">3422</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3423" class="pln"><span class="n"><a href="#t3423">3423</a></span><span class="t"><span class="str">.. function:: median(input, dim=-1, keepdim=False, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3424" class="pln"><span class="n"><a href="#t3424">3424</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3425" class="pln"><span class="n"><a href="#t3425">3425</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values`` is the median</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3426" class="pln"><span class="n"><a href="#t3426">3426</a></span><span class="t"><span class="str">value of each row of the :attr:`input` tensor in the given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3427" class="pln"><span class="n"><a href="#t3427">3427</a></span><span class="t"><span class="str">:attr:`dim`. And ``indices`` is the index location of each median value found.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3428" class="pln"><span class="n"><a href="#t3428">3428</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3429" class="pln"><span class="n"><a href="#t3429">3429</a></span><span class="t"><span class="str">By default, :attr:`dim` is the last dimension of the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3430" class="pln"><span class="n"><a href="#t3430">3430</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3431" class="pln"><span class="n"><a href="#t3431">3431</a></span><span class="t"><span class="str">If :attr:`keepdim` is ``True``, the output tensors are of the same size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3432" class="pln"><span class="n"><a href="#t3432">3432</a></span><span class="t"><span class="str">as :attr:`input` except in the dimension :attr:`dim` where they are of size 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3433" class="pln"><span class="n"><a href="#t3433">3433</a></span><span class="t"><span class="str">Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3434" class="pln"><span class="n"><a href="#t3434">3434</a></span><span class="t"><span class="str">the outputs tensor having 1 fewer dimension than :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3435" class="pln"><span class="n"><a href="#t3435">3435</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3436" class="pln"><span class="n"><a href="#t3436">3436</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3437" class="pln"><span class="n"><a href="#t3437">3437</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3438" class="pln"><span class="n"><a href="#t3438">3438</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3439" class="pln"><span class="n"><a href="#t3439">3439</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3440" class="pln"><span class="n"><a href="#t3440">3440</a></span><span class="t"><span class="str">    out (tuple, optional): the result tuple of two output tensors (max, max_indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3441" class="pln"><span class="n"><a href="#t3441">3441</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3442" class="pln"><span class="n"><a href="#t3442">3442</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3443" class="pln"><span class="n"><a href="#t3443">3443</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3444" class="pln"><span class="n"><a href="#t3444">3444</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3445" class="pln"><span class="n"><a href="#t3445">3445</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3446" class="pln"><span class="n"><a href="#t3446">3446</a></span><span class="t"><span class="str">    tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3447" class="pln"><span class="n"><a href="#t3447">3447</a></span><span class="t"><span class="str">            [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3448" class="pln"><span class="n"><a href="#t3448">3448</a></span><span class="t"><span class="str">            [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3449" class="pln"><span class="n"><a href="#t3449">3449</a></span><span class="t"><span class="str">            [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3450" class="pln"><span class="n"><a href="#t3450">3450</a></span><span class="t"><span class="str">    >>> torch.median(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3451" class="pln"><span class="n"><a href="#t3451">3451</a></span><span class="t"><span class="str">    torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3452" class="pln"><span class="n"><a href="#t3452">3452</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3453" class="pln"><span class="n"><a href="#t3453">3453</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3454" class="run"><span class="n"><a href="#t3454">3454</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">min</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3455" class="pln"><span class="n"><a href="#t3455">3455</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3456" class="pln"><span class="n"><a href="#t3456">3456</a></span><span class="t"><span class="str">.. function:: min(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3457" class="pln"><span class="n"><a href="#t3457">3457</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3458" class="pln"><span class="n"><a href="#t3458">3458</a></span><span class="t"><span class="str">Returns the minimum value of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3459" class="pln"><span class="n"><a href="#t3459">3459</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3460" class="pln"><span class="n"><a href="#t3460">3460</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3461" class="pln"><span class="n"><a href="#t3461">3461</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3462" class="pln"><span class="n"><a href="#t3462">3462</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3463" class="pln"><span class="n"><a href="#t3463">3463</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3464" class="pln"><span class="n"><a href="#t3464">3464</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3465" class="pln"><span class="n"><a href="#t3465">3465</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3466" class="pln"><span class="n"><a href="#t3466">3466</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3467" class="pln"><span class="n"><a href="#t3467">3467</a></span><span class="t"><span class="str">    tensor([[ 0.6750,  1.0857,  1.7197]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3468" class="pln"><span class="n"><a href="#t3468">3468</a></span><span class="t"><span class="str">    >>> torch.min(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3469" class="pln"><span class="n"><a href="#t3469">3469</a></span><span class="t"><span class="str">    tensor(0.6750)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3470" class="pln"><span class="n"><a href="#t3470">3470</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3471" class="pln"><span class="n"><a href="#t3471">3471</a></span><span class="t"><span class="str">.. function:: min(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3472" class="pln"><span class="n"><a href="#t3472">3472</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3473" class="pln"><span class="n"><a href="#t3473">3473</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3474" class="pln"><span class="n"><a href="#t3474">3474</a></span><span class="t"><span class="str">value of each row of the :attr:`input` tensor in the given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3475" class="pln"><span class="n"><a href="#t3475">3475</a></span><span class="t"><span class="str">:attr:`dim`. And ``indices`` is the index location of each minimum value found</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3476" class="pln"><span class="n"><a href="#t3476">3476</a></span><span class="t"><span class="str">(argmin).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3477" class="pln"><span class="n"><a href="#t3477">3477</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3478" class="pln"><span class="n"><a href="#t3478">3478</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3479" class="pln"><span class="n"><a href="#t3479">3479</a></span><span class="t"><span class="str">    ``indices`` does not necessarily contain the first occurrence of each</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3480" class="pln"><span class="n"><a href="#t3480">3480</a></span><span class="t"><span class="str">    minimal value found, unless it is unique.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3481" class="pln"><span class="n"><a href="#t3481">3481</a></span><span class="t"><span class="str">    The exact implementation details are device-specific.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3482" class="pln"><span class="n"><a href="#t3482">3482</a></span><span class="t"><span class="str">    Do not expect the same result when run on CPU and GPU in general.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3483" class="pln"><span class="n"><a href="#t3483">3483</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3484" class="pln"><span class="n"><a href="#t3484">3484</a></span><span class="t"><span class="str">If :attr:`keepdim` is ``True``, the output tensors are of the same size as</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3485" class="pln"><span class="n"><a href="#t3485">3485</a></span><span class="t"><span class="str">:attr:`input` except in the dimension :attr:`dim` where they are of size 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3486" class="pln"><span class="n"><a href="#t3486">3486</a></span><span class="t"><span class="str">Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3487" class="pln"><span class="n"><a href="#t3487">3487</a></span><span class="t"><span class="str">the output tensors having 1 fewer dimension than :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3488" class="pln"><span class="n"><a href="#t3488">3488</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3489" class="pln"><span class="n"><a href="#t3489">3489</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3490" class="pln"><span class="n"><a href="#t3490">3490</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3491" class="pln"><span class="n"><a href="#t3491">3491</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3492" class="pln"><span class="n"><a href="#t3492">3492</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3493" class="pln"><span class="n"><a href="#t3493">3493</a></span><span class="t"><span class="str">    out (tuple, optional): the tuple of two output tensors (min, min_indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3494" class="pln"><span class="n"><a href="#t3494">3494</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3495" class="pln"><span class="n"><a href="#t3495">3495</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3496" class="pln"><span class="n"><a href="#t3496">3496</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3497" class="pln"><span class="n"><a href="#t3497">3497</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3498" class="pln"><span class="n"><a href="#t3498">3498</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3499" class="pln"><span class="n"><a href="#t3499">3499</a></span><span class="t"><span class="str">    tensor([[-0.6248,  1.1334, -1.1899, -0.2803],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3500" class="pln"><span class="n"><a href="#t3500">3500</a></span><span class="t"><span class="str">            [-1.4644, -0.2635, -0.3651,  0.6134],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3501" class="pln"><span class="n"><a href="#t3501">3501</a></span><span class="t"><span class="str">            [ 0.2457,  0.0384,  1.0128,  0.7015],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3502" class="pln"><span class="n"><a href="#t3502">3502</a></span><span class="t"><span class="str">            [-0.1153,  2.9849,  2.1458,  0.5788]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3503" class="pln"><span class="n"><a href="#t3503">3503</a></span><span class="t"><span class="str">    >>> torch.min(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3504" class="pln"><span class="n"><a href="#t3504">3504</a></span><span class="t"><span class="str">    torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3505" class="pln"><span class="n"><a href="#t3505">3505</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3506" class="pln"><span class="n"><a href="#t3506">3506</a></span><span class="t"><span class="str">.. function:: min(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3507" class="pln"><span class="n"><a href="#t3507">3507</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3508" class="pln"><span class="n"><a href="#t3508">3508</a></span><span class="t"><span class="str">Each element of the tensor :attr:`input` is compared with the corresponding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3509" class="pln"><span class="n"><a href="#t3509">3509</a></span><span class="t"><span class="str">element of the tensor :attr:`other` and an element-wise minimum is taken.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3510" class="pln"><span class="n"><a href="#t3510">3510</a></span><span class="t"><span class="str">The resulting tensor is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3511" class="pln"><span class="n"><a href="#t3511">3511</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3512" class="pln"><span class="n"><a href="#t3512">3512</a></span><span class="t"><span class="str">The shapes of :attr:`input` and :attr:`other` don't need to match,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3513" class="pln"><span class="n"><a href="#t3513">3513</a></span><span class="t"><span class="str">but they must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3514" class="pln"><span class="n"><a href="#t3514">3514</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3515" class="pln"><span class="n"><a href="#t3515">3515</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3516" class="pln"><span class="n"><a href="#t3516">3516</a></span><span class="t"><span class="str">    \text{out}_i = \min(\text{tensor}_i, \text{other}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3517" class="pln"><span class="n"><a href="#t3517">3517</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3518" class="pln"><span class="n"><a href="#t3518">3518</a></span><span class="t"><span class="str">.. note:: When the shapes do not match, the shape of the returned output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3519" class="pln"><span class="n"><a href="#t3519">3519</a></span><span class="t"><span class="str">          follows the :ref:`broadcasting rules &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3520" class="pln"><span class="n"><a href="#t3520">3520</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3521" class="pln"><span class="n"><a href="#t3521">3521</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3522" class="pln"><span class="n"><a href="#t3522">3522</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3523" class="pln"><span class="n"><a href="#t3523">3523</a></span><span class="t"><span class="str">    other (Tensor): the second input tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3524" class="pln"><span class="n"><a href="#t3524">3524</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3525" class="pln"><span class="n"><a href="#t3525">3525</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3526" class="pln"><span class="n"><a href="#t3526">3526</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3527" class="pln"><span class="n"><a href="#t3527">3527</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3528" class="pln"><span class="n"><a href="#t3528">3528</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3529" class="pln"><span class="n"><a href="#t3529">3529</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3530" class="pln"><span class="n"><a href="#t3530">3530</a></span><span class="t"><span class="str">    tensor([ 0.8137, -1.1740, -0.6460,  0.6308])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3531" class="pln"><span class="n"><a href="#t3531">3531</a></span><span class="t"><span class="str">    >>> b = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3532" class="pln"><span class="n"><a href="#t3532">3532</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3533" class="pln"><span class="n"><a href="#t3533">3533</a></span><span class="t"><span class="str">    tensor([-0.1369,  0.1555,  0.4019, -0.1929])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3534" class="pln"><span class="n"><a href="#t3534">3534</a></span><span class="t"><span class="str">    >>> torch.min(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3535" class="pln"><span class="n"><a href="#t3535">3535</a></span><span class="t"><span class="str">    tensor([-0.1369, -1.1740, -0.6460, -0.1929])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3536" class="pln"><span class="n"><a href="#t3536">3536</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3537" class="pln"><span class="n"><a href="#t3537">3537</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3538" class="run"><span class="n"><a href="#t3538">3538</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">argmin</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3539" class="pln"><span class="n"><a href="#t3539">3539</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3540" class="pln"><span class="n"><a href="#t3540">3540</a></span><span class="t"><span class="str">.. function:: argmin(input) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3541" class="pln"><span class="n"><a href="#t3541">3541</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3542" class="pln"><span class="n"><a href="#t3542">3542</a></span><span class="t"><span class="str">Returns the indices of the minimum value of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3543" class="pln"><span class="n"><a href="#t3543">3543</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3544" class="pln"><span class="n"><a href="#t3544">3544</a></span><span class="t"><span class="str">This is the second value returned by :meth:`torch.min`. See its</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3545" class="pln"><span class="n"><a href="#t3545">3545</a></span><span class="t"><span class="str">documentation for the exact semantics of this method.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3546" class="pln"><span class="n"><a href="#t3546">3546</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3547" class="pln"><span class="n"><a href="#t3547">3547</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3548" class="pln"><span class="n"><a href="#t3548">3548</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3549" class="pln"><span class="n"><a href="#t3549">3549</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3550" class="pln"><span class="n"><a href="#t3550">3550</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3551" class="pln"><span class="n"><a href="#t3551">3551</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3552" class="pln"><span class="n"><a href="#t3552">3552</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3553" class="pln"><span class="n"><a href="#t3553">3553</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3554" class="pln"><span class="n"><a href="#t3554">3554</a></span><span class="t"><span class="str">    tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3555" class="pln"><span class="n"><a href="#t3555">3555</a></span><span class="t"><span class="str">            [ 1.0100, -1.1975, -0.0102, -0.4732],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3556" class="pln"><span class="n"><a href="#t3556">3556</a></span><span class="t"><span class="str">            [-0.9240,  0.1207, -0.7506, -1.0213],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3557" class="pln"><span class="n"><a href="#t3557">3557</a></span><span class="t"><span class="str">            [ 1.7809, -1.2960,  0.9384,  0.1438]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3558" class="pln"><span class="n"><a href="#t3558">3558</a></span><span class="t"><span class="str">    >>> torch.argmin(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3559" class="pln"><span class="n"><a href="#t3559">3559</a></span><span class="t"><span class="str">    tensor(13)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3560" class="pln"><span class="n"><a href="#t3560">3560</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3561" class="pln"><span class="n"><a href="#t3561">3561</a></span><span class="t"><span class="str">.. function:: argmin(input, dim, keepdim=False, out=None) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3562" class="pln"><span class="n"><a href="#t3562">3562</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3563" class="pln"><span class="n"><a href="#t3563">3563</a></span><span class="t"><span class="str">Returns the indices of the minimum values of a tensor across a dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3564" class="pln"><span class="n"><a href="#t3564">3564</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3565" class="pln"><span class="n"><a href="#t3565">3565</a></span><span class="t"><span class="str">This is the second value returned by :meth:`torch.min`. See its</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3566" class="pln"><span class="n"><a href="#t3566">3566</a></span><span class="t"><span class="str">documentation for the exact semantics of this method.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3567" class="pln"><span class="n"><a href="#t3567">3567</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3568" class="pln"><span class="n"><a href="#t3568">3568</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3569" class="pln"><span class="n"><a href="#t3569">3569</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3570" class="pln"><span class="n"><a href="#t3570">3570</a></span><span class="t"><span class="str">    {dim} If ``None``, the argmin of the flattened input is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3571" class="pln"><span class="n"><a href="#t3571">3571</a></span><span class="t"><span class="str">    {keepdim} Ignored if ``dim=None``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3572" class="pln"><span class="n"><a href="#t3572">3572</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3573" class="pln"><span class="n"><a href="#t3573">3573</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3574" class="pln"><span class="n"><a href="#t3574">3574</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3575" class="pln"><span class="n"><a href="#t3575">3575</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3576" class="pln"><span class="n"><a href="#t3576">3576</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3577" class="pln"><span class="n"><a href="#t3577">3577</a></span><span class="t"><span class="str">    tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3578" class="pln"><span class="n"><a href="#t3578">3578</a></span><span class="t"><span class="str">            [ 1.0100, -1.1975, -0.0102, -0.4732],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3579" class="pln"><span class="n"><a href="#t3579">3579</a></span><span class="t"><span class="str">            [-0.9240,  0.1207, -0.7506, -1.0213],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3580" class="pln"><span class="n"><a href="#t3580">3580</a></span><span class="t"><span class="str">            [ 1.7809, -1.2960,  0.9384,  0.1438]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3581" class="pln"><span class="n"><a href="#t3581">3581</a></span><span class="t"><span class="str">    >>> torch.argmin(a, dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3582" class="pln"><span class="n"><a href="#t3582">3582</a></span><span class="t"><span class="str">    tensor([ 2,  1,  3,  1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3583" class="pln"><span class="n"><a href="#t3583">3583</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3584" class="pln"><span class="n"><a href="#t3584">3584</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3585" class="run"><span class="n"><a href="#t3585">3585</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">mm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3586" class="pln"><span class="n"><a href="#t3586">3586</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3587" class="pln"><span class="n"><a href="#t3587">3587</a></span><span class="t"><span class="str">mm(input, mat2, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3588" class="pln"><span class="n"><a href="#t3588">3588</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3589" class="pln"><span class="n"><a href="#t3589">3589</a></span><span class="t"><span class="str">Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3590" class="pln"><span class="n"><a href="#t3590">3590</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3591" class="pln"><span class="n"><a href="#t3591">3591</a></span><span class="t"><span class="str">If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`mat2` is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3592" class="pln"><span class="n"><a href="#t3592">3592</a></span><span class="t"><span class="str">:math:`(m \times p)` tensor, :attr:`out` will be a :math:`(n \times p)` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3593" class="pln"><span class="n"><a href="#t3593">3593</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3594" class="pln"><span class="n"><a href="#t3594">3594</a></span><span class="t"><span class="str">.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3595" class="pln"><span class="n"><a href="#t3595">3595</a></span><span class="t"><span class="str">          For broadcasting matrix products, see :func:`torch.matmul`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3596" class="pln"><span class="n"><a href="#t3596">3596</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3597" class="pln"><span class="n"><a href="#t3597">3597</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3598" class="pln"><span class="n"><a href="#t3598">3598</a></span><span class="t"><span class="str">    input (Tensor): the first matrix to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3599" class="pln"><span class="n"><a href="#t3599">3599</a></span><span class="t"><span class="str">    mat2 (Tensor): the second matrix to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3600" class="pln"><span class="n"><a href="#t3600">3600</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3601" class="pln"><span class="n"><a href="#t3601">3601</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3602" class="pln"><span class="n"><a href="#t3602">3602</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3603" class="pln"><span class="n"><a href="#t3603">3603</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3604" class="pln"><span class="n"><a href="#t3604">3604</a></span><span class="t"><span class="str">    >>> mat1 = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3605" class="pln"><span class="n"><a href="#t3605">3605</a></span><span class="t"><span class="str">    >>> mat2 = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3606" class="pln"><span class="n"><a href="#t3606">3606</a></span><span class="t"><span class="str">    >>> torch.mm(mat1, mat2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3607" class="pln"><span class="n"><a href="#t3607">3607</a></span><span class="t"><span class="str">    tensor([[ 0.4851,  0.5037, -0.3633],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3608" class="pln"><span class="n"><a href="#t3608">3608</a></span><span class="t"><span class="str">            [-0.0760, -3.6705,  2.4784]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3609" class="pln"><span class="n"><a href="#t3609">3609</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3610" class="pln"><span class="n"><a href="#t3610">3610</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3611" class="run"><span class="n"><a href="#t3611">3611</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">matmul</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3612" class="pln"><span class="n"><a href="#t3612">3612</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3613" class="pln"><span class="n"><a href="#t3613">3613</a></span><span class="t"><span class="str">matmul(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3614" class="pln"><span class="n"><a href="#t3614">3614</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3615" class="pln"><span class="n"><a href="#t3615">3615</a></span><span class="t"><span class="str">Matrix product of two tensors.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3616" class="pln"><span class="n"><a href="#t3616">3616</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3617" class="pln"><span class="n"><a href="#t3617">3617</a></span><span class="t"><span class="str">The behavior depends on the dimensionality of the tensors as follows:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3618" class="pln"><span class="n"><a href="#t3618">3618</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3619" class="pln"><span class="n"><a href="#t3619">3619</a></span><span class="t"><span class="str">- If both tensors are 1-dimensional, the dot product (scalar) is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3620" class="pln"><span class="n"><a href="#t3620">3620</a></span><span class="t"><span class="str">- If both arguments are 2-dimensional, the matrix-matrix product is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3621" class="pln"><span class="n"><a href="#t3621">3621</a></span><span class="t"><span class="str">- If the first argument is 1-dimensional and the second argument is 2-dimensional,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3622" class="pln"><span class="n"><a href="#t3622">3622</a></span><span class="t"><span class="str">  a 1 is prepended to its dimension for the purpose of the matrix multiply.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3623" class="pln"><span class="n"><a href="#t3623">3623</a></span><span class="t"><span class="str">  After the matrix multiply, the prepended dimension is removed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3624" class="pln"><span class="n"><a href="#t3624">3624</a></span><span class="t"><span class="str">- If the first argument is 2-dimensional and the second argument is 1-dimensional,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3625" class="pln"><span class="n"><a href="#t3625">3625</a></span><span class="t"><span class="str">  the matrix-vector product is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3626" class="pln"><span class="n"><a href="#t3626">3626</a></span><span class="t"><span class="str">- If both arguments are at least 1-dimensional and at least one argument is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3627" class="pln"><span class="n"><a href="#t3627">3627</a></span><span class="t"><span class="str">  N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3628" class="pln"><span class="n"><a href="#t3628">3628</a></span><span class="t"><span class="str">  argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3629" class="pln"><span class="n"><a href="#t3629">3629</a></span><span class="t"><span class="str">  batched matrix multiply and removed after.  If the second argument is 1-dimensional, a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3630" class="pln"><span class="n"><a href="#t3630">3630</a></span><span class="t"><span class="str">  1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3631" class="pln"><span class="n"><a href="#t3631">3631</a></span><span class="t"><span class="str">  The non-matrix (i.e. batch) dimensions are :ref:`broadcasted &lt;broadcasting-semantics>` (and thus</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3632" class="pln"><span class="n"><a href="#t3632">3632</a></span><span class="t"><span class="str">  must be broadcastable).  For example, if :attr:`input` is a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3633" class="pln"><span class="n"><a href="#t3633">3633</a></span><span class="t"><span class="str">  :math:`(j \times 1 \times n \times m)` tensor and :attr:`other` is a :math:`(k \times m \times p)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3634" class="pln"><span class="n"><a href="#t3634">3634</a></span><span class="t"><span class="str">  tensor, :attr:`out` will be an :math:`(j \times k \times n \times p)` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3635" class="pln"><span class="n"><a href="#t3635">3635</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3636" class="pln"><span class="n"><a href="#t3636">3636</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3637" class="pln"><span class="n"><a href="#t3637">3637</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3638" class="pln"><span class="n"><a href="#t3638">3638</a></span><span class="t"><span class="str">    The 1-dimensional dot product version of this function does not support an :attr:`out` parameter.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3639" class="pln"><span class="n"><a href="#t3639">3639</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3640" class="pln"><span class="n"><a href="#t3640">3640</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3641" class="pln"><span class="n"><a href="#t3641">3641</a></span><span class="t"><span class="str">    input (Tensor): the first tensor to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3642" class="pln"><span class="n"><a href="#t3642">3642</a></span><span class="t"><span class="str">    other (Tensor): the second tensor to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3643" class="pln"><span class="n"><a href="#t3643">3643</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3644" class="pln"><span class="n"><a href="#t3644">3644</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3645" class="pln"><span class="n"><a href="#t3645">3645</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3646" class="pln"><span class="n"><a href="#t3646">3646</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3647" class="pln"><span class="n"><a href="#t3647">3647</a></span><span class="t"><span class="str">    >>> # vector x vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3648" class="pln"><span class="n"><a href="#t3648">3648</a></span><span class="t"><span class="str">    >>> tensor1 = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3649" class="pln"><span class="n"><a href="#t3649">3649</a></span><span class="t"><span class="str">    >>> tensor2 = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3650" class="pln"><span class="n"><a href="#t3650">3650</a></span><span class="t"><span class="str">    >>> torch.matmul(tensor1, tensor2).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3651" class="pln"><span class="n"><a href="#t3651">3651</a></span><span class="t"><span class="str">    torch.Size([])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3652" class="pln"><span class="n"><a href="#t3652">3652</a></span><span class="t"><span class="str">    >>> # matrix x vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3653" class="pln"><span class="n"><a href="#t3653">3653</a></span><span class="t"><span class="str">    >>> tensor1 = torch.randn(3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3654" class="pln"><span class="n"><a href="#t3654">3654</a></span><span class="t"><span class="str">    >>> tensor2 = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3655" class="pln"><span class="n"><a href="#t3655">3655</a></span><span class="t"><span class="str">    >>> torch.matmul(tensor1, tensor2).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3656" class="pln"><span class="n"><a href="#t3656">3656</a></span><span class="t"><span class="str">    torch.Size([3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3657" class="pln"><span class="n"><a href="#t3657">3657</a></span><span class="t"><span class="str">    >>> # batched matrix x broadcasted vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3658" class="pln"><span class="n"><a href="#t3658">3658</a></span><span class="t"><span class="str">    >>> tensor1 = torch.randn(10, 3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3659" class="pln"><span class="n"><a href="#t3659">3659</a></span><span class="t"><span class="str">    >>> tensor2 = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3660" class="pln"><span class="n"><a href="#t3660">3660</a></span><span class="t"><span class="str">    >>> torch.matmul(tensor1, tensor2).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3661" class="pln"><span class="n"><a href="#t3661">3661</a></span><span class="t"><span class="str">    torch.Size([10, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3662" class="pln"><span class="n"><a href="#t3662">3662</a></span><span class="t"><span class="str">    >>> # batched matrix x batched matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3663" class="pln"><span class="n"><a href="#t3663">3663</a></span><span class="t"><span class="str">    >>> tensor1 = torch.randn(10, 3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3664" class="pln"><span class="n"><a href="#t3664">3664</a></span><span class="t"><span class="str">    >>> tensor2 = torch.randn(10, 4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3665" class="pln"><span class="n"><a href="#t3665">3665</a></span><span class="t"><span class="str">    >>> torch.matmul(tensor1, tensor2).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3666" class="pln"><span class="n"><a href="#t3666">3666</a></span><span class="t"><span class="str">    torch.Size([10, 3, 5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3667" class="pln"><span class="n"><a href="#t3667">3667</a></span><span class="t"><span class="str">    >>> # batched matrix x broadcasted matrix</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3668" class="pln"><span class="n"><a href="#t3668">3668</a></span><span class="t"><span class="str">    >>> tensor1 = torch.randn(10, 3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3669" class="pln"><span class="n"><a href="#t3669">3669</a></span><span class="t"><span class="str">    >>> tensor2 = torch.randn(4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3670" class="pln"><span class="n"><a href="#t3670">3670</a></span><span class="t"><span class="str">    >>> torch.matmul(tensor1, tensor2).size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3671" class="pln"><span class="n"><a href="#t3671">3671</a></span><span class="t"><span class="str">    torch.Size([10, 3, 5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3672" class="pln"><span class="n"><a href="#t3672">3672</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3673" class="pln"><span class="n"><a href="#t3673">3673</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3674" class="pln"><span class="n"><a href="#t3674">3674</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3675" class="run"><span class="n"><a href="#t3675">3675</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">mode</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3676" class="pln"><span class="n"><a href="#t3676">3676</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3677" class="pln"><span class="n"><a href="#t3677">3677</a></span><span class="t"><span class="str">mode(input, dim=-1, keepdim=False, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3678" class="pln"><span class="n"><a href="#t3678">3678</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3679" class="pln"><span class="n"><a href="#t3679">3679</a></span><span class="t"><span class="str">Returns a namedtuple ``(values, indices)`` where ``values`` is the mode</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3680" class="pln"><span class="n"><a href="#t3680">3680</a></span><span class="t"><span class="str">value of each row of the :attr:`input` tensor in the given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3681" class="pln"><span class="n"><a href="#t3681">3681</a></span><span class="t"><span class="str">:attr:`dim`, i.e. a value which appears most often</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3682" class="pln"><span class="n"><a href="#t3682">3682</a></span><span class="t"><span class="str">in that row, and ``indices`` is the index location of each mode value found.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3683" class="pln"><span class="n"><a href="#t3683">3683</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3684" class="pln"><span class="n"><a href="#t3684">3684</a></span><span class="t"><span class="str">By default, :attr:`dim` is the last dimension of the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3685" class="pln"><span class="n"><a href="#t3685">3685</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3686" class="pln"><span class="n"><a href="#t3686">3686</a></span><span class="t"><span class="str">If :attr:`keepdim` is ``True``, the output tensors are of the same size as</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3687" class="pln"><span class="n"><a href="#t3687">3687</a></span><span class="t"><span class="str">:attr:`input` except in the dimension :attr:`dim` where they are of size 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3688" class="pln"><span class="n"><a href="#t3688">3688</a></span><span class="t"><span class="str">Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3689" class="pln"><span class="n"><a href="#t3689">3689</a></span><span class="t"><span class="str">in the output tensors having 1 fewer dimension than :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3690" class="pln"><span class="n"><a href="#t3690">3690</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3691" class="pln"><span class="n"><a href="#t3691">3691</a></span><span class="t"><span class="str">.. note:: This function is not defined for ``torch.cuda.Tensor`` yet.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3692" class="pln"><span class="n"><a href="#t3692">3692</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3693" class="pln"><span class="n"><a href="#t3693">3693</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3694" class="pln"><span class="n"><a href="#t3694">3694</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3695" class="pln"><span class="n"><a href="#t3695">3695</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3696" class="pln"><span class="n"><a href="#t3696">3696</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3697" class="pln"><span class="n"><a href="#t3697">3697</a></span><span class="t"><span class="str">    out (tuple, optional): the result tuple of two output tensors (values, indices)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3698" class="pln"><span class="n"><a href="#t3698">3698</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3699" class="pln"><span class="n"><a href="#t3699">3699</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3700" class="pln"><span class="n"><a href="#t3700">3700</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3701" class="pln"><span class="n"><a href="#t3701">3701</a></span><span class="t"><span class="str">    >>> a = torch.randint(10, (5,))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3702" class="pln"><span class="n"><a href="#t3702">3702</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3703" class="pln"><span class="n"><a href="#t3703">3703</a></span><span class="t"><span class="str">    tensor([6, 5, 1, 0, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3704" class="pln"><span class="n"><a href="#t3704">3704</a></span><span class="t"><span class="str">    >>> b = a + (torch.randn(50, 1) * 5).long()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3705" class="pln"><span class="n"><a href="#t3705">3705</a></span><span class="t"><span class="str">    >>> torch.mode(b, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3706" class="pln"><span class="n"><a href="#t3706">3706</a></span><span class="t"><span class="str">    torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3707" class="pln"><span class="n"><a href="#t3707">3707</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3708" class="pln"><span class="n"><a href="#t3708">3708</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3709" class="run"><span class="n"><a href="#t3709">3709</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">mul</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3710" class="pln"><span class="n"><a href="#t3710">3710</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3711" class="pln"><span class="n"><a href="#t3711">3711</a></span><span class="t"><span class="str">.. function:: mul(input, other, out=None)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3712" class="pln"><span class="n"><a href="#t3712">3712</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3713" class="pln"><span class="n"><a href="#t3713">3713</a></span><span class="t"><span class="str">Multiplies each element of the input :attr:`input` with the scalar</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3714" class="pln"><span class="n"><a href="#t3714">3714</a></span><span class="t"><span class="str">:attr:`other` and returns a new resulting tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3715" class="pln"><span class="n"><a href="#t3715">3715</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3716" class="pln"><span class="n"><a href="#t3716">3716</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3717" class="pln"><span class="n"><a href="#t3717">3717</a></span><span class="t"><span class="str">    \text{out}_i = \text{other} \times \text{input}_i</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3718" class="pln"><span class="n"><a href="#t3718">3718</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3719" class="pln"><span class="n"><a href="#t3719">3719</a></span><span class="t"><span class="str">If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, :attr:`other`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3720" class="pln"><span class="n"><a href="#t3720">3720</a></span><span class="t"><span class="str">should be a real number, otherwise it should be an integer</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3721" class="pln"><span class="n"><a href="#t3721">3721</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3722" class="pln"><span class="n"><a href="#t3722">3722</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3723" class="pln"><span class="n"><a href="#t3723">3723</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3724" class="pln"><span class="n"><a href="#t3724">3724</a></span><span class="t"><span class="str">    value (Number): the number to be multiplied to each element of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3725" class="pln"><span class="n"><a href="#t3725">3725</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3726" class="pln"><span class="n"><a href="#t3726">3726</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3727" class="pln"><span class="n"><a href="#t3727">3727</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3728" class="pln"><span class="n"><a href="#t3728">3728</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3729" class="pln"><span class="n"><a href="#t3729">3729</a></span><span class="t"><span class="str">    >>> a = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3730" class="pln"><span class="n"><a href="#t3730">3730</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3731" class="pln"><span class="n"><a href="#t3731">3731</a></span><span class="t"><span class="str">    tensor([ 0.2015, -0.4255,  2.6087])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3732" class="pln"><span class="n"><a href="#t3732">3732</a></span><span class="t"><span class="str">    >>> torch.mul(a, 100)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3733" class="pln"><span class="n"><a href="#t3733">3733</a></span><span class="t"><span class="str">    tensor([  20.1494,  -42.5491,  260.8663])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3734" class="pln"><span class="n"><a href="#t3734">3734</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3735" class="pln"><span class="n"><a href="#t3735">3735</a></span><span class="t"><span class="str">.. function:: mul(input, other, out=None)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3736" class="pln"><span class="n"><a href="#t3736">3736</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3737" class="pln"><span class="n"><a href="#t3737">3737</a></span><span class="t"><span class="str">Each element of the tensor :attr:`input` is multiplied by the corresponding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3738" class="pln"><span class="n"><a href="#t3738">3738</a></span><span class="t"><span class="str">element of the Tensor :attr:`other`. The resulting tensor is returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3739" class="pln"><span class="n"><a href="#t3739">3739</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3740" class="pln"><span class="n"><a href="#t3740">3740</a></span><span class="t"><span class="str">The shapes of :attr:`input` and :attr:`other` must be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3741" class="pln"><span class="n"><a href="#t3741">3741</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3742" class="pln"><span class="n"><a href="#t3742">3742</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3743" class="pln"><span class="n"><a href="#t3743">3743</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3744" class="pln"><span class="n"><a href="#t3744">3744</a></span><span class="t"><span class="str">    \text{out}_i = \text{input}_i \times \text{other}_i</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3745" class="pln"><span class="n"><a href="#t3745">3745</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3746" class="pln"><span class="n"><a href="#t3746">3746</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3747" class="pln"><span class="n"><a href="#t3747">3747</a></span><span class="t"><span class="str">    input (Tensor): the first multiplicand tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3748" class="pln"><span class="n"><a href="#t3748">3748</a></span><span class="t"><span class="str">    other (Tensor): the second multiplicand tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3749" class="pln"><span class="n"><a href="#t3749">3749</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3750" class="pln"><span class="n"><a href="#t3750">3750</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3751" class="pln"><span class="n"><a href="#t3751">3751</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3752" class="pln"><span class="n"><a href="#t3752">3752</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3753" class="pln"><span class="n"><a href="#t3753">3753</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3754" class="pln"><span class="n"><a href="#t3754">3754</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3755" class="pln"><span class="n"><a href="#t3755">3755</a></span><span class="t"><span class="str">    tensor([[ 1.1207],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3756" class="pln"><span class="n"><a href="#t3756">3756</a></span><span class="t"><span class="str">            [-0.3137],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3757" class="pln"><span class="n"><a href="#t3757">3757</a></span><span class="t"><span class="str">            [ 0.0700],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3758" class="pln"><span class="n"><a href="#t3758">3758</a></span><span class="t"><span class="str">            [ 0.8378]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3759" class="pln"><span class="n"><a href="#t3759">3759</a></span><span class="t"><span class="str">    >>> b = torch.randn(1, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3760" class="pln"><span class="n"><a href="#t3760">3760</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3761" class="pln"><span class="n"><a href="#t3761">3761</a></span><span class="t"><span class="str">    tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3762" class="pln"><span class="n"><a href="#t3762">3762</a></span><span class="t"><span class="str">    >>> torch.mul(a, b)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3763" class="pln"><span class="n"><a href="#t3763">3763</a></span><span class="t"><span class="str">    tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3764" class="pln"><span class="n"><a href="#t3764">3764</a></span><span class="t"><span class="str">            [-0.1614, -0.0382,  0.1645, -0.7021],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3765" class="pln"><span class="n"><a href="#t3765">3765</a></span><span class="t"><span class="str">            [ 0.0360,  0.0085, -0.0367,  0.1567],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3766" class="pln"><span class="n"><a href="#t3766">3766</a></span><span class="t"><span class="str">            [ 0.4312,  0.1019, -0.4394,  1.8753]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3767" class="pln"><span class="n"><a href="#t3767">3767</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3768" class="pln"><span class="n"><a href="#t3768">3768</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3769" class="run"><span class="n"><a href="#t3769">3769</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">multinomial</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3770" class="pln"><span class="n"><a href="#t3770">3770</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3771" class="pln"><span class="n"><a href="#t3771">3771</a></span><span class="t"><span class="str">multinomial(input, num_samples, replacement=False, *, generator=None, out=None) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3772" class="pln"><span class="n"><a href="#t3772">3772</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3773" class="pln"><span class="n"><a href="#t3773">3773</a></span><span class="t"><span class="str">Returns a tensor where each row contains :attr:`num_samples` indices sampled</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3774" class="pln"><span class="n"><a href="#t3774">3774</a></span><span class="t"><span class="str">from the multinomial probability distribution located in the corresponding row</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3775" class="pln"><span class="n"><a href="#t3775">3775</a></span><span class="t"><span class="str">of tensor :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3776" class="pln"><span class="n"><a href="#t3776">3776</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3777" class="pln"><span class="n"><a href="#t3777">3777</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3778" class="pln"><span class="n"><a href="#t3778">3778</a></span><span class="t"><span class="str">    The rows of :attr:`input` do not need to sum to one (in which case we use</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3779" class="pln"><span class="n"><a href="#t3779">3779</a></span><span class="t"><span class="str">    the values as weights), but must be non-negative, finite and have</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3780" class="pln"><span class="n"><a href="#t3780">3780</a></span><span class="t"><span class="str">    a non-zero sum.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3781" class="pln"><span class="n"><a href="#t3781">3781</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3782" class="pln"><span class="n"><a href="#t3782">3782</a></span><span class="t"><span class="str">Indices are ordered from left to right according to when each was sampled</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3783" class="pln"><span class="n"><a href="#t3783">3783</a></span><span class="t"><span class="str">(first samples are placed in first column).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3784" class="pln"><span class="n"><a href="#t3784">3784</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3785" class="pln"><span class="n"><a href="#t3785">3785</a></span><span class="t"><span class="str">If :attr:`input` is a vector, :attr:`out` is a vector of size :attr:`num_samples`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3786" class="pln"><span class="n"><a href="#t3786">3786</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3787" class="pln"><span class="n"><a href="#t3787">3787</a></span><span class="t"><span class="str">If :attr:`input` is a matrix with `m` rows, :attr:`out` is an matrix of shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3788" class="pln"><span class="n"><a href="#t3788">3788</a></span><span class="t"><span class="str">:math:`(m \times \text{{num\_samples}})`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3789" class="pln"><span class="n"><a href="#t3789">3789</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3790" class="pln"><span class="n"><a href="#t3790">3790</a></span><span class="t"><span class="str">If replacement is ``True``, samples are drawn with replacement.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3791" class="pln"><span class="n"><a href="#t3791">3791</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3792" class="pln"><span class="n"><a href="#t3792">3792</a></span><span class="t"><span class="str">If not, they are drawn without replacement, which means that when a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3793" class="pln"><span class="n"><a href="#t3793">3793</a></span><span class="t"><span class="str">sample index is drawn for a row, it cannot be drawn again for that row.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3794" class="pln"><span class="n"><a href="#t3794">3794</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3795" class="pln"><span class="n"><a href="#t3795">3795</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3796" class="pln"><span class="n"><a href="#t3796">3796</a></span><span class="t"><span class="str">    When drawn without replacement, :attr:`num_samples` must be lower than</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3797" class="pln"><span class="n"><a href="#t3797">3797</a></span><span class="t"><span class="str">    number of non-zero elements in :attr:`input` (or the min number of non-zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3798" class="pln"><span class="n"><a href="#t3798">3798</a></span><span class="t"><span class="str">    elements in each row of :attr:`input` if it is a matrix).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3799" class="pln"><span class="n"><a href="#t3799">3799</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3800" class="pln"><span class="n"><a href="#t3800">3800</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3801" class="pln"><span class="n"><a href="#t3801">3801</a></span><span class="t"><span class="str">    input (Tensor): the input tensor containing probabilities</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3802" class="pln"><span class="n"><a href="#t3802">3802</a></span><span class="t"><span class="str">    num_samples (int): number of samples to draw</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3803" class="pln"><span class="n"><a href="#t3803">3803</a></span><span class="t"><span class="str">    replacement (bool, optional): whether to draw with replacement or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3804" class="pln"><span class="n"><a href="#t3804">3804</a></span><span class="t"><span class="str">    {generator}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3805" class="pln"><span class="n"><a href="#t3805">3805</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3806" class="pln"><span class="n"><a href="#t3806">3806</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3807" class="pln"><span class="n"><a href="#t3807">3807</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3808" class="pln"><span class="n"><a href="#t3808">3808</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3809" class="pln"><span class="n"><a href="#t3809">3809</a></span><span class="t"><span class="str">    >>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3810" class="pln"><span class="n"><a href="#t3810">3810</a></span><span class="t"><span class="str">    >>> torch.multinomial(weights, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3811" class="pln"><span class="n"><a href="#t3811">3811</a></span><span class="t"><span class="str">    tensor([1, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3812" class="pln"><span class="n"><a href="#t3812">3812</a></span><span class="t"><span class="str">    >>> torch.multinomial(weights, 4) # ERROR!</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3813" class="pln"><span class="n"><a href="#t3813">3813</a></span><span class="t"><span class="str">    RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3814" class="pln"><span class="n"><a href="#t3814">3814</a></span><span class="t"><span class="str">    not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3815" class="pln"><span class="n"><a href="#t3815">3815</a></span><span class="t"><span class="str">    >>> torch.multinomial(weights, 4, replacement=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3816" class="pln"><span class="n"><a href="#t3816">3816</a></span><span class="t"><span class="str">    tensor([ 2,  1,  1,  1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3817" class="pln"><span class="n"><a href="#t3817">3817</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3818" class="pln"><span class="n"><a href="#t3818">3818</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3819" class="run"><span class="n"><a href="#t3819">3819</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">mv</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3820" class="pln"><span class="n"><a href="#t3820">3820</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3821" class="pln"><span class="n"><a href="#t3821">3821</a></span><span class="t"><span class="str">mv(input, vec, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3822" class="pln"><span class="n"><a href="#t3822">3822</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3823" class="pln"><span class="n"><a href="#t3823">3823</a></span><span class="t"><span class="str">Performs a matrix-vector product of the matrix :attr:`input` and the vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3824" class="pln"><span class="n"><a href="#t3824">3824</a></span><span class="t"><span class="str">:attr:`vec`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3825" class="pln"><span class="n"><a href="#t3825">3825</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3826" class="pln"><span class="n"><a href="#t3826">3826</a></span><span class="t"><span class="str">If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3827" class="pln"><span class="n"><a href="#t3827">3827</a></span><span class="t"><span class="str">size :math:`m`, :attr:`out` will be 1-D of size :math:`n`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3828" class="pln"><span class="n"><a href="#t3828">3828</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3829" class="pln"><span class="n"><a href="#t3829">3829</a></span><span class="t"><span class="str">.. note:: This function does not :ref:`broadcast &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3830" class="pln"><span class="n"><a href="#t3830">3830</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3831" class="pln"><span class="n"><a href="#t3831">3831</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3832" class="pln"><span class="n"><a href="#t3832">3832</a></span><span class="t"><span class="str">    input (Tensor): matrix to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3833" class="pln"><span class="n"><a href="#t3833">3833</a></span><span class="t"><span class="str">    vec (Tensor): vector to be multiplied</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3834" class="pln"><span class="n"><a href="#t3834">3834</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3835" class="pln"><span class="n"><a href="#t3835">3835</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3836" class="pln"><span class="n"><a href="#t3836">3836</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3837" class="pln"><span class="n"><a href="#t3837">3837</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3838" class="pln"><span class="n"><a href="#t3838">3838</a></span><span class="t"><span class="str">    >>> mat = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3839" class="pln"><span class="n"><a href="#t3839">3839</a></span><span class="t"><span class="str">    >>> vec = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3840" class="pln"><span class="n"><a href="#t3840">3840</a></span><span class="t"><span class="str">    >>> torch.mv(mat, vec)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3841" class="pln"><span class="n"><a href="#t3841">3841</a></span><span class="t"><span class="str">    tensor([ 1.0404, -0.6361])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3842" class="pln"><span class="n"><a href="#t3842">3842</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3843" class="pln"><span class="n"><a href="#t3843">3843</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3844" class="run"><span class="n"><a href="#t3844">3844</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">mvlgamma</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3845" class="pln"><span class="n"><a href="#t3845">3845</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3846" class="pln"><span class="n"><a href="#t3846">3846</a></span><span class="t"><span class="str">mvlgamma(input, p) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3847" class="pln"><span class="n"><a href="#t3847">3847</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3848" class="pln"><span class="n"><a href="#t3848">3848</a></span><span class="t"><span class="str">Computes the `multivariate log-gamma function</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3849" class="pln"><span class="n"><a href="#t3849">3849</a></span><span class="t"><span class="str">&lt;https://en.wikipedia.org/wiki/Multivariate_gamma_function>`_) with dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3850" class="pln"><span class="n"><a href="#t3850">3850</a></span><span class="t"><span class="str">:math:`p` element-wise, given by</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3851" class="pln"><span class="n"><a href="#t3851">3851</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3852" class="pln"><span class="n"><a href="#t3852">3852</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3853" class="pln"><span class="n"><a href="#t3853">3853</a></span><span class="t"><span class="str">    \log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3854" class="pln"><span class="n"><a href="#t3854">3854</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3855" class="pln"><span class="n"><a href="#t3855">3855</a></span><span class="t"><span class="str">where :math:`C = \log(\pi) \times \frac{p (p - 1)}{4}` and :math:`\Gamma(\cdot)` is the Gamma function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3856" class="pln"><span class="n"><a href="#t3856">3856</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3857" class="pln"><span class="n"><a href="#t3857">3857</a></span><span class="t"><span class="str">All elements must be greater than :math:`\frac{p - 1}{2}`, otherwise an error would be thrown.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3858" class="pln"><span class="n"><a href="#t3858">3858</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3859" class="pln"><span class="n"><a href="#t3859">3859</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3860" class="pln"><span class="n"><a href="#t3860">3860</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compute the multivariate log-gamma function</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3861" class="pln"><span class="n"><a href="#t3861">3861</a></span><span class="t"><span class="str">    p (int): the number of dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3862" class="pln"><span class="n"><a href="#t3862">3862</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3863" class="pln"><span class="n"><a href="#t3863">3863</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3864" class="pln"><span class="n"><a href="#t3864">3864</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3865" class="pln"><span class="n"><a href="#t3865">3865</a></span><span class="t"><span class="str">    >>> a = torch.empty(2, 3).uniform_(1, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3866" class="pln"><span class="n"><a href="#t3866">3866</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3867" class="pln"><span class="n"><a href="#t3867">3867</a></span><span class="t"><span class="str">    tensor([[1.6835, 1.8474, 1.1929],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3868" class="pln"><span class="n"><a href="#t3868">3868</a></span><span class="t"><span class="str">            [1.0475, 1.7162, 1.4180]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3869" class="pln"><span class="n"><a href="#t3869">3869</a></span><span class="t"><span class="str">    >>> torch.mvlgamma(a, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3870" class="pln"><span class="n"><a href="#t3870">3870</a></span><span class="t"><span class="str">    tensor([[0.3928, 0.4007, 0.7586],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3871" class="pln"><span class="n"><a href="#t3871">3871</a></span><span class="t"><span class="str">            [1.0311, 0.3901, 0.5049]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3872" class="pln"><span class="n"><a href="#t3872">3872</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3873" class="pln"><span class="n"><a href="#t3873">3873</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3874" class="run"><span class="n"><a href="#t3874">3874</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">narrow</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3875" class="pln"><span class="n"><a href="#t3875">3875</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3876" class="pln"><span class="n"><a href="#t3876">3876</a></span><span class="t"><span class="str">narrow(input, dim, start, length) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3877" class="pln"><span class="n"><a href="#t3877">3877</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3878" class="pln"><span class="n"><a href="#t3878">3878</a></span><span class="t"><span class="str">Returns a new tensor that is a narrowed version of :attr:`input` tensor. The</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3879" class="pln"><span class="n"><a href="#t3879">3879</a></span><span class="t"><span class="str">dimension :attr:`dim` is input from :attr:`start` to :attr:`start + length`. The</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3880" class="pln"><span class="n"><a href="#t3880">3880</a></span><span class="t"><span class="str">returned tensor and :attr:`input` tensor share the same underlying storage.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3881" class="pln"><span class="n"><a href="#t3881">3881</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3882" class="pln"><span class="n"><a href="#t3882">3882</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3883" class="pln"><span class="n"><a href="#t3883">3883</a></span><span class="t"><span class="str">    input (Tensor): the tensor to narrow</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3884" class="pln"><span class="n"><a href="#t3884">3884</a></span><span class="t"><span class="str">    dim (int): the dimension along which to narrow</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3885" class="pln"><span class="n"><a href="#t3885">3885</a></span><span class="t"><span class="str">    start (int): the starting dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3886" class="pln"><span class="n"><a href="#t3886">3886</a></span><span class="t"><span class="str">    length (int): the distance to the ending dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3887" class="pln"><span class="n"><a href="#t3887">3887</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3888" class="pln"><span class="n"><a href="#t3888">3888</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3889" class="pln"><span class="n"><a href="#t3889">3889</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3890" class="pln"><span class="n"><a href="#t3890">3890</a></span><span class="t"><span class="str">    >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3891" class="pln"><span class="n"><a href="#t3891">3891</a></span><span class="t"><span class="str">    >>> torch.narrow(x, 0, 0, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3892" class="pln"><span class="n"><a href="#t3892">3892</a></span><span class="t"><span class="str">    tensor([[ 1,  2,  3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3893" class="pln"><span class="n"><a href="#t3893">3893</a></span><span class="t"><span class="str">            [ 4,  5,  6]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3894" class="pln"><span class="n"><a href="#t3894">3894</a></span><span class="t"><span class="str">    >>> torch.narrow(x, 1, 1, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3895" class="pln"><span class="n"><a href="#t3895">3895</a></span><span class="t"><span class="str">    tensor([[ 2,  3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3896" class="pln"><span class="n"><a href="#t3896">3896</a></span><span class="t"><span class="str">            [ 5,  6],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3897" class="pln"><span class="n"><a href="#t3897">3897</a></span><span class="t"><span class="str">            [ 8,  9]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3898" class="pln"><span class="n"><a href="#t3898">3898</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3899" class="pln"><span class="n"><a href="#t3899">3899</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3900" class="run"><span class="n"><a href="#t3900">3900</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ne</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3901" class="pln"><span class="n"><a href="#t3901">3901</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3902" class="pln"><span class="n"><a href="#t3902">3902</a></span><span class="t"><span class="str">ne(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3903" class="pln"><span class="n"><a href="#t3903">3903</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3904" class="pln"><span class="n"><a href="#t3904">3904</a></span><span class="t"><span class="str">Computes :math:`input \neq other` element-wise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3905" class="pln"><span class="n"><a href="#t3905">3905</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3906" class="pln"><span class="n"><a href="#t3906">3906</a></span><span class="t"><span class="str">The second argument can be a number or a tensor whose shape is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3907" class="pln"><span class="n"><a href="#t3907">3907</a></span><span class="t"><span class="str">:ref:`broadcastable &lt;broadcasting-semantics>` with the first argument.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3908" class="pln"><span class="n"><a href="#t3908">3908</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3909" class="pln"><span class="n"><a href="#t3909">3909</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3910" class="pln"><span class="n"><a href="#t3910">3910</a></span><span class="t"><span class="str">    input (Tensor): the tensor to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3911" class="pln"><span class="n"><a href="#t3911">3911</a></span><span class="t"><span class="str">    other (Tensor or float): the tensor or value to compare</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3912" class="pln"><span class="n"><a href="#t3912">3912</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor that must be a `BoolTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3913" class="pln"><span class="n"><a href="#t3913">3913</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3914" class="pln"><span class="n"><a href="#t3914">3914</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3915" class="pln"><span class="n"><a href="#t3915">3915</a></span><span class="t"><span class="str">    Tensor: A ``torch.BoolTensor`` containing a True at each location where comparison is true.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3916" class="pln"><span class="n"><a href="#t3916">3916</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3917" class="pln"><span class="n"><a href="#t3917">3917</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3918" class="pln"><span class="n"><a href="#t3918">3918</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3919" class="pln"><span class="n"><a href="#t3919">3919</a></span><span class="t"><span class="str">    >>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3920" class="pln"><span class="n"><a href="#t3920">3920</a></span><span class="t"><span class="str">    tensor([[False, True], [True, False]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3921" class="pln"><span class="n"><a href="#t3921">3921</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3922" class="pln"><span class="n"><a href="#t3922">3922</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3923" class="run"><span class="n"><a href="#t3923">3923</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">neg</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3924" class="pln"><span class="n"><a href="#t3924">3924</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3925" class="pln"><span class="n"><a href="#t3925">3925</a></span><span class="t"><span class="str">neg(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3926" class="pln"><span class="n"><a href="#t3926">3926</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3927" class="pln"><span class="n"><a href="#t3927">3927</a></span><span class="t"><span class="str">Returns a new tensor with the negative of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3928" class="pln"><span class="n"><a href="#t3928">3928</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3929" class="pln"><span class="n"><a href="#t3929">3929</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3930" class="pln"><span class="n"><a href="#t3930">3930</a></span><span class="t"><span class="str">    \text{out} = -1 \times \text{input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3931" class="pln"><span class="n"><a href="#t3931">3931</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3932" class="pln"><span class="n"><a href="#t3932">3932</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3933" class="pln"><span class="n"><a href="#t3933">3933</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3934" class="pln"><span class="n"><a href="#t3934">3934</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3935" class="pln"><span class="n"><a href="#t3935">3935</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3936" class="pln"><span class="n"><a href="#t3936">3936</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3937" class="pln"><span class="n"><a href="#t3937">3937</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3938" class="pln"><span class="n"><a href="#t3938">3938</a></span><span class="t"><span class="str">    >>> a = torch.randn(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3939" class="pln"><span class="n"><a href="#t3939">3939</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3940" class="pln"><span class="n"><a href="#t3940">3940</a></span><span class="t"><span class="str">    tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3941" class="pln"><span class="n"><a href="#t3941">3941</a></span><span class="t"><span class="str">    >>> torch.neg(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3942" class="pln"><span class="n"><a href="#t3942">3942</a></span><span class="t"><span class="str">    tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3943" class="pln"><span class="n"><a href="#t3943">3943</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3944" class="pln"><span class="n"><a href="#t3944">3944</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3945" class="run"><span class="n"><a href="#t3945">3945</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">nonzero</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3946" class="pln"><span class="n"><a href="#t3946">3946</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3947" class="pln"><span class="n"><a href="#t3947">3947</a></span><span class="t"><span class="str">nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3948" class="pln"><span class="n"><a href="#t3948">3948</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3949" class="pln"><span class="n"><a href="#t3949">3949</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3950" class="pln"><span class="n"><a href="#t3950">3950</a></span><span class="t"><span class="str">    :func:`torch.nonzero(..., as_tuple=False) &lt;torch.nonzero>` (default) returns a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3951" class="pln"><span class="n"><a href="#t3951">3951</a></span><span class="t"><span class="str">    2-D tensor where each row is the index for a nonzero value.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3952" class="pln"><span class="n"><a href="#t3952">3952</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3953" class="pln"><span class="n"><a href="#t3953">3953</a></span><span class="t"><span class="str">    :func:`torch.nonzero(..., as_tuple=True) &lt;torch.nonzero>` returns a tuple of 1-D</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3954" class="pln"><span class="n"><a href="#t3954">3954</a></span><span class="t"><span class="str">    index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3955" class="pln"><span class="n"><a href="#t3955">3955</a></span><span class="t"><span class="str">    gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3956" class="pln"><span class="n"><a href="#t3956">3956</a></span><span class="t"><span class="str">    contains nonzero indices for a certain dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3957" class="pln"><span class="n"><a href="#t3957">3957</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3958" class="pln"><span class="n"><a href="#t3958">3958</a></span><span class="t"><span class="str">    See below for more details on the two behaviors.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3959" class="pln"><span class="n"><a href="#t3959">3959</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3960" class="pln"><span class="n"><a href="#t3960">3960</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3961" class="pln"><span class="n"><a href="#t3961">3961</a></span><span class="t"><span class="str">**When** :attr:`as_tuple` **is ``False`` (default)**:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3962" class="pln"><span class="n"><a href="#t3962">3962</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3963" class="pln"><span class="n"><a href="#t3963">3963</a></span><span class="t"><span class="str">Returns a tensor containing the indices of all non-zero elements of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3964" class="pln"><span class="n"><a href="#t3964">3964</a></span><span class="t"><span class="str">:attr:`input`.  Each row in the result contains the indices of a non-zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3965" class="pln"><span class="n"><a href="#t3965">3965</a></span><span class="t"><span class="str">element in :attr:`input`. The result is sorted lexicographically, with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3966" class="pln"><span class="n"><a href="#t3966">3966</a></span><span class="t"><span class="str">the last index changing the fastest (C-style).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3967" class="pln"><span class="n"><a href="#t3967">3967</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3968" class="pln"><span class="n"><a href="#t3968">3968</a></span><span class="t"><span class="str">If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3969" class="pln"><span class="n"><a href="#t3969">3969</a></span><span class="t"><span class="str">:attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3970" class="pln"><span class="n"><a href="#t3970">3970</a></span><span class="t"><span class="str">non-zero elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3971" class="pln"><span class="n"><a href="#t3971">3971</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3972" class="pln"><span class="n"><a href="#t3972">3972</a></span><span class="t"><span class="str">**When** :attr:`as_tuple` **is ``True``**:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3973" class="pln"><span class="n"><a href="#t3973">3973</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3974" class="pln"><span class="n"><a href="#t3974">3974</a></span><span class="t"><span class="str">Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3975" class="pln"><span class="n"><a href="#t3975">3975</a></span><span class="t"><span class="str">each containing the indices (in that dimension) of all non-zero elements of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3976" class="pln"><span class="n"><a href="#t3976">3976</a></span><span class="t"><span class="str">:attr:`input` .</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3977" class="pln"><span class="n"><a href="#t3977">3977</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3978" class="pln"><span class="n"><a href="#t3978">3978</a></span><span class="t"><span class="str">If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3979" class="pln"><span class="n"><a href="#t3979">3979</a></span><span class="t"><span class="str">tensors of size :math:`z`, where :math:`z` is the total number of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3980" class="pln"><span class="n"><a href="#t3980">3980</a></span><span class="t"><span class="str">non-zero elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3981" class="pln"><span class="n"><a href="#t3981">3981</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3982" class="pln"><span class="n"><a href="#t3982">3982</a></span><span class="t"><span class="str">As a special case, when :attr:`input` has zero dimensions and a nonzero scalar</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3983" class="pln"><span class="n"><a href="#t3983">3983</a></span><span class="t"><span class="str">value, it is treated as a one-dimensional tensor with one element.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3984" class="pln"><span class="n"><a href="#t3984">3984</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3985" class="pln"><span class="n"><a href="#t3985">3985</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3986" class="pln"><span class="n"><a href="#t3986">3986</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3987" class="pln"><span class="n"><a href="#t3987">3987</a></span><span class="t"><span class="str">    out (LongTensor, optional): the output tensor containing indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3988" class="pln"><span class="n"><a href="#t3988">3988</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3989" class="pln"><span class="n"><a href="#t3989">3989</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3990" class="pln"><span class="n"><a href="#t3990">3990</a></span><span class="t"><span class="str">    LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3991" class="pln"><span class="n"><a href="#t3991">3991</a></span><span class="t"><span class="str">    tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3992" class="pln"><span class="n"><a href="#t3992">3992</a></span><span class="t"><span class="str">    each dimension, containing the indices of each nonzero element along that</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3993" class="pln"><span class="n"><a href="#t3993">3993</a></span><span class="t"><span class="str">    dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3994" class="pln"><span class="n"><a href="#t3994">3994</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3995" class="pln"><span class="n"><a href="#t3995">3995</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3996" class="pln"><span class="n"><a href="#t3996">3996</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t3997" class="pln"><span class="n"><a href="#t3997">3997</a></span><span class="t"><span class="str">    >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3998" class="pln"><span class="n"><a href="#t3998">3998</a></span><span class="t"><span class="str">    tensor([[ 0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3999" class="pln"><span class="n"><a href="#t3999">3999</a></span><span class="t"><span class="str">            [ 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4000" class="pln"><span class="n"><a href="#t4000">4000</a></span><span class="t"><span class="str">            [ 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4001" class="pln"><span class="n"><a href="#t4001">4001</a></span><span class="t"><span class="str">            [ 4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4002" class="pln"><span class="n"><a href="#t4002">4002</a></span><span class="t"><span class="str">    >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4003" class="pln"><span class="n"><a href="#t4003">4003</a></span><span class="t"><span class="str">                                    [0.0, 0.4, 0.0, 0.0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4004" class="pln"><span class="n"><a href="#t4004">4004</a></span><span class="t"><span class="str">                                    [0.0, 0.0, 1.2, 0.0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4005" class="pln"><span class="n"><a href="#t4005">4005</a></span><span class="t"><span class="str">                                    [0.0, 0.0, 0.0,-0.4]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4006" class="pln"><span class="n"><a href="#t4006">4006</a></span><span class="t"><span class="str">    tensor([[ 0,  0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4007" class="pln"><span class="n"><a href="#t4007">4007</a></span><span class="t"><span class="str">            [ 1,  1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4008" class="pln"><span class="n"><a href="#t4008">4008</a></span><span class="t"><span class="str">            [ 2,  2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4009" class="pln"><span class="n"><a href="#t4009">4009</a></span><span class="t"><span class="str">            [ 3,  3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4010" class="pln"><span class="n"><a href="#t4010">4010</a></span><span class="t"><span class="str">    >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4011" class="pln"><span class="n"><a href="#t4011">4011</a></span><span class="t"><span class="str">    (tensor([0, 1, 2, 4]),)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4012" class="pln"><span class="n"><a href="#t4012">4012</a></span><span class="t"><span class="str">    >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4013" class="pln"><span class="n"><a href="#t4013">4013</a></span><span class="t"><span class="str">                                    [0.0, 0.4, 0.0, 0.0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4014" class="pln"><span class="n"><a href="#t4014">4014</a></span><span class="t"><span class="str">                                    [0.0, 0.0, 1.2, 0.0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4015" class="pln"><span class="n"><a href="#t4015">4015</a></span><span class="t"><span class="str">                                    [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4016" class="pln"><span class="n"><a href="#t4016">4016</a></span><span class="t"><span class="str">    (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4017" class="pln"><span class="n"><a href="#t4017">4017</a></span><span class="t"><span class="str">    >>> torch.nonzero(torch.tensor(5), as_tuple=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4018" class="pln"><span class="n"><a href="#t4018">4018</a></span><span class="t"><span class="str">    (tensor([0]),)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4019" class="pln"><span class="n"><a href="#t4019">4019</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4020" class="pln"><span class="n"><a href="#t4020">4020</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4021" class="run"><span class="n"><a href="#t4021">4021</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">normal</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4022" class="pln"><span class="n"><a href="#t4022">4022</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4023" class="pln"><span class="n"><a href="#t4023">4023</a></span><span class="t"><span class="str">.. function:: normal(mean, std, *, generator=None, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4024" class="pln"><span class="n"><a href="#t4024">4024</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4025" class="pln"><span class="n"><a href="#t4025">4025</a></span><span class="t"><span class="str">Returns a tensor of random numbers drawn from separate normal distributions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4026" class="pln"><span class="n"><a href="#t4026">4026</a></span><span class="t"><span class="str">whose mean and standard deviation are given.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4027" class="pln"><span class="n"><a href="#t4027">4027</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4028" class="pln"><span class="n"><a href="#t4028">4028</a></span><span class="t"><span class="str">The :attr:`mean` is a tensor with the mean of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4029" class="pln"><span class="n"><a href="#t4029">4029</a></span><span class="t"><span class="str">each output element's normal distribution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4030" class="pln"><span class="n"><a href="#t4030">4030</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4031" class="pln"><span class="n"><a href="#t4031">4031</a></span><span class="t"><span class="str">The :attr:`std` is a tensor with the standard deviation of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4032" class="pln"><span class="n"><a href="#t4032">4032</a></span><span class="t"><span class="str">each output element's normal distribution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4033" class="pln"><span class="n"><a href="#t4033">4033</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4034" class="pln"><span class="n"><a href="#t4034">4034</a></span><span class="t"><span class="str">The shapes of :attr:`mean` and :attr:`std` don't need to match, but the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4035" class="pln"><span class="n"><a href="#t4035">4035</a></span><span class="t"><span class="str">total number of elements in each tensor need to be the same.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4036" class="pln"><span class="n"><a href="#t4036">4036</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4037" class="pln"><span class="n"><a href="#t4037">4037</a></span><span class="t"><span class="str">.. note:: When the shapes do not match, the shape of :attr:`mean`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4038" class="pln"><span class="n"><a href="#t4038">4038</a></span><span class="t"><span class="str">          is used as the shape for the returned output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4039" class="pln"><span class="n"><a href="#t4039">4039</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4040" class="pln"><span class="n"><a href="#t4040">4040</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4041" class="pln"><span class="n"><a href="#t4041">4041</a></span><span class="t"><span class="str">    mean (Tensor): the tensor of per-element means</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4042" class="pln"><span class="n"><a href="#t4042">4042</a></span><span class="t"><span class="str">    std (Tensor): the tensor of per-element standard deviations</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4043" class="pln"><span class="n"><a href="#t4043">4043</a></span><span class="t"><span class="str">    {generator}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4044" class="pln"><span class="n"><a href="#t4044">4044</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4045" class="pln"><span class="n"><a href="#t4045">4045</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4046" class="pln"><span class="n"><a href="#t4046">4046</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4047" class="pln"><span class="n"><a href="#t4047">4047</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4048" class="pln"><span class="n"><a href="#t4048">4048</a></span><span class="t"><span class="str">    >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4049" class="pln"><span class="n"><a href="#t4049">4049</a></span><span class="t"><span class="str">    tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4050" class="pln"><span class="n"><a href="#t4050">4050</a></span><span class="t"><span class="str">              8.0505,   8.1408,   9.0563,  10.0566])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4051" class="pln"><span class="n"><a href="#t4051">4051</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4052" class="pln"><span class="n"><a href="#t4052">4052</a></span><span class="t"><span class="str">.. function:: normal(mean=0.0, std, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4053" class="pln"><span class="n"><a href="#t4053">4053</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4054" class="pln"><span class="n"><a href="#t4054">4054</a></span><span class="t"><span class="str">Similar to the function above, but the means are shared among all drawn</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4055" class="pln"><span class="n"><a href="#t4055">4055</a></span><span class="t"><span class="str">elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4056" class="pln"><span class="n"><a href="#t4056">4056</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4057" class="pln"><span class="n"><a href="#t4057">4057</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4058" class="pln"><span class="n"><a href="#t4058">4058</a></span><span class="t"><span class="str">    mean (float, optional): the mean for all distributions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4059" class="pln"><span class="n"><a href="#t4059">4059</a></span><span class="t"><span class="str">    std (Tensor): the tensor of per-element standard deviations</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4060" class="pln"><span class="n"><a href="#t4060">4060</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4061" class="pln"><span class="n"><a href="#t4061">4061</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4062" class="pln"><span class="n"><a href="#t4062">4062</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4063" class="pln"><span class="n"><a href="#t4063">4063</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4064" class="pln"><span class="n"><a href="#t4064">4064</a></span><span class="t"><span class="str">    >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4065" class="pln"><span class="n"><a href="#t4065">4065</a></span><span class="t"><span class="str">    tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4066" class="pln"><span class="n"><a href="#t4066">4066</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4067" class="pln"><span class="n"><a href="#t4067">4067</a></span><span class="t"><span class="str">.. function:: normal(mean, std=1.0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4068" class="pln"><span class="n"><a href="#t4068">4068</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4069" class="pln"><span class="n"><a href="#t4069">4069</a></span><span class="t"><span class="str">Similar to the function above, but the standard-deviations are shared among</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4070" class="pln"><span class="n"><a href="#t4070">4070</a></span><span class="t"><span class="str">all drawn elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4071" class="pln"><span class="n"><a href="#t4071">4071</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4072" class="pln"><span class="n"><a href="#t4072">4072</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4073" class="pln"><span class="n"><a href="#t4073">4073</a></span><span class="t"><span class="str">    mean (Tensor): the tensor of per-element means</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4074" class="pln"><span class="n"><a href="#t4074">4074</a></span><span class="t"><span class="str">    std (float, optional): the standard deviation for all distributions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4075" class="pln"><span class="n"><a href="#t4075">4075</a></span><span class="t"><span class="str">    out (Tensor, optional): the output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4076" class="pln"><span class="n"><a href="#t4076">4076</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4077" class="pln"><span class="n"><a href="#t4077">4077</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4078" class="pln"><span class="n"><a href="#t4078">4078</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4079" class="pln"><span class="n"><a href="#t4079">4079</a></span><span class="t"><span class="str">    >>> torch.normal(mean=torch.arange(1., 6.))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4080" class="pln"><span class="n"><a href="#t4080">4080</a></span><span class="t"><span class="str">    tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4081" class="pln"><span class="n"><a href="#t4081">4081</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4082" class="pln"><span class="n"><a href="#t4082">4082</a></span><span class="t"><span class="str">.. function:: normal(mean, std, size, *, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4083" class="pln"><span class="n"><a href="#t4083">4083</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4084" class="pln"><span class="n"><a href="#t4084">4084</a></span><span class="t"><span class="str">Similar to the function above, but the means and standard deviations are shared</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4085" class="pln"><span class="n"><a href="#t4085">4085</a></span><span class="t"><span class="str">among all drawn elements. The resulting tensor has size given by :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4086" class="pln"><span class="n"><a href="#t4086">4086</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4087" class="pln"><span class="n"><a href="#t4087">4087</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4088" class="pln"><span class="n"><a href="#t4088">4088</a></span><span class="t"><span class="str">    mean (float): the mean for all distributions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4089" class="pln"><span class="n"><a href="#t4089">4089</a></span><span class="t"><span class="str">    std (float): the standard deviation for all distributions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4090" class="pln"><span class="n"><a href="#t4090">4090</a></span><span class="t"><span class="str">    size (int...): a sequence of integers defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4091" class="pln"><span class="n"><a href="#t4091">4091</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4092" class="pln"><span class="n"><a href="#t4092">4092</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4093" class="pln"><span class="n"><a href="#t4093">4093</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4094" class="pln"><span class="n"><a href="#t4094">4094</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4095" class="pln"><span class="n"><a href="#t4095">4095</a></span><span class="t"><span class="str">    >>> torch.normal(2, 3, size=(1, 4))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4096" class="pln"><span class="n"><a href="#t4096">4096</a></span><span class="t"><span class="str">    tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4097" class="pln"><span class="n"><a href="#t4097">4097</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4098" class="pln"><span class="n"><a href="#t4098">4098</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4099" class="run"><span class="n"><a href="#t4099">4099</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">numel</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4100" class="pln"><span class="n"><a href="#t4100">4100</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4101" class="pln"><span class="n"><a href="#t4101">4101</a></span><span class="t"><span class="str">numel(input) -> int</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4102" class="pln"><span class="n"><a href="#t4102">4102</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4103" class="pln"><span class="n"><a href="#t4103">4103</a></span><span class="t"><span class="str">Returns the total number of elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4104" class="pln"><span class="n"><a href="#t4104">4104</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4105" class="pln"><span class="n"><a href="#t4105">4105</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4106" class="pln"><span class="n"><a href="#t4106">4106</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4107" class="pln"><span class="n"><a href="#t4107">4107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4108" class="pln"><span class="n"><a href="#t4108">4108</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4109" class="pln"><span class="n"><a href="#t4109">4109</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4110" class="pln"><span class="n"><a href="#t4110">4110</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 2, 3, 4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4111" class="pln"><span class="n"><a href="#t4111">4111</a></span><span class="t"><span class="str">    >>> torch.numel(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4112" class="pln"><span class="n"><a href="#t4112">4112</a></span><span class="t"><span class="str">    120</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4113" class="pln"><span class="n"><a href="#t4113">4113</a></span><span class="t"><span class="str">    >>> a = torch.zeros(4,4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4114" class="pln"><span class="n"><a href="#t4114">4114</a></span><span class="t"><span class="str">    >>> torch.numel(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4115" class="pln"><span class="n"><a href="#t4115">4115</a></span><span class="t"><span class="str">    16</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4116" class="pln"><span class="n"><a href="#t4116">4116</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4117" class="pln"><span class="n"><a href="#t4117">4117</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4118" class="pln"><span class="n"><a href="#t4118">4118</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4119" class="run"><span class="n"><a href="#t4119">4119</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ones</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4120" class="pln"><span class="n"><a href="#t4120">4120</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4121" class="pln"><span class="n"><a href="#t4121">4121</a></span><span class="t"><span class="str">ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4122" class="pln"><span class="n"><a href="#t4122">4122</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4123" class="pln"><span class="n"><a href="#t4123">4123</a></span><span class="t"><span class="str">Returns a tensor filled with the scalar value `1`, with the shape defined</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4124" class="pln"><span class="n"><a href="#t4124">4124</a></span><span class="t"><span class="str">by the variable argument :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4125" class="pln"><span class="n"><a href="#t4125">4125</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4126" class="pln"><span class="n"><a href="#t4126">4126</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4127" class="pln"><span class="n"><a href="#t4127">4127</a></span><span class="t"><span class="str">    size (int...): a sequence of integers defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4128" class="pln"><span class="n"><a href="#t4128">4128</a></span><span class="t"><span class="str">        Can be a variable number of arguments or a collection like a list or tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4129" class="pln"><span class="n"><a href="#t4129">4129</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4130" class="pln"><span class="n"><a href="#t4130">4130</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4131" class="pln"><span class="n"><a href="#t4131">4131</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4132" class="pln"><span class="n"><a href="#t4132">4132</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4133" class="pln"><span class="n"><a href="#t4133">4133</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4134" class="pln"><span class="n"><a href="#t4134">4134</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4135" class="pln"><span class="n"><a href="#t4135">4135</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4136" class="pln"><span class="n"><a href="#t4136">4136</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4137" class="pln"><span class="n"><a href="#t4137">4137</a></span><span class="t"><span class="str">    >>> torch.ones(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4138" class="pln"><span class="n"><a href="#t4138">4138</a></span><span class="t"><span class="str">    tensor([[ 1.,  1.,  1.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4139" class="pln"><span class="n"><a href="#t4139">4139</a></span><span class="t"><span class="str">            [ 1.,  1.,  1.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4140" class="pln"><span class="n"><a href="#t4140">4140</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4141" class="pln"><span class="n"><a href="#t4141">4141</a></span><span class="t"><span class="str">    >>> torch.ones(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4142" class="pln"><span class="n"><a href="#t4142">4142</a></span><span class="t"><span class="str">    tensor([ 1.,  1.,  1.,  1.,  1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4143" class="pln"><span class="n"><a href="#t4143">4143</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4144" class="pln"><span class="n"><a href="#t4144">4144</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4145" class="pln"><span class="n"><a href="#t4145">4145</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4146" class="run"><span class="n"><a href="#t4146">4146</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ones_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4147" class="pln"><span class="n"><a href="#t4147">4147</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4148" class="pln"><span class="n"><a href="#t4148">4148</a></span><span class="t"><span class="str">ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4149" class="pln"><span class="n"><a href="#t4149">4149</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4150" class="pln"><span class="n"><a href="#t4150">4150</a></span><span class="t"><span class="str">Returns a tensor filled with the scalar value `1`, with the same size as</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4151" class="pln"><span class="n"><a href="#t4151">4151</a></span><span class="t"><span class="str">:attr:`input`. ``torch.ones_like(input)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4152" class="pln"><span class="n"><a href="#t4152">4152</a></span><span class="t"><span class="str">``torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4153" class="pln"><span class="n"><a href="#t4153">4153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4154" class="pln"><span class="n"><a href="#t4154">4154</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4155" class="pln"><span class="n"><a href="#t4155">4155</a></span><span class="t"><span class="str">    As of 0.4, this function does not support an :attr:`out` keyword. As an alternative,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4156" class="pln"><span class="n"><a href="#t4156">4156</a></span><span class="t"><span class="str">    the old ``torch.ones_like(input, out=output)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4157" class="pln"><span class="n"><a href="#t4157">4157</a></span><span class="t"><span class="str">    ``torch.ones(input.size(), out=output)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4158" class="pln"><span class="n"><a href="#t4158">4158</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4159" class="pln"><span class="n"><a href="#t4159">4159</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4160" class="pln"><span class="n"><a href="#t4160">4160</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4161" class="pln"><span class="n"><a href="#t4161">4161</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4162" class="pln"><span class="n"><a href="#t4162">4162</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4163" class="pln"><span class="n"><a href="#t4163">4163</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4164" class="pln"><span class="n"><a href="#t4164">4164</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4165" class="pln"><span class="n"><a href="#t4165">4165</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4166" class="pln"><span class="n"><a href="#t4166">4166</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4167" class="pln"><span class="n"><a href="#t4167">4167</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4168" class="pln"><span class="n"><a href="#t4168">4168</a></span><span class="t"><span class="str">    >>> input = torch.empty(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4169" class="pln"><span class="n"><a href="#t4169">4169</a></span><span class="t"><span class="str">    >>> torch.ones_like(input)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4170" class="pln"><span class="n"><a href="#t4170">4170</a></span><span class="t"><span class="str">    tensor([[ 1.,  1.,  1.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4171" class="pln"><span class="n"><a href="#t4171">4171</a></span><span class="t"><span class="str">            [ 1.,  1.,  1.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4172" class="pln"><span class="n"><a href="#t4172">4172</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4173" class="pln"><span class="n"><a href="#t4173">4173</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4174" class="run"><span class="n"><a href="#t4174">4174</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">orgqr</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4175" class="pln"><span class="n"><a href="#t4175">4175</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4176" class="pln"><span class="n"><a href="#t4176">4176</a></span><span class="t"><span class="str">orgqr(input, input2) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4177" class="pln"><span class="n"><a href="#t4177">4177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4178" class="pln"><span class="n"><a href="#t4178">4178</a></span><span class="t"><span class="str">Computes the orthogonal matrix `Q` of a QR factorization, from the `(input, input2)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4179" class="pln"><span class="n"><a href="#t4179">4179</a></span><span class="t"><span class="str">tuple returned by :func:`torch.geqrf`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4180" class="pln"><span class="n"><a href="#t4180">4180</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4181" class="pln"><span class="n"><a href="#t4181">4181</a></span><span class="t"><span class="str">This directly calls the underlying LAPACK function `?orgqr`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4182" class="pln"><span class="n"><a href="#t4182">4182</a></span><span class="t"><span class="str">See `LAPACK documentation for orgqr`_ for further details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4183" class="pln"><span class="n"><a href="#t4183">4183</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4184" class="pln"><span class="n"><a href="#t4184">4184</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4185" class="pln"><span class="n"><a href="#t4185">4185</a></span><span class="t"><span class="str">    input (Tensor): the `a` from :func:`torch.geqrf`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4186" class="pln"><span class="n"><a href="#t4186">4186</a></span><span class="t"><span class="str">    input2 (Tensor): the `tau` from :func:`torch.geqrf`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4187" class="pln"><span class="n"><a href="#t4187">4187</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4188" class="pln"><span class="n"><a href="#t4188">4188</a></span><span class="t"><span class="str">.. _LAPACK documentation for orgqr:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4189" class="pln"><span class="n"><a href="#t4189">4189</a></span><span class="t"><span class="str">    https://software.intel.com/en-us/mkl-developer-reference-c-orgqr</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4190" class="pln"><span class="n"><a href="#t4190">4190</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4191" class="pln"><span class="n"><a href="#t4191">4191</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4192" class="pln"><span class="n"><a href="#t4192">4192</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4193" class="run"><span class="n"><a href="#t4193">4193</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ormqr</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4194" class="pln"><span class="n"><a href="#t4194">4194</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4195" class="pln"><span class="n"><a href="#t4195">4195</a></span><span class="t"><span class="str">ormqr(input, input2, input3, left=True, transpose=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4196" class="pln"><span class="n"><a href="#t4196">4196</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4197" class="pln"><span class="n"><a href="#t4197">4197</a></span><span class="t"><span class="str">Multiplies `mat` (given by :attr:`input3`) by the orthogonal `Q` matrix of the QR factorization</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4198" class="pln"><span class="n"><a href="#t4198">4198</a></span><span class="t"><span class="str">formed by :func:`torch.geqrf` that is represented by `(a, tau)` (given by (:attr:`input`, :attr:`input2`)).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4199" class="pln"><span class="n"><a href="#t4199">4199</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4200" class="pln"><span class="n"><a href="#t4200">4200</a></span><span class="t"><span class="str">This directly calls the underlying LAPACK function `?ormqr`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4201" class="pln"><span class="n"><a href="#t4201">4201</a></span><span class="t"><span class="str">See `LAPACK documentation for ormqr`_ for further details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4202" class="pln"><span class="n"><a href="#t4202">4202</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4203" class="pln"><span class="n"><a href="#t4203">4203</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4204" class="pln"><span class="n"><a href="#t4204">4204</a></span><span class="t"><span class="str">    input (Tensor): the `a` from :func:`torch.geqrf`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4205" class="pln"><span class="n"><a href="#t4205">4205</a></span><span class="t"><span class="str">    input2 (Tensor): the `tau` from :func:`torch.geqrf`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4206" class="pln"><span class="n"><a href="#t4206">4206</a></span><span class="t"><span class="str">    input3 (Tensor): the matrix to be multiplied.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4207" class="pln"><span class="n"><a href="#t4207">4207</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4208" class="pln"><span class="n"><a href="#t4208">4208</a></span><span class="t"><span class="str">.. _LAPACK documentation for ormqr:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4209" class="pln"><span class="n"><a href="#t4209">4209</a></span><span class="t"><span class="str">    https://software.intel.com/en-us/mkl-developer-reference-c-ormqr</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4210" class="pln"><span class="n"><a href="#t4210">4210</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4211" class="pln"><span class="n"><a href="#t4211">4211</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4212" class="pln"><span class="n"><a href="#t4212">4212</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4213" class="run"><span class="n"><a href="#t4213">4213</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">poisson</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4214" class="pln"><span class="n"><a href="#t4214">4214</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4215" class="pln"><span class="n"><a href="#t4215">4215</a></span><span class="t"><span class="str">poisson(input *, generator=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4216" class="pln"><span class="n"><a href="#t4216">4216</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4217" class="pln"><span class="n"><a href="#t4217">4217</a></span><span class="t"><span class="str">Returns a tensor of the same size as :attr:`input` with each element</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4218" class="pln"><span class="n"><a href="#t4218">4218</a></span><span class="t"><span class="str">sampled from a Poisson distribution with rate parameter given by the corresponding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4219" class="pln"><span class="n"><a href="#t4219">4219</a></span><span class="t"><span class="str">element in :attr:`input` i.e.,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4220" class="pln"><span class="n"><a href="#t4220">4220</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4221" class="pln"><span class="n"><a href="#t4221">4221</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4222" class="pln"><span class="n"><a href="#t4222">4222</a></span><span class="t"><span class="str">    \text{{out}}_i \sim \text{{Poisson}}(\text{{input}}_i)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4223" class="pln"><span class="n"><a href="#t4223">4223</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4224" class="pln"><span class="n"><a href="#t4224">4224</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4225" class="pln"><span class="n"><a href="#t4225">4225</a></span><span class="t"><span class="str">    input (Tensor): the input tensor containing the rates of the Poisson distribution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4226" class="pln"><span class="n"><a href="#t4226">4226</a></span><span class="t"><span class="str">    {generator}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4227" class="pln"><span class="n"><a href="#t4227">4227</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4228" class="pln"><span class="n"><a href="#t4228">4228</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4229" class="pln"><span class="n"><a href="#t4229">4229</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4230" class="pln"><span class="n"><a href="#t4230">4230</a></span><span class="t"><span class="str">    >>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4231" class="pln"><span class="n"><a href="#t4231">4231</a></span><span class="t"><span class="str">    >>> torch.poisson(rates)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4232" class="pln"><span class="n"><a href="#t4232">4232</a></span><span class="t"><span class="str">    tensor([[9., 1., 3., 5.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4233" class="pln"><span class="n"><a href="#t4233">4233</a></span><span class="t"><span class="str">            [8., 6., 6., 0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4234" class="pln"><span class="n"><a href="#t4234">4234</a></span><span class="t"><span class="str">            [0., 4., 5., 3.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4235" class="pln"><span class="n"><a href="#t4235">4235</a></span><span class="t"><span class="str">            [2., 1., 4., 2.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4236" class="pln"><span class="n"><a href="#t4236">4236</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4237" class="pln"><span class="n"><a href="#t4237">4237</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4238" class="run"><span class="n"><a href="#t4238">4238</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">polygamma</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4239" class="pln"><span class="n"><a href="#t4239">4239</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4240" class="pln"><span class="n"><a href="#t4240">4240</a></span><span class="t"><span class="str">polygamma(n, input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4241" class="pln"><span class="n"><a href="#t4241">4241</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4242" class="pln"><span class="n"><a href="#t4242">4242</a></span><span class="t"><span class="str">Computes the :math:`n^{th}` derivative of the digamma function on :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4243" class="pln"><span class="n"><a href="#t4243">4243</a></span><span class="t"><span class="str">:math:`n \geq 0` is called the order of the polygamma function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4244" class="pln"><span class="n"><a href="#t4244">4244</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4245" class="pln"><span class="n"><a href="#t4245">4245</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4246" class="pln"><span class="n"><a href="#t4246">4246</a></span><span class="t"><span class="str">    \psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4247" class="pln"><span class="n"><a href="#t4247">4247</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4248" class="pln"><span class="n"><a href="#t4248">4248</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4249" class="pln"><span class="n"><a href="#t4249">4249</a></span><span class="t"><span class="str">    This function is not implemented for :math:`n \geq 2`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4250" class="pln"><span class="n"><a href="#t4250">4250</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4251" class="pln"><span class="n"><a href="#t4251">4251</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4252" class="pln"><span class="n"><a href="#t4252">4252</a></span><span class="t"><span class="str">    n (int): the order of the polygamma function</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4253" class="pln"><span class="n"><a href="#t4253">4253</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4254" class="pln"><span class="n"><a href="#t4254">4254</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4255" class="pln"><span class="n"><a href="#t4255">4255</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4256" class="pln"><span class="n"><a href="#t4256">4256</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4257" class="pln"><span class="n"><a href="#t4257">4257</a></span><span class="t"><span class="str">    >>> a = torch.tensor([1, 0.5])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4258" class="pln"><span class="n"><a href="#t4258">4258</a></span><span class="t"><span class="str">    >>> torch.polygamma(1, a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4259" class="pln"><span class="n"><a href="#t4259">4259</a></span><span class="t"><span class="str">    tensor([1.64493, 4.9348])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4260" class="pln"><span class="n"><a href="#t4260">4260</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4261" class="pln"><span class="n"><a href="#t4261">4261</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4262" class="run"><span class="n"><a href="#t4262">4262</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">pow</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4263" class="pln"><span class="n"><a href="#t4263">4263</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4264" class="pln"><span class="n"><a href="#t4264">4264</a></span><span class="t"><span class="str">.. function:: pow(input, exponent, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4265" class="pln"><span class="n"><a href="#t4265">4265</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4266" class="pln"><span class="n"><a href="#t4266">4266</a></span><span class="t"><span class="str">Takes the power of each element in :attr:`input` with :attr:`exponent` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4267" class="pln"><span class="n"><a href="#t4267">4267</a></span><span class="t"><span class="str">returns a tensor with the result.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4268" class="pln"><span class="n"><a href="#t4268">4268</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4269" class="pln"><span class="n"><a href="#t4269">4269</a></span><span class="t"><span class="str">:attr:`exponent` can be either a single ``float`` number or a `Tensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4270" class="pln"><span class="n"><a href="#t4270">4270</a></span><span class="t"><span class="str">with the same number of elements as :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4271" class="pln"><span class="n"><a href="#t4271">4271</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4272" class="pln"><span class="n"><a href="#t4272">4272</a></span><span class="t"><span class="str">When :attr:`exponent` is a scalar value, the operation applied is:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4273" class="pln"><span class="n"><a href="#t4273">4273</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4274" class="pln"><span class="n"><a href="#t4274">4274</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4275" class="pln"><span class="n"><a href="#t4275">4275</a></span><span class="t"><span class="str">    \text{out}_i = x_i ^ \text{exponent}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4276" class="pln"><span class="n"><a href="#t4276">4276</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4277" class="pln"><span class="n"><a href="#t4277">4277</a></span><span class="t"><span class="str">When :attr:`exponent` is a tensor, the operation applied is:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4278" class="pln"><span class="n"><a href="#t4278">4278</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4279" class="pln"><span class="n"><a href="#t4279">4279</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4280" class="pln"><span class="n"><a href="#t4280">4280</a></span><span class="t"><span class="str">    \text{out}_i = x_i ^ {\text{exponent}_i}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4281" class="pln"><span class="n"><a href="#t4281">4281</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4282" class="pln"><span class="n"><a href="#t4282">4282</a></span><span class="t"><span class="str">When :attr:`exponent` is a tensor, the shapes of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4283" class="pln"><span class="n"><a href="#t4283">4283</a></span><span class="t"><span class="str">and :attr:`exponent` must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4284" class="pln"><span class="n"><a href="#t4284">4284</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4285" class="pln"><span class="n"><a href="#t4285">4285</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4286" class="pln"><span class="n"><a href="#t4286">4286</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4287" class="pln"><span class="n"><a href="#t4287">4287</a></span><span class="t"><span class="str">    exponent (float or tensor): the exponent value</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4288" class="pln"><span class="n"><a href="#t4288">4288</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4289" class="pln"><span class="n"><a href="#t4289">4289</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4290" class="pln"><span class="n"><a href="#t4290">4290</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4291" class="pln"><span class="n"><a href="#t4291">4291</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4292" class="pln"><span class="n"><a href="#t4292">4292</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4293" class="pln"><span class="n"><a href="#t4293">4293</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4294" class="pln"><span class="n"><a href="#t4294">4294</a></span><span class="t"><span class="str">    tensor([ 0.4331,  1.2475,  0.6834, -0.2791])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4295" class="pln"><span class="n"><a href="#t4295">4295</a></span><span class="t"><span class="str">    >>> torch.pow(a, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4296" class="pln"><span class="n"><a href="#t4296">4296</a></span><span class="t"><span class="str">    tensor([ 0.1875,  1.5561,  0.4670,  0.0779])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4297" class="pln"><span class="n"><a href="#t4297">4297</a></span><span class="t"><span class="str">    >>> exp = torch.arange(1., 5.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4298" class="pln"><span class="n"><a href="#t4298">4298</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4299" class="pln"><span class="n"><a href="#t4299">4299</a></span><span class="t"><span class="str">    >>> a = torch.arange(1., 5.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4300" class="pln"><span class="n"><a href="#t4300">4300</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4301" class="pln"><span class="n"><a href="#t4301">4301</a></span><span class="t"><span class="str">    tensor([ 1.,  2.,  3.,  4.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4302" class="pln"><span class="n"><a href="#t4302">4302</a></span><span class="t"><span class="str">    >>> exp</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4303" class="pln"><span class="n"><a href="#t4303">4303</a></span><span class="t"><span class="str">    tensor([ 1.,  2.,  3.,  4.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4304" class="pln"><span class="n"><a href="#t4304">4304</a></span><span class="t"><span class="str">    >>> torch.pow(a, exp)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4305" class="pln"><span class="n"><a href="#t4305">4305</a></span><span class="t"><span class="str">    tensor([   1.,    4.,   27.,  256.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4306" class="pln"><span class="n"><a href="#t4306">4306</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4307" class="pln"><span class="n"><a href="#t4307">4307</a></span><span class="t"><span class="str">.. function:: pow(self, exponent, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4308" class="pln"><span class="n"><a href="#t4308">4308</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4309" class="pln"><span class="n"><a href="#t4309">4309</a></span><span class="t"><span class="str">:attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4310" class="pln"><span class="n"><a href="#t4310">4310</a></span><span class="t"><span class="str">The returned tensor :attr:`out` is of the same shape as :attr:`exponent`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4311" class="pln"><span class="n"><a href="#t4311">4311</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4312" class="pln"><span class="n"><a href="#t4312">4312</a></span><span class="t"><span class="str">The operation applied is:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4313" class="pln"><span class="n"><a href="#t4313">4313</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4314" class="pln"><span class="n"><a href="#t4314">4314</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4315" class="pln"><span class="n"><a href="#t4315">4315</a></span><span class="t"><span class="str">    \text{{out}}_i = \text{{self}} ^ {{\text{{exponent}}_i}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4316" class="pln"><span class="n"><a href="#t4316">4316</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4317" class="pln"><span class="n"><a href="#t4317">4317</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4318" class="pln"><span class="n"><a href="#t4318">4318</a></span><span class="t"><span class="str">    self (float): the scalar base value for the power operation</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4319" class="pln"><span class="n"><a href="#t4319">4319</a></span><span class="t"><span class="str">    exponent (Tensor): the exponent tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4320" class="pln"><span class="n"><a href="#t4320">4320</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4321" class="pln"><span class="n"><a href="#t4321">4321</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4322" class="pln"><span class="n"><a href="#t4322">4322</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4323" class="pln"><span class="n"><a href="#t4323">4323</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4324" class="pln"><span class="n"><a href="#t4324">4324</a></span><span class="t"><span class="str">    >>> exp = torch.arange(1., 5.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4325" class="pln"><span class="n"><a href="#t4325">4325</a></span><span class="t"><span class="str">    >>> base = 2</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4326" class="pln"><span class="n"><a href="#t4326">4326</a></span><span class="t"><span class="str">    >>> torch.pow(base, exp)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4327" class="pln"><span class="n"><a href="#t4327">4327</a></span><span class="t"><span class="str">    tensor([  2.,   4.,   8.,  16.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4328" class="pln"><span class="n"><a href="#t4328">4328</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4329" class="pln"><span class="n"><a href="#t4329">4329</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4330" class="run"><span class="n"><a href="#t4330">4330</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">prod</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4331" class="pln"><span class="n"><a href="#t4331">4331</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4332" class="pln"><span class="n"><a href="#t4332">4332</a></span><span class="t"><span class="str">.. function:: prod(input, dtype=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4333" class="pln"><span class="n"><a href="#t4333">4333</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4334" class="pln"><span class="n"><a href="#t4334">4334</a></span><span class="t"><span class="str">Returns the product of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4335" class="pln"><span class="n"><a href="#t4335">4335</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4336" class="pln"><span class="n"><a href="#t4336">4336</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4337" class="pln"><span class="n"><a href="#t4337">4337</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4338" class="pln"><span class="n"><a href="#t4338">4338</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4339" class="pln"><span class="n"><a href="#t4339">4339</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4340" class="pln"><span class="n"><a href="#t4340">4340</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4341" class="pln"><span class="n"><a href="#t4341">4341</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4342" class="pln"><span class="n"><a href="#t4342">4342</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4343" class="pln"><span class="n"><a href="#t4343">4343</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4344" class="pln"><span class="n"><a href="#t4344">4344</a></span><span class="t"><span class="str">    tensor([[-0.8020,  0.5428, -1.5854]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4345" class="pln"><span class="n"><a href="#t4345">4345</a></span><span class="t"><span class="str">    >>> torch.prod(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4346" class="pln"><span class="n"><a href="#t4346">4346</a></span><span class="t"><span class="str">    tensor(0.6902)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4347" class="pln"><span class="n"><a href="#t4347">4347</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4348" class="pln"><span class="n"><a href="#t4348">4348</a></span><span class="t"><span class="str">.. function:: prod(input, dim, keepdim=False, dtype=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4349" class="pln"><span class="n"><a href="#t4349">4349</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4350" class="pln"><span class="n"><a href="#t4350">4350</a></span><span class="t"><span class="str">Returns the product of each row of the :attr:`input` tensor in the given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4351" class="pln"><span class="n"><a href="#t4351">4351</a></span><span class="t"><span class="str">dimension :attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4352" class="pln"><span class="n"><a href="#t4352">4352</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4353" class="pln"><span class="n"><a href="#t4353">4353</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4354" class="pln"><span class="n"><a href="#t4354">4354</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4355" class="pln"><span class="n"><a href="#t4355">4355</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4356" class="pln"><span class="n"><a href="#t4356">4356</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4357" class="pln"><span class="n"><a href="#t4357">4357</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4358" class="pln"><span class="n"><a href="#t4358">4358</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4359" class="pln"><span class="n"><a href="#t4359">4359</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4360" class="pln"><span class="n"><a href="#t4360">4360</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4361" class="pln"><span class="n"><a href="#t4361">4361</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4362" class="pln"><span class="n"><a href="#t4362">4362</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4363" class="pln"><span class="n"><a href="#t4363">4363</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4364" class="pln"><span class="n"><a href="#t4364">4364</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4365" class="pln"><span class="n"><a href="#t4365">4365</a></span><span class="t"><span class="str">    tensor([[ 0.5261, -0.3837],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4366" class="pln"><span class="n"><a href="#t4366">4366</a></span><span class="t"><span class="str">            [ 1.1857, -0.2498],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4367" class="pln"><span class="n"><a href="#t4367">4367</a></span><span class="t"><span class="str">            [-1.1646,  0.0705],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4368" class="pln"><span class="n"><a href="#t4368">4368</a></span><span class="t"><span class="str">            [ 1.1131, -1.0629]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4369" class="pln"><span class="n"><a href="#t4369">4369</a></span><span class="t"><span class="str">    >>> torch.prod(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4370" class="pln"><span class="n"><a href="#t4370">4370</a></span><span class="t"><span class="str">    tensor([-0.2018, -0.2962, -0.0821, -1.1831])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4371" class="pln"><span class="n"><a href="#t4371">4371</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">single_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4372" class="pln"><span class="n"><a href="#t4372">4372</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4373" class="run"><span class="n"><a href="#t4373">4373</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">promote_types</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4374" class="pln"><span class="n"><a href="#t4374">4374</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4375" class="pln"><span class="n"><a href="#t4375">4375</a></span><span class="t"><span class="str">promote_types(type1, type2) -> dtype</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4376" class="pln"><span class="n"><a href="#t4376">4376</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4377" class="pln"><span class="n"><a href="#t4377">4377</a></span><span class="t"><span class="str">Returns the :class:`torch.dtype` with the smallest size and scalar kind that is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4378" class="pln"><span class="n"><a href="#t4378">4378</a></span><span class="t"><span class="str">not smaller nor of lower kind than either `type1` or `type2`. See type promotion</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4379" class="pln"><span class="n"><a href="#t4379">4379</a></span><span class="t"><span class="str">:ref:`documentation &lt;type-promotion-doc>` for more information on the type</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4380" class="pln"><span class="n"><a href="#t4380">4380</a></span><span class="t"><span class="str">promotion logic.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4381" class="pln"><span class="n"><a href="#t4381">4381</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4382" class="pln"><span class="n"><a href="#t4382">4382</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4383" class="pln"><span class="n"><a href="#t4383">4383</a></span><span class="t"><span class="str">    type1 (:class:`torch.dtype`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4384" class="pln"><span class="n"><a href="#t4384">4384</a></span><span class="t"><span class="str">    type2 (:class:`torch.dtype`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4385" class="pln"><span class="n"><a href="#t4385">4385</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4386" class="pln"><span class="n"><a href="#t4386">4386</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4387" class="pln"><span class="n"><a href="#t4387">4387</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4388" class="pln"><span class="n"><a href="#t4388">4388</a></span><span class="t"><span class="str">    >>> torch.promote_types(torch.int32, torch.float32))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4389" class="pln"><span class="n"><a href="#t4389">4389</a></span><span class="t"><span class="str">    torch.float32</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4390" class="pln"><span class="n"><a href="#t4390">4390</a></span><span class="t"><span class="str">    >>> torch.promote_types(torch.uint8, torch.long)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4391" class="pln"><span class="n"><a href="#t4391">4391</a></span><span class="t"><span class="str">    torch.long</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4392" class="pln"><span class="n"><a href="#t4392">4392</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4393" class="pln"><span class="n"><a href="#t4393">4393</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4394" class="run"><span class="n"><a href="#t4394">4394</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">qr</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4395" class="pln"><span class="n"><a href="#t4395">4395</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4396" class="pln"><span class="n"><a href="#t4396">4396</a></span><span class="t"><span class="str">qr(input, some=True, out=None) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4397" class="pln"><span class="n"><a href="#t4397">4397</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4398" class="pln"><span class="n"><a href="#t4398">4398</a></span><span class="t"><span class="str">Computes the QR decomposition of a matrix or a batch of matrices :attr:`input`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4399" class="pln"><span class="n"><a href="#t4399">4399</a></span><span class="t"><span class="str">and returns a namedtuple (Q, R) of tensors such that :math:`\text{input} = Q R`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4400" class="pln"><span class="n"><a href="#t4400">4400</a></span><span class="t"><span class="str">with :math:`Q` being an orthogonal matrix or batch of orthogonal matrices and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4401" class="pln"><span class="n"><a href="#t4401">4401</a></span><span class="t"><span class="str">:math:`R` being an upper triangular matrix or batch of upper triangular matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4402" class="pln"><span class="n"><a href="#t4402">4402</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4403" class="pln"><span class="n"><a href="#t4403">4403</a></span><span class="t"><span class="str">If :attr:`some` is ``True``, then this function returns the thin (reduced) QR factorization.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4404" class="pln"><span class="n"><a href="#t4404">4404</a></span><span class="t"><span class="str">Otherwise, if :attr:`some` is ``False``, this function returns the complete QR factorization.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4405" class="pln"><span class="n"><a href="#t4405">4405</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4406" class="pln"><span class="n"><a href="#t4406">4406</a></span><span class="t"><span class="str">.. note:: precision may be lost if the magnitudes of the elements of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4407" class="pln"><span class="n"><a href="#t4407">4407</a></span><span class="t"><span class="str">          are large</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4408" class="pln"><span class="n"><a href="#t4408">4408</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4409" class="pln"><span class="n"><a href="#t4409">4409</a></span><span class="t"><span class="str">.. note:: While it should always give you a valid decomposition, it may not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4410" class="pln"><span class="n"><a href="#t4410">4410</a></span><span class="t"><span class="str">          give you the same one across platforms - it will depend on your</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4411" class="pln"><span class="n"><a href="#t4411">4411</a></span><span class="t"><span class="str">          LAPACK implementation.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4412" class="pln"><span class="n"><a href="#t4412">4412</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4413" class="pln"><span class="n"><a href="#t4413">4413</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4414" class="pln"><span class="n"><a href="#t4414">4414</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size :math:`(*, m, n)` where `*` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4415" class="pln"><span class="n"><a href="#t4415">4415</a></span><span class="t"><span class="str">                batch dimensions consisting of matrices of dimension :math:`m \times n`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4416" class="pln"><span class="n"><a href="#t4416">4416</a></span><span class="t"><span class="str">    some (bool, optional): Set to ``True`` for reduced QR decomposition and ``False`` for</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4417" class="pln"><span class="n"><a href="#t4417">4417</a></span><span class="t"><span class="str">                complete QR decomposition.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4418" class="pln"><span class="n"><a href="#t4418">4418</a></span><span class="t"><span class="str">    out (tuple, optional): tuple of `Q` and `R` tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4419" class="pln"><span class="n"><a href="#t4419">4419</a></span><span class="t"><span class="str">                satisfying :code:`input = torch.matmul(Q, R)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4420" class="pln"><span class="n"><a href="#t4420">4420</a></span><span class="t"><span class="str">                The dimensions of `Q` and `R` are :math:`(*, m, k)` and :math:`(*, k, n)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4421" class="pln"><span class="n"><a href="#t4421">4421</a></span><span class="t"><span class="str">                respectively, where :math:`k = \min(m, n)` if :attr:`some:` is ``True`` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4422" class="pln"><span class="n"><a href="#t4422">4422</a></span><span class="t"><span class="str">                :math:`k = m` otherwise.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4423" class="pln"><span class="n"><a href="#t4423">4423</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4424" class="pln"><span class="n"><a href="#t4424">4424</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4425" class="pln"><span class="n"><a href="#t4425">4425</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4426" class="pln"><span class="n"><a href="#t4426">4426</a></span><span class="t"><span class="str">    >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4427" class="pln"><span class="n"><a href="#t4427">4427</a></span><span class="t"><span class="str">    >>> q, r = torch.qr(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4428" class="pln"><span class="n"><a href="#t4428">4428</a></span><span class="t"><span class="str">    >>> q</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4429" class="pln"><span class="n"><a href="#t4429">4429</a></span><span class="t"><span class="str">    tensor([[-0.8571,  0.3943,  0.3314],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4430" class="pln"><span class="n"><a href="#t4430">4430</a></span><span class="t"><span class="str">            [-0.4286, -0.9029, -0.0343],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4431" class="pln"><span class="n"><a href="#t4431">4431</a></span><span class="t"><span class="str">            [ 0.2857, -0.1714,  0.9429]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4432" class="pln"><span class="n"><a href="#t4432">4432</a></span><span class="t"><span class="str">    >>> r</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4433" class="pln"><span class="n"><a href="#t4433">4433</a></span><span class="t"><span class="str">    tensor([[ -14.0000,  -21.0000,   14.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4434" class="pln"><span class="n"><a href="#t4434">4434</a></span><span class="t"><span class="str">            [   0.0000, -175.0000,   70.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4435" class="pln"><span class="n"><a href="#t4435">4435</a></span><span class="t"><span class="str">            [   0.0000,    0.0000,  -35.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4436" class="pln"><span class="n"><a href="#t4436">4436</a></span><span class="t"><span class="str">    >>> torch.mm(q, r).round()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4437" class="pln"><span class="n"><a href="#t4437">4437</a></span><span class="t"><span class="str">    tensor([[  12.,  -51.,    4.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4438" class="pln"><span class="n"><a href="#t4438">4438</a></span><span class="t"><span class="str">            [   6.,  167.,  -68.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4439" class="pln"><span class="n"><a href="#t4439">4439</a></span><span class="t"><span class="str">            [  -4.,   24.,  -41.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4440" class="pln"><span class="n"><a href="#t4440">4440</a></span><span class="t"><span class="str">    >>> torch.mm(q.t(), q).round()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4441" class="pln"><span class="n"><a href="#t4441">4441</a></span><span class="t"><span class="str">    tensor([[ 1.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4442" class="pln"><span class="n"><a href="#t4442">4442</a></span><span class="t"><span class="str">            [ 0.,  1., -0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4443" class="pln"><span class="n"><a href="#t4443">4443</a></span><span class="t"><span class="str">            [ 0., -0.,  1.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4444" class="pln"><span class="n"><a href="#t4444">4444</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4445" class="pln"><span class="n"><a href="#t4445">4445</a></span><span class="t"><span class="str">    >>> q, r = torch.qr(a, some=False)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4446" class="pln"><span class="n"><a href="#t4446">4446</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.matmul(q, r), a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4447" class="pln"><span class="n"><a href="#t4447">4447</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4448" class="pln"><span class="n"><a href="#t4448">4448</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4449" class="pln"><span class="n"><a href="#t4449">4449</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4450" class="pln"><span class="n"><a href="#t4450">4450</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4451" class="pln"><span class="n"><a href="#t4451">4451</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4452" class="run"><span class="n"><a href="#t4452">4452</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">rand</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4453" class="pln"><span class="n"><a href="#t4453">4453</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4454" class="pln"><span class="n"><a href="#t4454">4454</a></span><span class="t"><span class="str">rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4455" class="pln"><span class="n"><a href="#t4455">4455</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4456" class="pln"><span class="n"><a href="#t4456">4456</a></span><span class="t"><span class="str">Returns a tensor filled with random numbers from a uniform distribution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4457" class="pln"><span class="n"><a href="#t4457">4457</a></span><span class="t"><span class="str">on the interval :math:`[0, 1)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4458" class="pln"><span class="n"><a href="#t4458">4458</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4459" class="pln"><span class="n"><a href="#t4459">4459</a></span><span class="t"><span class="str">The shape of the tensor is defined by the variable argument :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4460" class="pln"><span class="n"><a href="#t4460">4460</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4461" class="pln"><span class="n"><a href="#t4461">4461</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4462" class="pln"><span class="n"><a href="#t4462">4462</a></span><span class="t"><span class="str">    size (int...): a sequence of integers defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4463" class="pln"><span class="n"><a href="#t4463">4463</a></span><span class="t"><span class="str">        Can be a variable number of arguments or a collection like a list or tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4464" class="pln"><span class="n"><a href="#t4464">4464</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4465" class="pln"><span class="n"><a href="#t4465">4465</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4466" class="pln"><span class="n"><a href="#t4466">4466</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4467" class="pln"><span class="n"><a href="#t4467">4467</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4468" class="pln"><span class="n"><a href="#t4468">4468</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4469" class="pln"><span class="n"><a href="#t4469">4469</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4470" class="pln"><span class="n"><a href="#t4470">4470</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4471" class="pln"><span class="n"><a href="#t4471">4471</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4472" class="pln"><span class="n"><a href="#t4472">4472</a></span><span class="t"><span class="str">    >>> torch.rand(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4473" class="pln"><span class="n"><a href="#t4473">4473</a></span><span class="t"><span class="str">    tensor([ 0.5204,  0.2503,  0.3525,  0.5673])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4474" class="pln"><span class="n"><a href="#t4474">4474</a></span><span class="t"><span class="str">    >>> torch.rand(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4475" class="pln"><span class="n"><a href="#t4475">4475</a></span><span class="t"><span class="str">    tensor([[ 0.8237,  0.5781,  0.6879],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4476" class="pln"><span class="n"><a href="#t4476">4476</a></span><span class="t"><span class="str">            [ 0.3816,  0.7249,  0.0998]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4477" class="pln"><span class="n"><a href="#t4477">4477</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4478" class="pln"><span class="n"><a href="#t4478">4478</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4479" class="run"><span class="n"><a href="#t4479">4479</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">rand_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4480" class="pln"><span class="n"><a href="#t4480">4480</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4481" class="pln"><span class="n"><a href="#t4481">4481</a></span><span class="t"><span class="str">rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4482" class="pln"><span class="n"><a href="#t4482">4482</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4483" class="pln"><span class="n"><a href="#t4483">4483</a></span><span class="t"><span class="str">Returns a tensor with the same size as :attr:`input` that is filled with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4484" class="pln"><span class="n"><a href="#t4484">4484</a></span><span class="t"><span class="str">random numbers from a uniform distribution on the interval :math:`[0, 1)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4485" class="pln"><span class="n"><a href="#t4485">4485</a></span><span class="t"><span class="str">``torch.rand_like(input)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4486" class="pln"><span class="n"><a href="#t4486">4486</a></span><span class="t"><span class="str">``torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4487" class="pln"><span class="n"><a href="#t4487">4487</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4488" class="pln"><span class="n"><a href="#t4488">4488</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4489" class="pln"><span class="n"><a href="#t4489">4489</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4490" class="pln"><span class="n"><a href="#t4490">4490</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4491" class="pln"><span class="n"><a href="#t4491">4491</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4492" class="pln"><span class="n"><a href="#t4492">4492</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4493" class="pln"><span class="n"><a href="#t4493">4493</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4494" class="pln"><span class="n"><a href="#t4494">4494</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4495" class="pln"><span class="n"><a href="#t4495">4495</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4496" class="pln"><span class="n"><a href="#t4496">4496</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4497" class="run"><span class="n"><a href="#t4497">4497</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">randint</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4498" class="pln"><span class="n"><a href="#t4498">4498</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4499" class="pln"><span class="n"><a href="#t4499">4499</a></span><span class="t"><span class="str">randint(low=0, high, size, *, generator=None, out=None, \</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4500" class="pln"><span class="n"><a href="#t4500">4500</a></span><span class="t"><span class="str">        dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4501" class="pln"><span class="n"><a href="#t4501">4501</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4502" class="pln"><span class="n"><a href="#t4502">4502</a></span><span class="t"><span class="str">Returns a tensor filled with random integers generated uniformly</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4503" class="pln"><span class="n"><a href="#t4503">4503</a></span><span class="t"><span class="str">between :attr:`low` (inclusive) and :attr:`high` (exclusive).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4504" class="pln"><span class="n"><a href="#t4504">4504</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4505" class="pln"><span class="n"><a href="#t4505">4505</a></span><span class="t"><span class="str">The shape of the tensor is defined by the variable argument :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4506" class="pln"><span class="n"><a href="#t4506">4506</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4507" class="pln"><span class="n"><a href="#t4507">4507</a></span><span class="t"><span class="str">.. note:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4508" class="pln"><span class="n"><a href="#t4508">4508</a></span><span class="t"><span class="str">    With the global dtype default (``torch.float32``), this function returns</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4509" class="pln"><span class="n"><a href="#t4509">4509</a></span><span class="t"><span class="str">    a tensor with dtype ``torch.int64``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4510" class="pln"><span class="n"><a href="#t4510">4510</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4511" class="pln"><span class="n"><a href="#t4511">4511</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4512" class="pln"><span class="n"><a href="#t4512">4512</a></span><span class="t"><span class="str">    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4513" class="pln"><span class="n"><a href="#t4513">4513</a></span><span class="t"><span class="str">    high (int): One above the highest integer to be drawn from the distribution.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4514" class="pln"><span class="n"><a href="#t4514">4514</a></span><span class="t"><span class="str">    size (tuple): a tuple defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4515" class="pln"><span class="n"><a href="#t4515">4515</a></span><span class="t"><span class="str">    {generator}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4516" class="pln"><span class="n"><a href="#t4516">4516</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4517" class="pln"><span class="n"><a href="#t4517">4517</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4518" class="pln"><span class="n"><a href="#t4518">4518</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4519" class="pln"><span class="n"><a href="#t4519">4519</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4520" class="pln"><span class="n"><a href="#t4520">4520</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4521" class="pln"><span class="n"><a href="#t4521">4521</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4522" class="pln"><span class="n"><a href="#t4522">4522</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4523" class="pln"><span class="n"><a href="#t4523">4523</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4524" class="pln"><span class="n"><a href="#t4524">4524</a></span><span class="t"><span class="str">    >>> torch.randint(3, 5, (3,))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4525" class="pln"><span class="n"><a href="#t4525">4525</a></span><span class="t"><span class="str">    tensor([4, 3, 4])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4526" class="pln"><span class="n"><a href="#t4526">4526</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4527" class="pln"><span class="n"><a href="#t4527">4527</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4528" class="pln"><span class="n"><a href="#t4528">4528</a></span><span class="t"><span class="str">    >>> torch.randint(10, (2, 2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4529" class="pln"><span class="n"><a href="#t4529">4529</a></span><span class="t"><span class="str">    tensor([[0, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4530" class="pln"><span class="n"><a href="#t4530">4530</a></span><span class="t"><span class="str">            [5, 5]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4531" class="pln"><span class="n"><a href="#t4531">4531</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4532" class="pln"><span class="n"><a href="#t4532">4532</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4533" class="pln"><span class="n"><a href="#t4533">4533</a></span><span class="t"><span class="str">    >>> torch.randint(3, 10, (2, 2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4534" class="pln"><span class="n"><a href="#t4534">4534</a></span><span class="t"><span class="str">    tensor([[4, 5],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4535" class="pln"><span class="n"><a href="#t4535">4535</a></span><span class="t"><span class="str">            [6, 7]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4536" class="pln"><span class="n"><a href="#t4536">4536</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4537" class="pln"><span class="n"><a href="#t4537">4537</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4538" class="pln"><span class="n"><a href="#t4538">4538</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4539" class="pln"><span class="n"><a href="#t4539">4539</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4540" class="run"><span class="n"><a href="#t4540">4540</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">randint_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4541" class="pln"><span class="n"><a href="#t4541">4541</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4542" class="pln"><span class="n"><a href="#t4542">4542</a></span><span class="t"><span class="str">randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4543" class="pln"><span class="n"><a href="#t4543">4543</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4544" class="pln"><span class="n"><a href="#t4544">4544</a></span><span class="t"><span class="str">Returns a tensor with the same shape as Tensor :attr:`input` filled with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4545" class="pln"><span class="n"><a href="#t4545">4545</a></span><span class="t"><span class="str">random integers generated uniformly between :attr:`low` (inclusive) and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4546" class="pln"><span class="n"><a href="#t4546">4546</a></span><span class="t"><span class="str">:attr:`high` (exclusive).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4547" class="pln"><span class="n"><a href="#t4547">4547</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4548" class="pln"><span class="n"><a href="#t4548">4548</a></span><span class="t"><span class="str">.. note:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4549" class="pln"><span class="n"><a href="#t4549">4549</a></span><span class="t"><span class="str">    With the global dtype default (``torch.float32``), this function returns</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4550" class="pln"><span class="n"><a href="#t4550">4550</a></span><span class="t"><span class="str">    a tensor with dtype ``torch.int64``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4551" class="pln"><span class="n"><a href="#t4551">4551</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4552" class="pln"><span class="n"><a href="#t4552">4552</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4553" class="pln"><span class="n"><a href="#t4553">4553</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4554" class="pln"><span class="n"><a href="#t4554">4554</a></span><span class="t"><span class="str">    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4555" class="pln"><span class="n"><a href="#t4555">4555</a></span><span class="t"><span class="str">    high (int): One above the highest integer to be drawn from the distribution.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4556" class="pln"><span class="n"><a href="#t4556">4556</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4557" class="pln"><span class="n"><a href="#t4557">4557</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4558" class="pln"><span class="n"><a href="#t4558">4558</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4559" class="pln"><span class="n"><a href="#t4559">4559</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4560" class="pln"><span class="n"><a href="#t4560">4560</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4561" class="pln"><span class="n"><a href="#t4561">4561</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4562" class="pln"><span class="n"><a href="#t4562">4562</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4563" class="run"><span class="n"><a href="#t4563">4563</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">randn</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4564" class="pln"><span class="n"><a href="#t4564">4564</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4565" class="pln"><span class="n"><a href="#t4565">4565</a></span><span class="t"><span class="str">randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4566" class="pln"><span class="n"><a href="#t4566">4566</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4567" class="pln"><span class="n"><a href="#t4567">4567</a></span><span class="t"><span class="str">Returns a tensor filled with random numbers from a normal distribution</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4568" class="pln"><span class="n"><a href="#t4568">4568</a></span><span class="t"><span class="str">with mean `0` and variance `1` (also called the standard normal</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4569" class="pln"><span class="n"><a href="#t4569">4569</a></span><span class="t"><span class="str">distribution).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4570" class="pln"><span class="n"><a href="#t4570">4570</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4571" class="pln"><span class="n"><a href="#t4571">4571</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4572" class="pln"><span class="n"><a href="#t4572">4572</a></span><span class="t"><span class="str">    \text{{out}}_{{i}} \sim \mathcal{{N}}(0, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4573" class="pln"><span class="n"><a href="#t4573">4573</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4574" class="pln"><span class="n"><a href="#t4574">4574</a></span><span class="t"><span class="str">The shape of the tensor is defined by the variable argument :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4575" class="pln"><span class="n"><a href="#t4575">4575</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4576" class="pln"><span class="n"><a href="#t4576">4576</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4577" class="pln"><span class="n"><a href="#t4577">4577</a></span><span class="t"><span class="str">    size (int...): a sequence of integers defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4578" class="pln"><span class="n"><a href="#t4578">4578</a></span><span class="t"><span class="str">        Can be a variable number of arguments or a collection like a list or tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4579" class="pln"><span class="n"><a href="#t4579">4579</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4580" class="pln"><span class="n"><a href="#t4580">4580</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4581" class="pln"><span class="n"><a href="#t4581">4581</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4582" class="pln"><span class="n"><a href="#t4582">4582</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4583" class="pln"><span class="n"><a href="#t4583">4583</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4584" class="pln"><span class="n"><a href="#t4584">4584</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4585" class="pln"><span class="n"><a href="#t4585">4585</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4586" class="pln"><span class="n"><a href="#t4586">4586</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4587" class="pln"><span class="n"><a href="#t4587">4587</a></span><span class="t"><span class="str">    >>> torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4588" class="pln"><span class="n"><a href="#t4588">4588</a></span><span class="t"><span class="str">    tensor([-2.1436,  0.9966,  2.3426, -0.6366])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4589" class="pln"><span class="n"><a href="#t4589">4589</a></span><span class="t"><span class="str">    >>> torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4590" class="pln"><span class="n"><a href="#t4590">4590</a></span><span class="t"><span class="str">    tensor([[ 1.5954,  2.8929, -1.0923],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4591" class="pln"><span class="n"><a href="#t4591">4591</a></span><span class="t"><span class="str">            [ 1.1719, -0.4709, -0.1996]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4592" class="pln"><span class="n"><a href="#t4592">4592</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4593" class="pln"><span class="n"><a href="#t4593">4593</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4594" class="run"><span class="n"><a href="#t4594">4594</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">randn_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4595" class="pln"><span class="n"><a href="#t4595">4595</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4596" class="pln"><span class="n"><a href="#t4596">4596</a></span><span class="t"><span class="str">randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4597" class="pln"><span class="n"><a href="#t4597">4597</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4598" class="pln"><span class="n"><a href="#t4598">4598</a></span><span class="t"><span class="str">Returns a tensor with the same size as :attr:`input` that is filled with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4599" class="pln"><span class="n"><a href="#t4599">4599</a></span><span class="t"><span class="str">random numbers from a normal distribution with mean 0 and variance 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4600" class="pln"><span class="n"><a href="#t4600">4600</a></span><span class="t"><span class="str">``torch.randn_like(input)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4601" class="pln"><span class="n"><a href="#t4601">4601</a></span><span class="t"><span class="str">``torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4602" class="pln"><span class="n"><a href="#t4602">4602</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4603" class="pln"><span class="n"><a href="#t4603">4603</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4604" class="pln"><span class="n"><a href="#t4604">4604</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4605" class="pln"><span class="n"><a href="#t4605">4605</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4606" class="pln"><span class="n"><a href="#t4606">4606</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4607" class="pln"><span class="n"><a href="#t4607">4607</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4608" class="pln"><span class="n"><a href="#t4608">4608</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4609" class="pln"><span class="n"><a href="#t4609">4609</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4610" class="pln"><span class="n"><a href="#t4610">4610</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4611" class="pln"><span class="n"><a href="#t4611">4611</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4612" class="run"><span class="n"><a href="#t4612">4612</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">randperm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4613" class="pln"><span class="n"><a href="#t4613">4613</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4614" class="pln"><span class="n"><a href="#t4614">4614</a></span><span class="t"><span class="str">randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4615" class="pln"><span class="n"><a href="#t4615">4615</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4616" class="pln"><span class="n"><a href="#t4616">4616</a></span><span class="t"><span class="str">Returns a random permutation of integers from ``0`` to ``n - 1``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4617" class="pln"><span class="n"><a href="#t4617">4617</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4618" class="pln"><span class="n"><a href="#t4618">4618</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4619" class="pln"><span class="n"><a href="#t4619">4619</a></span><span class="t"><span class="str">    n (int): the upper bound (exclusive)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4620" class="pln"><span class="n"><a href="#t4620">4620</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4621" class="pln"><span class="n"><a href="#t4621">4621</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4622" class="pln"><span class="n"><a href="#t4622">4622</a></span><span class="t"><span class="str">        Default: ``torch.int64``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4623" class="pln"><span class="n"><a href="#t4623">4623</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4624" class="pln"><span class="n"><a href="#t4624">4624</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4625" class="pln"><span class="n"><a href="#t4625">4625</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4626" class="pln"><span class="n"><a href="#t4626">4626</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4627" class="pln"><span class="n"><a href="#t4627">4627</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4628" class="pln"><span class="n"><a href="#t4628">4628</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4629" class="pln"><span class="n"><a href="#t4629">4629</a></span><span class="t"><span class="str">    >>> torch.randperm(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4630" class="pln"><span class="n"><a href="#t4630">4630</a></span><span class="t"><span class="str">    tensor([2, 1, 0, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4631" class="pln"><span class="n"><a href="#t4631">4631</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4632" class="pln"><span class="n"><a href="#t4632">4632</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4633" class="run"><span class="n"><a href="#t4633">4633</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4634" class="pln"><span class="n"><a href="#t4634">4634</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4635" class="pln"><span class="n"><a href="#t4635">4635</a></span><span class="t"><span class="str">tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4636" class="pln"><span class="n"><a href="#t4636">4636</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4637" class="pln"><span class="n"><a href="#t4637">4637</a></span><span class="t"><span class="str">Constructs a tensor with :attr:`data`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4638" class="pln"><span class="n"><a href="#t4638">4638</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4639" class="pln"><span class="n"><a href="#t4639">4639</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4640" class="pln"><span class="n"><a href="#t4640">4640</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4641" class="pln"><span class="n"><a href="#t4641">4641</a></span><span class="t"><span class="str">    :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4642" class="pln"><span class="n"><a href="#t4642">4642</a></span><span class="t"><span class="str">    ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4643" class="pln"><span class="n"><a href="#t4643">4643</a></span><span class="t"><span class="str">    or :func:`torch.Tensor.detach`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4644" class="pln"><span class="n"><a href="#t4644">4644</a></span><span class="t"><span class="str">    If you have a NumPy ``ndarray`` and want to avoid a copy, use</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4645" class="pln"><span class="n"><a href="#t4645">4645</a></span><span class="t"><span class="str">    :func:`torch.as_tensor`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4646" class="pln"><span class="n"><a href="#t4646">4646</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4647" class="pln"><span class="n"><a href="#t4647">4647</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4648" class="pln"><span class="n"><a href="#t4648">4648</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4649" class="pln"><span class="n"><a href="#t4649">4649</a></span><span class="t"><span class="str">    When data is a tensor `x`, :func:`torch.tensor` reads out 'the data' from whatever it is passed,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4650" class="pln"><span class="n"><a href="#t4650">4650</a></span><span class="t"><span class="str">    and constructs a leaf variable. Therefore ``torch.tensor(x)`` is equivalent to ``x.clone().detach()``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4651" class="pln"><span class="n"><a href="#t4651">4651</a></span><span class="t"><span class="str">    and ``torch.tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4652" class="pln"><span class="n"><a href="#t4652">4652</a></span><span class="t"><span class="str">    The equivalents using ``clone()`` and ``detach()`` are recommended.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4653" class="pln"><span class="n"><a href="#t4653">4653</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4654" class="pln"><span class="n"><a href="#t4654">4654</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4655" class="pln"><span class="n"><a href="#t4655">4655</a></span><span class="t"><span class="str">    {data}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4656" class="pln"><span class="n"><a href="#t4656">4656</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4657" class="pln"><span class="n"><a href="#t4657">4657</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4658" class="pln"><span class="n"><a href="#t4658">4658</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4659" class="pln"><span class="n"><a href="#t4659">4659</a></span><span class="t"><span class="str">    {pin_memory}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4660" class="pln"><span class="n"><a href="#t4660">4660</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4661" class="pln"><span class="n"><a href="#t4661">4661</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4662" class="pln"><span class="n"><a href="#t4662">4662</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4663" class="pln"><span class="n"><a href="#t4663">4663</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4664" class="pln"><span class="n"><a href="#t4664">4664</a></span><span class="t"><span class="str">    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4665" class="pln"><span class="n"><a href="#t4665">4665</a></span><span class="t"><span class="str">    tensor([[ 0.1000,  1.2000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4666" class="pln"><span class="n"><a href="#t4666">4666</a></span><span class="t"><span class="str">            [ 2.2000,  3.1000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4667" class="pln"><span class="n"><a href="#t4667">4667</a></span><span class="t"><span class="str">            [ 4.9000,  5.2000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4668" class="pln"><span class="n"><a href="#t4668">4668</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4669" class="pln"><span class="n"><a href="#t4669">4669</a></span><span class="t"><span class="str">    >>> torch.tensor([0, 1])  # Type inference on data</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4670" class="pln"><span class="n"><a href="#t4670">4670</a></span><span class="t"><span class="str">    tensor([ 0,  1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4671" class="pln"><span class="n"><a href="#t4671">4671</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4672" class="pln"><span class="n"><a href="#t4672">4672</a></span><span class="t"><span class="str">    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4673" class="pln"><span class="n"><a href="#t4673">4673</a></span><span class="t"><span class="str">                     dtype=torch.float64,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4674" class="pln"><span class="n"><a href="#t4674">4674</a></span><span class="t"><span class="str">                     device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4675" class="pln"><span class="n"><a href="#t4675">4675</a></span><span class="t"><span class="str">    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4676" class="pln"><span class="n"><a href="#t4676">4676</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4677" class="pln"><span class="n"><a href="#t4677">4677</a></span><span class="t"><span class="str">    >>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4678" class="pln"><span class="n"><a href="#t4678">4678</a></span><span class="t"><span class="str">    tensor(3.1416)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4679" class="pln"><span class="n"><a href="#t4679">4679</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4680" class="pln"><span class="n"><a href="#t4680">4680</a></span><span class="t"><span class="str">    >>> torch.tensor([])  # Create an empty tensor (of size (0,))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4681" class="pln"><span class="n"><a href="#t4681">4681</a></span><span class="t"><span class="str">    tensor([])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4682" class="pln"><span class="n"><a href="#t4682">4682</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_data_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4683" class="pln"><span class="n"><a href="#t4683">4683</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4684" class="run"><span class="n"><a href="#t4684">4684</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">range</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4685" class="pln"><span class="n"><a href="#t4685">4685</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4686" class="pln"><span class="n"><a href="#t4686">4686</a></span><span class="t"><span class="str">range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4687" class="pln"><span class="n"><a href="#t4687">4687</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4688" class="pln"><span class="n"><a href="#t4688">4688</a></span><span class="t"><span class="str">Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4689" class="pln"><span class="n"><a href="#t4689">4689</a></span><span class="t"><span class="str">with values from :attr:`start` to :attr:`end` with step :attr:`step`. Step is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4690" class="pln"><span class="n"><a href="#t4690">4690</a></span><span class="t"><span class="str">the gap between two values in the tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4691" class="pln"><span class="n"><a href="#t4691">4691</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4692" class="pln"><span class="n"><a href="#t4692">4692</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4693" class="pln"><span class="n"><a href="#t4693">4693</a></span><span class="t"><span class="str">    \text{out}_{i+1} = \text{out}_i + \text{step}.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4694" class="pln"><span class="n"><a href="#t4694">4694</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4695" class="pln"><span class="n"><a href="#t4695">4695</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4696" class="pln"><span class="n"><a href="#t4696">4696</a></span><span class="t"><span class="str">    This function is deprecated in favor of :func:`torch.arange`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4697" class="pln"><span class="n"><a href="#t4697">4697</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4698" class="pln"><span class="n"><a href="#t4698">4698</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4699" class="pln"><span class="n"><a href="#t4699">4699</a></span><span class="t"><span class="str">    start (float): the starting value for the set of points. Default: ``0``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4700" class="pln"><span class="n"><a href="#t4700">4700</a></span><span class="t"><span class="str">    end (float): the ending value for the set of points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4701" class="pln"><span class="n"><a href="#t4701">4701</a></span><span class="t"><span class="str">    step (float): the gap between each pair of adjacent points. Default: ``1``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4702" class="pln"><span class="n"><a href="#t4702">4702</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4703" class="pln"><span class="n"><a href="#t4703">4703</a></span><span class="t"><span class="str">    {dtype} If `dtype` is not given, infer the data type from the other input</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4704" class="pln"><span class="n"><a href="#t4704">4704</a></span><span class="t"><span class="str">        arguments. If any of `start`, `end`, or `stop` are floating-point, the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4705" class="pln"><span class="n"><a href="#t4705">4705</a></span><span class="t"><span class="str">        `dtype` is inferred to be the default dtype, see</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4706" class="pln"><span class="n"><a href="#t4706">4706</a></span><span class="t"><span class="str">        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4707" class="pln"><span class="n"><a href="#t4707">4707</a></span><span class="t"><span class="str">        be `torch.int64`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4708" class="pln"><span class="n"><a href="#t4708">4708</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4709" class="pln"><span class="n"><a href="#t4709">4709</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4710" class="pln"><span class="n"><a href="#t4710">4710</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4711" class="pln"><span class="n"><a href="#t4711">4711</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4712" class="pln"><span class="n"><a href="#t4712">4712</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4713" class="pln"><span class="n"><a href="#t4713">4713</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4714" class="pln"><span class="n"><a href="#t4714">4714</a></span><span class="t"><span class="str">    >>> torch.range(1, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4715" class="pln"><span class="n"><a href="#t4715">4715</a></span><span class="t"><span class="str">    tensor([ 1.,  2.,  3.,  4.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4716" class="pln"><span class="n"><a href="#t4716">4716</a></span><span class="t"><span class="str">    >>> torch.range(1, 4, 0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4717" class="pln"><span class="n"><a href="#t4717">4717</a></span><span class="t"><span class="str">    tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4718" class="pln"><span class="n"><a href="#t4718">4718</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4719" class="pln"><span class="n"><a href="#t4719">4719</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4720" class="run"><span class="n"><a href="#t4720">4720</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">arange</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4721" class="pln"><span class="n"><a href="#t4721">4721</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4722" class="pln"><span class="n"><a href="#t4722">4722</a></span><span class="t"><span class="str">arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4723" class="pln"><span class="n"><a href="#t4723">4723</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4724" class="pln"><span class="n"><a href="#t4724">4724</a></span><span class="t"><span class="str">Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4725" class="pln"><span class="n"><a href="#t4725">4725</a></span><span class="t"><span class="str">with values from the interval ``[start, end)`` taken with common difference</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4726" class="pln"><span class="n"><a href="#t4726">4726</a></span><span class="t"><span class="str">:attr:`step` beginning from `start`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4727" class="pln"><span class="n"><a href="#t4727">4727</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4728" class="pln"><span class="n"><a href="#t4728">4728</a></span><span class="t"><span class="str">Note that non-integer :attr:`step` is subject to floating point rounding errors when</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4729" class="pln"><span class="n"><a href="#t4729">4729</a></span><span class="t"><span class="str">comparing against :attr:`end`; to avoid inconsistency, we advise adding a small epsilon to :attr:`end`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4730" class="pln"><span class="n"><a href="#t4730">4730</a></span><span class="t"><span class="str">in such cases.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4731" class="pln"><span class="n"><a href="#t4731">4731</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4732" class="pln"><span class="n"><a href="#t4732">4732</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4733" class="pln"><span class="n"><a href="#t4733">4733</a></span><span class="t"><span class="str">    \text{out}_{{i+1}} = \text{out}_{i} + \text{step}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4734" class="pln"><span class="n"><a href="#t4734">4734</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4735" class="pln"><span class="n"><a href="#t4735">4735</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4736" class="pln"><span class="n"><a href="#t4736">4736</a></span><span class="t"><span class="str">    start (Number): the starting value for the set of points. Default: ``0``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4737" class="pln"><span class="n"><a href="#t4737">4737</a></span><span class="t"><span class="str">    end (Number): the ending value for the set of points</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4738" class="pln"><span class="n"><a href="#t4738">4738</a></span><span class="t"><span class="str">    step (Number): the gap between each pair of adjacent points. Default: ``1``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4739" class="pln"><span class="n"><a href="#t4739">4739</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4740" class="pln"><span class="n"><a href="#t4740">4740</a></span><span class="t"><span class="str">    {dtype} If `dtype` is not given, infer the data type from the other input</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4741" class="pln"><span class="n"><a href="#t4741">4741</a></span><span class="t"><span class="str">        arguments. If any of `start`, `end`, or `stop` are floating-point, the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4742" class="pln"><span class="n"><a href="#t4742">4742</a></span><span class="t"><span class="str">        `dtype` is inferred to be the default dtype, see</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4743" class="pln"><span class="n"><a href="#t4743">4743</a></span><span class="t"><span class="str">        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4744" class="pln"><span class="n"><a href="#t4744">4744</a></span><span class="t"><span class="str">        be `torch.int64`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4745" class="pln"><span class="n"><a href="#t4745">4745</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4746" class="pln"><span class="n"><a href="#t4746">4746</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4747" class="pln"><span class="n"><a href="#t4747">4747</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4748" class="pln"><span class="n"><a href="#t4748">4748</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4749" class="pln"><span class="n"><a href="#t4749">4749</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4750" class="pln"><span class="n"><a href="#t4750">4750</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4751" class="pln"><span class="n"><a href="#t4751">4751</a></span><span class="t"><span class="str">    >>> torch.arange(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4752" class="pln"><span class="n"><a href="#t4752">4752</a></span><span class="t"><span class="str">    tensor([ 0,  1,  2,  3,  4])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4753" class="pln"><span class="n"><a href="#t4753">4753</a></span><span class="t"><span class="str">    >>> torch.arange(1, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4754" class="pln"><span class="n"><a href="#t4754">4754</a></span><span class="t"><span class="str">    tensor([ 1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4755" class="pln"><span class="n"><a href="#t4755">4755</a></span><span class="t"><span class="str">    >>> torch.arange(1, 2.5, 0.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4756" class="pln"><span class="n"><a href="#t4756">4756</a></span><span class="t"><span class="str">    tensor([ 1.0000,  1.5000,  2.0000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4757" class="pln"><span class="n"><a href="#t4757">4757</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4758" class="pln"><span class="n"><a href="#t4758">4758</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4759" class="run"><span class="n"><a href="#t4759">4759</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">remainder</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4760" class="pln"><span class="n"><a href="#t4760">4760</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4761" class="pln"><span class="n"><a href="#t4761">4761</a></span><span class="t"><span class="str">remainder(input, other, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4762" class="pln"><span class="n"><a href="#t4762">4762</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4763" class="pln"><span class="n"><a href="#t4763">4763</a></span><span class="t"><span class="str">Computes the element-wise remainder of division.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4764" class="pln"><span class="n"><a href="#t4764">4764</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4765" class="pln"><span class="n"><a href="#t4765">4765</a></span><span class="t"><span class="str">The divisor and dividend may contain both for integer and floating point</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4766" class="pln"><span class="n"><a href="#t4766">4766</a></span><span class="t"><span class="str">numbers. The remainder has the same sign as the divisor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4767" class="pln"><span class="n"><a href="#t4767">4767</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4768" class="pln"><span class="n"><a href="#t4768">4768</a></span><span class="t"><span class="str">When :attr:`other` is a tensor, the shapes of :attr:`input` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4769" class="pln"><span class="n"><a href="#t4769">4769</a></span><span class="t"><span class="str">:attr:`other` must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4770" class="pln"><span class="n"><a href="#t4770">4770</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4771" class="pln"><span class="n"><a href="#t4771">4771</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4772" class="pln"><span class="n"><a href="#t4772">4772</a></span><span class="t"><span class="str">    input (Tensor): the dividend</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4773" class="pln"><span class="n"><a href="#t4773">4773</a></span><span class="t"><span class="str">    other (Tensor or float): the divisor that may be either a number or a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4774" class="pln"><span class="n"><a href="#t4774">4774</a></span><span class="t"><span class="str">                               Tensor of the same shape as the dividend</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4775" class="pln"><span class="n"><a href="#t4775">4775</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4776" class="pln"><span class="n"><a href="#t4776">4776</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4777" class="pln"><span class="n"><a href="#t4777">4777</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4778" class="pln"><span class="n"><a href="#t4778">4778</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4779" class="pln"><span class="n"><a href="#t4779">4779</a></span><span class="t"><span class="str">    >>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4780" class="pln"><span class="n"><a href="#t4780">4780</a></span><span class="t"><span class="str">    tensor([ 1.,  0.,  1.,  1.,  0.,  1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4781" class="pln"><span class="n"><a href="#t4781">4781</a></span><span class="t"><span class="str">    >>> torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4782" class="pln"><span class="n"><a href="#t4782">4782</a></span><span class="t"><span class="str">    tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4783" class="pln"><span class="n"><a href="#t4783">4783</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4784" class="pln"><span class="n"><a href="#t4784">4784</a></span><span class="t"><span class="str">.. seealso::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4785" class="pln"><span class="n"><a href="#t4785">4785</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4786" class="pln"><span class="n"><a href="#t4786">4786</a></span><span class="t"><span class="str">        :func:`torch.fmod`, which computes the element-wise remainder of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4787" class="pln"><span class="n"><a href="#t4787">4787</a></span><span class="t"><span class="str">        division equivalently to the C library function ``fmod()``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4788" class="pln"><span class="n"><a href="#t4788">4788</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4789" class="pln"><span class="n"><a href="#t4789">4789</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4790" class="run"><span class="n"><a href="#t4790">4790</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">renorm</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4791" class="pln"><span class="n"><a href="#t4791">4791</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4792" class="pln"><span class="n"><a href="#t4792">4792</a></span><span class="t"><span class="str">renorm(input, p, dim, maxnorm, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4793" class="pln"><span class="n"><a href="#t4793">4793</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4794" class="pln"><span class="n"><a href="#t4794">4794</a></span><span class="t"><span class="str">Returns a tensor where each sub-tensor of :attr:`input` along dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4795" class="pln"><span class="n"><a href="#t4795">4795</a></span><span class="t"><span class="str">:attr:`dim` is normalized such that the `p`-norm of the sub-tensor is lower</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4796" class="pln"><span class="n"><a href="#t4796">4796</a></span><span class="t"><span class="str">than the value :attr:`maxnorm`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4797" class="pln"><span class="n"><a href="#t4797">4797</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4798" class="pln"><span class="n"><a href="#t4798">4798</a></span><span class="t"><span class="str">.. note:: If the norm of a row is lower than `maxnorm`, the row is unchanged</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4799" class="pln"><span class="n"><a href="#t4799">4799</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4800" class="pln"><span class="n"><a href="#t4800">4800</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4801" class="pln"><span class="n"><a href="#t4801">4801</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4802" class="pln"><span class="n"><a href="#t4802">4802</a></span><span class="t"><span class="str">    p (float): the power for the norm computation</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4803" class="pln"><span class="n"><a href="#t4803">4803</a></span><span class="t"><span class="str">    dim (int): the dimension to slice over to get the sub-tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4804" class="pln"><span class="n"><a href="#t4804">4804</a></span><span class="t"><span class="str">    maxnorm (float): the maximum norm to keep each sub-tensor under</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4805" class="pln"><span class="n"><a href="#t4805">4805</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4806" class="pln"><span class="n"><a href="#t4806">4806</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4807" class="pln"><span class="n"><a href="#t4807">4807</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4808" class="pln"><span class="n"><a href="#t4808">4808</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4809" class="pln"><span class="n"><a href="#t4809">4809</a></span><span class="t"><span class="str">    >>> x = torch.ones(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4810" class="pln"><span class="n"><a href="#t4810">4810</a></span><span class="t"><span class="str">    >>> x[1].fill_(2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4811" class="pln"><span class="n"><a href="#t4811">4811</a></span><span class="t"><span class="str">    tensor([ 2.,  2.,  2.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4812" class="pln"><span class="n"><a href="#t4812">4812</a></span><span class="t"><span class="str">    >>> x[2].fill_(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4813" class="pln"><span class="n"><a href="#t4813">4813</a></span><span class="t"><span class="str">    tensor([ 3.,  3.,  3.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4814" class="pln"><span class="n"><a href="#t4814">4814</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4815" class="pln"><span class="n"><a href="#t4815">4815</a></span><span class="t"><span class="str">    tensor([[ 1.,  1.,  1.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4816" class="pln"><span class="n"><a href="#t4816">4816</a></span><span class="t"><span class="str">            [ 2.,  2.,  2.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4817" class="pln"><span class="n"><a href="#t4817">4817</a></span><span class="t"><span class="str">            [ 3.,  3.,  3.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4818" class="pln"><span class="n"><a href="#t4818">4818</a></span><span class="t"><span class="str">    >>> torch.renorm(x, 1, 0, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4819" class="pln"><span class="n"><a href="#t4819">4819</a></span><span class="t"><span class="str">    tensor([[ 1.0000,  1.0000,  1.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4820" class="pln"><span class="n"><a href="#t4820">4820</a></span><span class="t"><span class="str">            [ 1.6667,  1.6667,  1.6667],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4821" class="pln"><span class="n"><a href="#t4821">4821</a></span><span class="t"><span class="str">            [ 1.6667,  1.6667,  1.6667]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4822" class="pln"><span class="n"><a href="#t4822">4822</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4823" class="pln"><span class="n"><a href="#t4823">4823</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4824" class="run"><span class="n"><a href="#t4824">4824</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">reshape</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4825" class="pln"><span class="n"><a href="#t4825">4825</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4826" class="pln"><span class="n"><a href="#t4826">4826</a></span><span class="t"><span class="str">reshape(input, shape) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4827" class="pln"><span class="n"><a href="#t4827">4827</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4828" class="pln"><span class="n"><a href="#t4828">4828</a></span><span class="t"><span class="str">Returns a tensor with the same data and number of elements as :attr:`input`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4829" class="pln"><span class="n"><a href="#t4829">4829</a></span><span class="t"><span class="str">but with the specified shape. When possible, the returned tensor will be a view</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4830" class="pln"><span class="n"><a href="#t4830">4830</a></span><span class="t"><span class="str">of :attr:`input`. Otherwise, it will be a copy. Contiguous inputs and inputs</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4831" class="pln"><span class="n"><a href="#t4831">4831</a></span><span class="t"><span class="str">with compatible strides can be reshaped without copying, but you should not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4832" class="pln"><span class="n"><a href="#t4832">4832</a></span><span class="t"><span class="str">depend on the copying vs. viewing behavior.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4833" class="pln"><span class="n"><a href="#t4833">4833</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4834" class="pln"><span class="n"><a href="#t4834">4834</a></span><span class="t"><span class="str">See :meth:`torch.Tensor.view` on when it is possible to return a view.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4835" class="pln"><span class="n"><a href="#t4835">4835</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4836" class="pln"><span class="n"><a href="#t4836">4836</a></span><span class="t"><span class="str">A single dimension may be -1, in which case it's inferred from the remaining</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4837" class="pln"><span class="n"><a href="#t4837">4837</a></span><span class="t"><span class="str">dimensions and the number of elements in :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4838" class="pln"><span class="n"><a href="#t4838">4838</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4839" class="pln"><span class="n"><a href="#t4839">4839</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4840" class="pln"><span class="n"><a href="#t4840">4840</a></span><span class="t"><span class="str">    input (Tensor): the tensor to be reshaped</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4841" class="pln"><span class="n"><a href="#t4841">4841</a></span><span class="t"><span class="str">    shape (tuple of ints): the new shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4842" class="pln"><span class="n"><a href="#t4842">4842</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4843" class="pln"><span class="n"><a href="#t4843">4843</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4844" class="pln"><span class="n"><a href="#t4844">4844</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4845" class="pln"><span class="n"><a href="#t4845">4845</a></span><span class="t"><span class="str">    >>> a = torch.arange(4.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4846" class="pln"><span class="n"><a href="#t4846">4846</a></span><span class="t"><span class="str">    >>> torch.reshape(a, (2, 2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4847" class="pln"><span class="n"><a href="#t4847">4847</a></span><span class="t"><span class="str">    tensor([[ 0.,  1.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4848" class="pln"><span class="n"><a href="#t4848">4848</a></span><span class="t"><span class="str">            [ 2.,  3.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4849" class="pln"><span class="n"><a href="#t4849">4849</a></span><span class="t"><span class="str">    >>> b = torch.tensor([[0, 1], [2, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4850" class="pln"><span class="n"><a href="#t4850">4850</a></span><span class="t"><span class="str">    >>> torch.reshape(b, (-1,))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4851" class="pln"><span class="n"><a href="#t4851">4851</a></span><span class="t"><span class="str">    tensor([ 0,  1,  2,  3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4852" class="pln"><span class="n"><a href="#t4852">4852</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4853" class="pln"><span class="n"><a href="#t4853">4853</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4854" class="pln"><span class="n"><a href="#t4854">4854</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4855" class="run"><span class="n"><a href="#t4855">4855</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">result_type</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4856" class="pln"><span class="n"><a href="#t4856">4856</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4857" class="pln"><span class="n"><a href="#t4857">4857</a></span><span class="t"><span class="str">result_type(tensor1, tensor2) -> dtype</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4858" class="pln"><span class="n"><a href="#t4858">4858</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4859" class="pln"><span class="n"><a href="#t4859">4859</a></span><span class="t"><span class="str">Returns the :class:`torch.dtype` that would result from performing an arithmetic</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4860" class="pln"><span class="n"><a href="#t4860">4860</a></span><span class="t"><span class="str">operation on the provided input tensors. See type promotion :ref:`documentation &lt;type-promotion-doc>`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4861" class="pln"><span class="n"><a href="#t4861">4861</a></span><span class="t"><span class="str">for more information on the type promotion logic.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4862" class="pln"><span class="n"><a href="#t4862">4862</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4863" class="pln"><span class="n"><a href="#t4863">4863</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4864" class="pln"><span class="n"><a href="#t4864">4864</a></span><span class="t"><span class="str">    tensor1 (Tensor or Number): an input tensor or number</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4865" class="pln"><span class="n"><a href="#t4865">4865</a></span><span class="t"><span class="str">    tensor2 (Tensor or Number): an input tensor or number</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4866" class="pln"><span class="n"><a href="#t4866">4866</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4867" class="pln"><span class="n"><a href="#t4867">4867</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4868" class="pln"><span class="n"><a href="#t4868">4868</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4869" class="pln"><span class="n"><a href="#t4869">4869</a></span><span class="t"><span class="str">    >>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4870" class="pln"><span class="n"><a href="#t4870">4870</a></span><span class="t"><span class="str">    torch.float32</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4871" class="pln"><span class="n"><a href="#t4871">4871</a></span><span class="t"><span class="str">    >>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4872" class="pln"><span class="n"><a href="#t4872">4872</a></span><span class="t"><span class="str">    torch.uint8</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4873" class="pln"><span class="n"><a href="#t4873">4873</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4874" class="pln"><span class="n"><a href="#t4874">4874</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4875" class="pln"><span class="n"><a href="#t4875">4875</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4876" class="run"><span class="n"><a href="#t4876">4876</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">round</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4877" class="pln"><span class="n"><a href="#t4877">4877</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4878" class="pln"><span class="n"><a href="#t4878">4878</a></span><span class="t"><span class="str">round(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4879" class="pln"><span class="n"><a href="#t4879">4879</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4880" class="pln"><span class="n"><a href="#t4880">4880</a></span><span class="t"><span class="str">Returns a new tensor with each of the elements of :attr:`input` rounded</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4881" class="pln"><span class="n"><a href="#t4881">4881</a></span><span class="t"><span class="str">to the closest integer.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4882" class="pln"><span class="n"><a href="#t4882">4882</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4883" class="pln"><span class="n"><a href="#t4883">4883</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4884" class="pln"><span class="n"><a href="#t4884">4884</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4885" class="pln"><span class="n"><a href="#t4885">4885</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4886" class="pln"><span class="n"><a href="#t4886">4886</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4887" class="pln"><span class="n"><a href="#t4887">4887</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4888" class="pln"><span class="n"><a href="#t4888">4888</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4889" class="pln"><span class="n"><a href="#t4889">4889</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4890" class="pln"><span class="n"><a href="#t4890">4890</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4891" class="pln"><span class="n"><a href="#t4891">4891</a></span><span class="t"><span class="str">    tensor([ 0.9920,  0.6077,  0.9734, -1.0362])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4892" class="pln"><span class="n"><a href="#t4892">4892</a></span><span class="t"><span class="str">    >>> torch.round(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4893" class="pln"><span class="n"><a href="#t4893">4893</a></span><span class="t"><span class="str">    tensor([ 1.,  1.,  1., -1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4894" class="pln"><span class="n"><a href="#t4894">4894</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4895" class="pln"><span class="n"><a href="#t4895">4895</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4896" class="run"><span class="n"><a href="#t4896">4896</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">rsqrt</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4897" class="pln"><span class="n"><a href="#t4897">4897</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4898" class="pln"><span class="n"><a href="#t4898">4898</a></span><span class="t"><span class="str">rsqrt(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4899" class="pln"><span class="n"><a href="#t4899">4899</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4900" class="pln"><span class="n"><a href="#t4900">4900</a></span><span class="t"><span class="str">Returns a new tensor with the reciprocal of the square-root of each of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4901" class="pln"><span class="n"><a href="#t4901">4901</a></span><span class="t"><span class="str">the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4902" class="pln"><span class="n"><a href="#t4902">4902</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4903" class="pln"><span class="n"><a href="#t4903">4903</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4904" class="pln"><span class="n"><a href="#t4904">4904</a></span><span class="t"><span class="str">    \text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4905" class="pln"><span class="n"><a href="#t4905">4905</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4906" class="pln"><span class="n"><a href="#t4906">4906</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4907" class="pln"><span class="n"><a href="#t4907">4907</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4908" class="pln"><span class="n"><a href="#t4908">4908</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4909" class="pln"><span class="n"><a href="#t4909">4909</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4910" class="pln"><span class="n"><a href="#t4910">4910</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4911" class="pln"><span class="n"><a href="#t4911">4911</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4912" class="pln"><span class="n"><a href="#t4912">4912</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4913" class="pln"><span class="n"><a href="#t4913">4913</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4914" class="pln"><span class="n"><a href="#t4914">4914</a></span><span class="t"><span class="str">    tensor([-0.0370,  0.2970,  1.5420, -0.9105])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4915" class="pln"><span class="n"><a href="#t4915">4915</a></span><span class="t"><span class="str">    >>> torch.rsqrt(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4916" class="pln"><span class="n"><a href="#t4916">4916</a></span><span class="t"><span class="str">    tensor([    nan,  1.8351,  0.8053,     nan])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4917" class="pln"><span class="n"><a href="#t4917">4917</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4918" class="pln"><span class="n"><a href="#t4918">4918</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4919" class="run"><span class="n"><a href="#t4919">4919</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">set_flush_denormal</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4920" class="pln"><span class="n"><a href="#t4920">4920</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4921" class="pln"><span class="n"><a href="#t4921">4921</a></span><span class="t"><span class="str">set_flush_denormal(mode) -> bool</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4922" class="pln"><span class="n"><a href="#t4922">4922</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4923" class="pln"><span class="n"><a href="#t4923">4923</a></span><span class="t"><span class="str">Disables denormal floating numbers on CPU.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4924" class="pln"><span class="n"><a href="#t4924">4924</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4925" class="pln"><span class="n"><a href="#t4925">4925</a></span><span class="t"><span class="str">Returns ``True`` if your system supports flushing denormal numbers and it</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4926" class="pln"><span class="n"><a href="#t4926">4926</a></span><span class="t"><span class="str">successfully configures flush denormal mode.  :meth:`~torch.set_flush_denormal`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4927" class="pln"><span class="n"><a href="#t4927">4927</a></span><span class="t"><span class="str">is only supported on x86 architectures supporting SSE3.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4928" class="pln"><span class="n"><a href="#t4928">4928</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4929" class="pln"><span class="n"><a href="#t4929">4929</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4930" class="pln"><span class="n"><a href="#t4930">4930</a></span><span class="t"><span class="str">    mode (bool): Controls whether to enable flush denormal mode or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4931" class="pln"><span class="n"><a href="#t4931">4931</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4932" class="pln"><span class="n"><a href="#t4932">4932</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4933" class="pln"><span class="n"><a href="#t4933">4933</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4934" class="pln"><span class="n"><a href="#t4934">4934</a></span><span class="t"><span class="str">    >>> torch.set_flush_denormal(True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4935" class="pln"><span class="n"><a href="#t4935">4935</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4936" class="pln"><span class="n"><a href="#t4936">4936</a></span><span class="t"><span class="str">    >>> torch.tensor([1e-323], dtype=torch.float64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4937" class="pln"><span class="n"><a href="#t4937">4937</a></span><span class="t"><span class="str">    tensor([ 0.], dtype=torch.float64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4938" class="pln"><span class="n"><a href="#t4938">4938</a></span><span class="t"><span class="str">    >>> torch.set_flush_denormal(False)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4939" class="pln"><span class="n"><a href="#t4939">4939</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4940" class="pln"><span class="n"><a href="#t4940">4940</a></span><span class="t"><span class="str">    >>> torch.tensor([1e-323], dtype=torch.float64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4941" class="pln"><span class="n"><a href="#t4941">4941</a></span><span class="t"><span class="str">    tensor(9.88131e-324 *</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4942" class="pln"><span class="n"><a href="#t4942">4942</a></span><span class="t"><span class="str">           [ 1.0000], dtype=torch.float64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4943" class="pln"><span class="n"><a href="#t4943">4943</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4944" class="pln"><span class="n"><a href="#t4944">4944</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4945" class="run"><span class="n"><a href="#t4945">4945</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">set_num_threads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4946" class="pln"><span class="n"><a href="#t4946">4946</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4947" class="pln"><span class="n"><a href="#t4947">4947</a></span><span class="t"><span class="str">set_num_threads(int)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4948" class="pln"><span class="n"><a href="#t4948">4948</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4949" class="pln"><span class="n"><a href="#t4949">4949</a></span><span class="t"><span class="str">Sets the number of threads used for intraop parallelism on CPU.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4950" class="pln"><span class="n"><a href="#t4950">4950</a></span><span class="t"><span class="str">WARNING:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4951" class="pln"><span class="n"><a href="#t4951">4951</a></span><span class="t"><span class="str">To ensure that the correct number of threads is used, set_num_threads</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4952" class="pln"><span class="n"><a href="#t4952">4952</a></span><span class="t"><span class="str">must be called before running eager, JIT or autograd code.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4953" class="pln"><span class="n"><a href="#t4953">4953</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4954" class="pln"><span class="n"><a href="#t4954">4954</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4955" class="run"><span class="n"><a href="#t4955">4955</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">set_num_interop_threads</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4956" class="pln"><span class="n"><a href="#t4956">4956</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4957" class="pln"><span class="n"><a href="#t4957">4957</a></span><span class="t"><span class="str">set_num_interop_threads(int)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4958" class="pln"><span class="n"><a href="#t4958">4958</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4959" class="pln"><span class="n"><a href="#t4959">4959</a></span><span class="t"><span class="str">Sets the number of threads used for interop parallelism</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4960" class="pln"><span class="n"><a href="#t4960">4960</a></span><span class="t"><span class="str">(e.g. in JIT interpreter) on CPU.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4961" class="pln"><span class="n"><a href="#t4961">4961</a></span><span class="t"><span class="str">WARNING: Can only be called once and before any inter-op parallel work</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4962" class="pln"><span class="n"><a href="#t4962">4962</a></span><span class="t"><span class="str">is started (e.g. JIT execution).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4963" class="pln"><span class="n"><a href="#t4963">4963</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4964" class="pln"><span class="n"><a href="#t4964">4964</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4965" class="run"><span class="n"><a href="#t4965">4965</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sigmoid</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4966" class="pln"><span class="n"><a href="#t4966">4966</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4967" class="pln"><span class="n"><a href="#t4967">4967</a></span><span class="t"><span class="str">sigmoid(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4968" class="pln"><span class="n"><a href="#t4968">4968</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4969" class="pln"><span class="n"><a href="#t4969">4969</a></span><span class="t"><span class="str">Returns a new tensor with the sigmoid of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4970" class="pln"><span class="n"><a href="#t4970">4970</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4971" class="pln"><span class="n"><a href="#t4971">4971</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4972" class="pln"><span class="n"><a href="#t4972">4972</a></span><span class="t"><span class="str">    \text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4973" class="pln"><span class="n"><a href="#t4973">4973</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4974" class="pln"><span class="n"><a href="#t4974">4974</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4975" class="pln"><span class="n"><a href="#t4975">4975</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4976" class="pln"><span class="n"><a href="#t4976">4976</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4977" class="pln"><span class="n"><a href="#t4977">4977</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4978" class="pln"><span class="n"><a href="#t4978">4978</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4979" class="pln"><span class="n"><a href="#t4979">4979</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4980" class="pln"><span class="n"><a href="#t4980">4980</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4981" class="pln"><span class="n"><a href="#t4981">4981</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4982" class="pln"><span class="n"><a href="#t4982">4982</a></span><span class="t"><span class="str">    tensor([ 0.9213,  1.0887, -0.8858, -1.7683])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4983" class="pln"><span class="n"><a href="#t4983">4983</a></span><span class="t"><span class="str">    >>> torch.sigmoid(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4984" class="pln"><span class="n"><a href="#t4984">4984</a></span><span class="t"><span class="str">    tensor([ 0.7153,  0.7481,  0.2920,  0.1458])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4985" class="pln"><span class="n"><a href="#t4985">4985</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4986" class="pln"><span class="n"><a href="#t4986">4986</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4987" class="run"><span class="n"><a href="#t4987">4987</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sign</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4988" class="pln"><span class="n"><a href="#t4988">4988</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4989" class="pln"><span class="n"><a href="#t4989">4989</a></span><span class="t"><span class="str">sign(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4990" class="pln"><span class="n"><a href="#t4990">4990</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4991" class="pln"><span class="n"><a href="#t4991">4991</a></span><span class="t"><span class="str">Returns a new tensor with the signs of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4992" class="pln"><span class="n"><a href="#t4992">4992</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t4993" class="pln"><span class="n"><a href="#t4993">4993</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4994" class="pln"><span class="n"><a href="#t4994">4994</a></span><span class="t"><span class="str">    \text{out}_{i} = \operatorname{sgn}(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4995" class="pln"><span class="n"><a href="#t4995">4995</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4996" class="pln"><span class="n"><a href="#t4996">4996</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4997" class="pln"><span class="n"><a href="#t4997">4997</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4998" class="pln"><span class="n"><a href="#t4998">4998</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4999" class="pln"><span class="n"><a href="#t4999">4999</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5000" class="pln"><span class="n"><a href="#t5000">5000</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5001" class="pln"><span class="n"><a href="#t5001">5001</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5002" class="pln"><span class="n"><a href="#t5002">5002</a></span><span class="t"><span class="str">    >>> a = torch.tensor([0.7, -1.2, 0., 2.3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5003" class="pln"><span class="n"><a href="#t5003">5003</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5004" class="pln"><span class="n"><a href="#t5004">5004</a></span><span class="t"><span class="str">    tensor([ 0.7000, -1.2000,  0.0000,  2.3000])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5005" class="pln"><span class="n"><a href="#t5005">5005</a></span><span class="t"><span class="str">    >>> torch.sign(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5006" class="pln"><span class="n"><a href="#t5006">5006</a></span><span class="t"><span class="str">    tensor([ 1., -1.,  0.,  1.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5007" class="pln"><span class="n"><a href="#t5007">5007</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5008" class="pln"><span class="n"><a href="#t5008">5008</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5009" class="run"><span class="n"><a href="#t5009">5009</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sin</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5010" class="pln"><span class="n"><a href="#t5010">5010</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5011" class="pln"><span class="n"><a href="#t5011">5011</a></span><span class="t"><span class="str">sin(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5012" class="pln"><span class="n"><a href="#t5012">5012</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5013" class="pln"><span class="n"><a href="#t5013">5013</a></span><span class="t"><span class="str">Returns a new tensor with the sine of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5014" class="pln"><span class="n"><a href="#t5014">5014</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5015" class="pln"><span class="n"><a href="#t5015">5015</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5016" class="pln"><span class="n"><a href="#t5016">5016</a></span><span class="t"><span class="str">    \text{out}_{i} = \sin(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5017" class="pln"><span class="n"><a href="#t5017">5017</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5018" class="pln"><span class="n"><a href="#t5018">5018</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5019" class="pln"><span class="n"><a href="#t5019">5019</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5020" class="pln"><span class="n"><a href="#t5020">5020</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5021" class="pln"><span class="n"><a href="#t5021">5021</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5022" class="pln"><span class="n"><a href="#t5022">5022</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5023" class="pln"><span class="n"><a href="#t5023">5023</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5024" class="pln"><span class="n"><a href="#t5024">5024</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5025" class="pln"><span class="n"><a href="#t5025">5025</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5026" class="pln"><span class="n"><a href="#t5026">5026</a></span><span class="t"><span class="str">    tensor([-0.5461,  0.1347, -2.7266, -0.2746])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5027" class="pln"><span class="n"><a href="#t5027">5027</a></span><span class="t"><span class="str">    >>> torch.sin(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5028" class="pln"><span class="n"><a href="#t5028">5028</a></span><span class="t"><span class="str">    tensor([-0.5194,  0.1343, -0.4032, -0.2711])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5029" class="pln"><span class="n"><a href="#t5029">5029</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5030" class="pln"><span class="n"><a href="#t5030">5030</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5031" class="run"><span class="n"><a href="#t5031">5031</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sinh</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5032" class="pln"><span class="n"><a href="#t5032">5032</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5033" class="pln"><span class="n"><a href="#t5033">5033</a></span><span class="t"><span class="str">sinh(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5034" class="pln"><span class="n"><a href="#t5034">5034</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5035" class="pln"><span class="n"><a href="#t5035">5035</a></span><span class="t"><span class="str">Returns a new tensor with the hyperbolic sine of the elements of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5036" class="pln"><span class="n"><a href="#t5036">5036</a></span><span class="t"><span class="str">:attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5037" class="pln"><span class="n"><a href="#t5037">5037</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5038" class="pln"><span class="n"><a href="#t5038">5038</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5039" class="pln"><span class="n"><a href="#t5039">5039</a></span><span class="t"><span class="str">    \text{out}_{i} = \sinh(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5040" class="pln"><span class="n"><a href="#t5040">5040</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5041" class="pln"><span class="n"><a href="#t5041">5041</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5042" class="pln"><span class="n"><a href="#t5042">5042</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5043" class="pln"><span class="n"><a href="#t5043">5043</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5044" class="pln"><span class="n"><a href="#t5044">5044</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5045" class="pln"><span class="n"><a href="#t5045">5045</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5046" class="pln"><span class="n"><a href="#t5046">5046</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5047" class="pln"><span class="n"><a href="#t5047">5047</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5048" class="pln"><span class="n"><a href="#t5048">5048</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5049" class="pln"><span class="n"><a href="#t5049">5049</a></span><span class="t"><span class="str">    tensor([ 0.5380, -0.8632, -0.1265,  0.9399])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5050" class="pln"><span class="n"><a href="#t5050">5050</a></span><span class="t"><span class="str">    >>> torch.sinh(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5051" class="pln"><span class="n"><a href="#t5051">5051</a></span><span class="t"><span class="str">    tensor([ 0.5644, -0.9744, -0.1268,  1.0845])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5052" class="pln"><span class="n"><a href="#t5052">5052</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5053" class="pln"><span class="n"><a href="#t5053">5053</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5054" class="run"><span class="n"><a href="#t5054">5054</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sort</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5055" class="pln"><span class="n"><a href="#t5055">5055</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5056" class="pln"><span class="n"><a href="#t5056">5056</a></span><span class="t"><span class="str">sort(input, dim=-1, descending=False, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5057" class="pln"><span class="n"><a href="#t5057">5057</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5058" class="pln"><span class="n"><a href="#t5058">5058</a></span><span class="t"><span class="str">Sorts the elements of the :attr:`input` tensor along a given dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5059" class="pln"><span class="n"><a href="#t5059">5059</a></span><span class="t"><span class="str">in ascending order by value.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5060" class="pln"><span class="n"><a href="#t5060">5060</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5061" class="pln"><span class="n"><a href="#t5061">5061</a></span><span class="t"><span class="str">If :attr:`dim` is not given, the last dimension of the `input` is chosen.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5062" class="pln"><span class="n"><a href="#t5062">5062</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5063" class="pln"><span class="n"><a href="#t5063">5063</a></span><span class="t"><span class="str">If :attr:`descending` is ``True`` then the elements are sorted in descending</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5064" class="pln"><span class="n"><a href="#t5064">5064</a></span><span class="t"><span class="str">order by value.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5065" class="pln"><span class="n"><a href="#t5065">5065</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5066" class="pln"><span class="n"><a href="#t5066">5066</a></span><span class="t"><span class="str">A namedtuple of (values, indices) is returned, where the `values` are the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5067" class="pln"><span class="n"><a href="#t5067">5067</a></span><span class="t"><span class="str">sorted values and `indices` are the indices of the elements in the original</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5068" class="pln"><span class="n"><a href="#t5068">5068</a></span><span class="t"><span class="str">`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5069" class="pln"><span class="n"><a href="#t5069">5069</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5070" class="pln"><span class="n"><a href="#t5070">5070</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5071" class="pln"><span class="n"><a href="#t5071">5071</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5072" class="pln"><span class="n"><a href="#t5072">5072</a></span><span class="t"><span class="str">    dim (int, optional): the dimension to sort along</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5073" class="pln"><span class="n"><a href="#t5073">5073</a></span><span class="t"><span class="str">    descending (bool, optional): controls the sorting order (ascending or descending)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5074" class="pln"><span class="n"><a href="#t5074">5074</a></span><span class="t"><span class="str">    out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5075" class="pln"><span class="n"><a href="#t5075">5075</a></span><span class="t"><span class="str">        be optionally given to be used as output buffers</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5076" class="pln"><span class="n"><a href="#t5076">5076</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5077" class="pln"><span class="n"><a href="#t5077">5077</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5078" class="pln"><span class="n"><a href="#t5078">5078</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5079" class="pln"><span class="n"><a href="#t5079">5079</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5080" class="pln"><span class="n"><a href="#t5080">5080</a></span><span class="t"><span class="str">    >>> sorted, indices = torch.sort(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5081" class="pln"><span class="n"><a href="#t5081">5081</a></span><span class="t"><span class="str">    >>> sorted</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5082" class="pln"><span class="n"><a href="#t5082">5082</a></span><span class="t"><span class="str">    tensor([[-0.2162,  0.0608,  0.6719,  2.3332],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5083" class="pln"><span class="n"><a href="#t5083">5083</a></span><span class="t"><span class="str">            [-0.5793,  0.0061,  0.6058,  0.9497],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5084" class="pln"><span class="n"><a href="#t5084">5084</a></span><span class="t"><span class="str">            [-0.5071,  0.3343,  0.9553,  1.0960]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5085" class="pln"><span class="n"><a href="#t5085">5085</a></span><span class="t"><span class="str">    >>> indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5086" class="pln"><span class="n"><a href="#t5086">5086</a></span><span class="t"><span class="str">    tensor([[ 1,  0,  2,  3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5087" class="pln"><span class="n"><a href="#t5087">5087</a></span><span class="t"><span class="str">            [ 3,  1,  0,  2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5088" class="pln"><span class="n"><a href="#t5088">5088</a></span><span class="t"><span class="str">            [ 0,  3,  1,  2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5089" class="pln"><span class="n"><a href="#t5089">5089</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5090" class="pln"><span class="n"><a href="#t5090">5090</a></span><span class="t"><span class="str">    >>> sorted, indices = torch.sort(x, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5091" class="pln"><span class="n"><a href="#t5091">5091</a></span><span class="t"><span class="str">    >>> sorted</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5092" class="pln"><span class="n"><a href="#t5092">5092</a></span><span class="t"><span class="str">    tensor([[-0.5071, -0.2162,  0.6719, -0.5793],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5093" class="pln"><span class="n"><a href="#t5093">5093</a></span><span class="t"><span class="str">            [ 0.0608,  0.0061,  0.9497,  0.3343],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5094" class="pln"><span class="n"><a href="#t5094">5094</a></span><span class="t"><span class="str">            [ 0.6058,  0.9553,  1.0960,  2.3332]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5095" class="pln"><span class="n"><a href="#t5095">5095</a></span><span class="t"><span class="str">    >>> indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5096" class="pln"><span class="n"><a href="#t5096">5096</a></span><span class="t"><span class="str">    tensor([[ 2,  0,  0,  1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5097" class="pln"><span class="n"><a href="#t5097">5097</a></span><span class="t"><span class="str">            [ 0,  1,  1,  2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5098" class="pln"><span class="n"><a href="#t5098">5098</a></span><span class="t"><span class="str">            [ 1,  2,  2,  0]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5099" class="pln"><span class="n"><a href="#t5099">5099</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5100" class="pln"><span class="n"><a href="#t5100">5100</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5101" class="run"><span class="n"><a href="#t5101">5101</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">argsort</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5102" class="pln"><span class="n"><a href="#t5102">5102</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5103" class="pln"><span class="n"><a href="#t5103">5103</a></span><span class="t"><span class="str">argsort(input, dim=-1, descending=False) -> LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5104" class="pln"><span class="n"><a href="#t5104">5104</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5105" class="pln"><span class="n"><a href="#t5105">5105</a></span><span class="t"><span class="str">Returns the indices that sort a tensor along a given dimension in ascending</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5106" class="pln"><span class="n"><a href="#t5106">5106</a></span><span class="t"><span class="str">order by value.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5107" class="pln"><span class="n"><a href="#t5107">5107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5108" class="pln"><span class="n"><a href="#t5108">5108</a></span><span class="t"><span class="str">This is the second value returned by :meth:`torch.sort`.  See its documentation</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5109" class="pln"><span class="n"><a href="#t5109">5109</a></span><span class="t"><span class="str">for the exact semantics of this method.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5110" class="pln"><span class="n"><a href="#t5110">5110</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5111" class="pln"><span class="n"><a href="#t5111">5111</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5112" class="pln"><span class="n"><a href="#t5112">5112</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5113" class="pln"><span class="n"><a href="#t5113">5113</a></span><span class="t"><span class="str">    dim (int, optional): the dimension to sort along</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5114" class="pln"><span class="n"><a href="#t5114">5114</a></span><span class="t"><span class="str">    descending (bool, optional): controls the sorting order (ascending or descending)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5115" class="pln"><span class="n"><a href="#t5115">5115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5116" class="pln"><span class="n"><a href="#t5116">5116</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5117" class="pln"><span class="n"><a href="#t5117">5117</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5118" class="pln"><span class="n"><a href="#t5118">5118</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5119" class="pln"><span class="n"><a href="#t5119">5119</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5120" class="pln"><span class="n"><a href="#t5120">5120</a></span><span class="t"><span class="str">    tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5121" class="pln"><span class="n"><a href="#t5121">5121</a></span><span class="t"><span class="str">            [ 0.1598,  0.0788, -0.0745, -1.2700],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5122" class="pln"><span class="n"><a href="#t5122">5122</a></span><span class="t"><span class="str">            [ 1.2208,  1.0722, -0.7064,  1.2564],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5123" class="pln"><span class="n"><a href="#t5123">5123</a></span><span class="t"><span class="str">            [ 0.0669, -0.2318, -0.8229, -0.9280]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5124" class="pln"><span class="n"><a href="#t5124">5124</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5125" class="pln"><span class="n"><a href="#t5125">5125</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5126" class="pln"><span class="n"><a href="#t5126">5126</a></span><span class="t"><span class="str">    >>> torch.argsort(a, dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5127" class="pln"><span class="n"><a href="#t5127">5127</a></span><span class="t"><span class="str">    tensor([[2, 0, 3, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5128" class="pln"><span class="n"><a href="#t5128">5128</a></span><span class="t"><span class="str">            [3, 2, 1, 0],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5129" class="pln"><span class="n"><a href="#t5129">5129</a></span><span class="t"><span class="str">            [2, 1, 0, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5130" class="pln"><span class="n"><a href="#t5130">5130</a></span><span class="t"><span class="str">            [3, 2, 1, 0]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5131" class="pln"><span class="n"><a href="#t5131">5131</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5132" class="pln"><span class="n"><a href="#t5132">5132</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5133" class="run"><span class="n"><a href="#t5133">5133</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sparse_coo_tensor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5134" class="pln"><span class="n"><a href="#t5134">5134</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5135" class="pln"><span class="n"><a href="#t5135">5135</a></span><span class="t"><span class="str">sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5136" class="pln"><span class="n"><a href="#t5136">5136</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5137" class="pln"><span class="n"><a href="#t5137">5137</a></span><span class="t"><span class="str">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given :attr:`indices`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5138" class="pln"><span class="n"><a href="#t5138">5138</a></span><span class="t"><span class="str">with the given :attr:`values`. A sparse tensor can be `uncoalesced`, in that case, there are duplicate</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5139" class="pln"><span class="n"><a href="#t5139">5139</a></span><span class="t"><span class="str">coordinates in the indices, and the value at that index is the sum of all duplicate value entries:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5140" class="pln"><span class="n"><a href="#t5140">5140</a></span><span class="t"><span class="str">`torch.sparse`_.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5141" class="pln"><span class="n"><a href="#t5141">5141</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5142" class="pln"><span class="n"><a href="#t5142">5142</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5143" class="pln"><span class="n"><a href="#t5143">5143</a></span><span class="t"><span class="str">    indices (array_like): Initial data for the tensor. Can be a list, tuple,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5144" class="pln"><span class="n"><a href="#t5144">5144</a></span><span class="t"><span class="str">        NumPy ``ndarray``, scalar, and other types. Will be cast to a :class:`torch.LongTensor`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5145" class="pln"><span class="n"><a href="#t5145">5145</a></span><span class="t"><span class="str">        internally. The indices are the coordinates of the non-zero values in the matrix, and thus</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5146" class="pln"><span class="n"><a href="#t5146">5146</a></span><span class="t"><span class="str">        should be two-dimensional where the first dimension is the number of tensor dimensions and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5147" class="pln"><span class="n"><a href="#t5147">5147</a></span><span class="t"><span class="str">        the second dimension is the number of non-zero values.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5148" class="pln"><span class="n"><a href="#t5148">5148</a></span><span class="t"><span class="str">    values (array_like): Initial values for the tensor. Can be a list, tuple,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5149" class="pln"><span class="n"><a href="#t5149">5149</a></span><span class="t"><span class="str">        NumPy ``ndarray``, scalar, and other types.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5150" class="pln"><span class="n"><a href="#t5150">5150</a></span><span class="t"><span class="str">    size (list, tuple, or :class:`torch.Size`, optional): Size of the sparse tensor. If not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5151" class="pln"><span class="n"><a href="#t5151">5151</a></span><span class="t"><span class="str">        provided the size will be inferred as the minimum size big enough to hold all non-zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5152" class="pln"><span class="n"><a href="#t5152">5152</a></span><span class="t"><span class="str">        elements.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5153" class="pln"><span class="n"><a href="#t5153">5153</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5154" class="pln"><span class="n"><a href="#t5154">5154</a></span><span class="t"><span class="str">        Default: if None, infers data type from :attr:`values`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5155" class="pln"><span class="n"><a href="#t5155">5155</a></span><span class="t"><span class="str">    device (:class:`torch.device`, optional): the desired device of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5156" class="pln"><span class="n"><a href="#t5156">5156</a></span><span class="t"><span class="str">        Default: if None, uses the current device for the default tensor type</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5157" class="pln"><span class="n"><a href="#t5157">5157</a></span><span class="t"><span class="str">        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5158" class="pln"><span class="n"><a href="#t5158">5158</a></span><span class="t"><span class="str">        for CPU tensor types and the current CUDA device for CUDA tensor types.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5159" class="pln"><span class="n"><a href="#t5159">5159</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5160" class="pln"><span class="n"><a href="#t5160">5160</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5161" class="pln"><span class="n"><a href="#t5161">5161</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5162" class="pln"><span class="n"><a href="#t5162">5162</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5163" class="pln"><span class="n"><a href="#t5163">5163</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5164" class="pln"><span class="n"><a href="#t5164">5164</a></span><span class="t"><span class="str">    >>> i = torch.tensor([[0, 1, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5165" class="pln"><span class="n"><a href="#t5165">5165</a></span><span class="t"><span class="str">                          [2, 0, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5166" class="pln"><span class="n"><a href="#t5166">5166</a></span><span class="t"><span class="str">    >>> v = torch.tensor([3, 4, 5], dtype=torch.float32)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5167" class="pln"><span class="n"><a href="#t5167">5167</a></span><span class="t"><span class="str">    >>> torch.sparse_coo_tensor(i, v, [2, 4])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5168" class="pln"><span class="n"><a href="#t5168">5168</a></span><span class="t"><span class="str">    tensor(indices=tensor([[0, 1, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5169" class="pln"><span class="n"><a href="#t5169">5169</a></span><span class="t"><span class="str">                           [2, 0, 2]]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5170" class="pln"><span class="n"><a href="#t5170">5170</a></span><span class="t"><span class="str">           values=tensor([3., 4., 5.]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5171" class="pln"><span class="n"><a href="#t5171">5171</a></span><span class="t"><span class="str">           size=(2, 4), nnz=3, layout=torch.sparse_coo)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5172" class="pln"><span class="n"><a href="#t5172">5172</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5173" class="pln"><span class="n"><a href="#t5173">5173</a></span><span class="t"><span class="str">    >>> torch.sparse_coo_tensor(i, v)  # Shape inference</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5174" class="pln"><span class="n"><a href="#t5174">5174</a></span><span class="t"><span class="str">    tensor(indices=tensor([[0, 1, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5175" class="pln"><span class="n"><a href="#t5175">5175</a></span><span class="t"><span class="str">                           [2, 0, 2]]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5176" class="pln"><span class="n"><a href="#t5176">5176</a></span><span class="t"><span class="str">           values=tensor([3., 4., 5.]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5177" class="pln"><span class="n"><a href="#t5177">5177</a></span><span class="t"><span class="str">           size=(2, 3), nnz=3, layout=torch.sparse_coo)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5178" class="pln"><span class="n"><a href="#t5178">5178</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5179" class="pln"><span class="n"><a href="#t5179">5179</a></span><span class="t"><span class="str">    >>> torch.sparse_coo_tensor(i, v, [2, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5180" class="pln"><span class="n"><a href="#t5180">5180</a></span><span class="t"><span class="str">                                dtype=torch.float64,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5181" class="pln"><span class="n"><a href="#t5181">5181</a></span><span class="t"><span class="str">                                device=torch.device('cuda:0'))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5182" class="pln"><span class="n"><a href="#t5182">5182</a></span><span class="t"><span class="str">    tensor(indices=tensor([[0, 1, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5183" class="pln"><span class="n"><a href="#t5183">5183</a></span><span class="t"><span class="str">                           [2, 0, 2]]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5184" class="pln"><span class="n"><a href="#t5184">5184</a></span><span class="t"><span class="str">           values=tensor([3., 4., 5.]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5185" class="pln"><span class="n"><a href="#t5185">5185</a></span><span class="t"><span class="str">           device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5186" class="pln"><span class="n"><a href="#t5186">5186</a></span><span class="t"><span class="str">           layout=torch.sparse_coo)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5187" class="pln"><span class="n"><a href="#t5187">5187</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5188" class="pln"><span class="n"><a href="#t5188">5188</a></span><span class="t"><span class="str">    # Create an empty sparse tensor with the following invariants:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5189" class="pln"><span class="n"><a href="#t5189">5189</a></span><span class="t"><span class="str">    #   1. sparse_dim + dense_dim = len(SparseTensor.shape)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5190" class="pln"><span class="n"><a href="#t5190">5190</a></span><span class="t"><span class="str">    #   2. SparseTensor._indices().shape = (sparse_dim, nnz)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5191" class="pln"><span class="n"><a href="#t5191">5191</a></span><span class="t"><span class="str">    #   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5192" class="pln"><span class="n"><a href="#t5192">5192</a></span><span class="t"><span class="str">    #</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5193" class="pln"><span class="n"><a href="#t5193">5193</a></span><span class="t"><span class="str">    # For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5194" class="pln"><span class="n"><a href="#t5194">5194</a></span><span class="t"><span class="str">    # sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5195" class="pln"><span class="n"><a href="#t5195">5195</a></span><span class="t"><span class="str">    >>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5196" class="pln"><span class="n"><a href="#t5196">5196</a></span><span class="t"><span class="str">    tensor(indices=tensor([], size=(1, 0)),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5197" class="pln"><span class="n"><a href="#t5197">5197</a></span><span class="t"><span class="str">           values=tensor([], size=(0,)),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5198" class="pln"><span class="n"><a href="#t5198">5198</a></span><span class="t"><span class="str">           size=(1,), nnz=0, layout=torch.sparse_coo)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5199" class="pln"><span class="n"><a href="#t5199">5199</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5200" class="pln"><span class="n"><a href="#t5200">5200</a></span><span class="t"><span class="str">    # and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5201" class="pln"><span class="n"><a href="#t5201">5201</a></span><span class="t"><span class="str">    # sparse_dim = 1</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5202" class="pln"><span class="n"><a href="#t5202">5202</a></span><span class="t"><span class="str">    >>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5203" class="pln"><span class="n"><a href="#t5203">5203</a></span><span class="t"><span class="str">    tensor(indices=tensor([], size=(1, 0)),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5204" class="pln"><span class="n"><a href="#t5204">5204</a></span><span class="t"><span class="str">           values=tensor([], size=(0, 2)),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5205" class="pln"><span class="n"><a href="#t5205">5205</a></span><span class="t"><span class="str">           size=(1, 2), nnz=0, layout=torch.sparse_coo)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5206" class="pln"><span class="n"><a href="#t5206">5206</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5207" class="pln"><span class="n"><a href="#t5207">5207</a></span><span class="t"><span class="str">.. _torch.sparse: https://pytorch.org/docs/stable/sparse.html</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5208" class="pln"><span class="n"><a href="#t5208">5208</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5209" class="pln"><span class="n"><a href="#t5209">5209</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5210" class="run"><span class="n"><a href="#t5210">5210</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sqrt</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5211" class="pln"><span class="n"><a href="#t5211">5211</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5212" class="pln"><span class="n"><a href="#t5212">5212</a></span><span class="t"><span class="str">sqrt(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5213" class="pln"><span class="n"><a href="#t5213">5213</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5214" class="pln"><span class="n"><a href="#t5214">5214</a></span><span class="t"><span class="str">Returns a new tensor with the square-root of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5215" class="pln"><span class="n"><a href="#t5215">5215</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5216" class="pln"><span class="n"><a href="#t5216">5216</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5217" class="pln"><span class="n"><a href="#t5217">5217</a></span><span class="t"><span class="str">    \text{out}_{i} = \sqrt{\text{input}_{i}}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5218" class="pln"><span class="n"><a href="#t5218">5218</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5219" class="pln"><span class="n"><a href="#t5219">5219</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5220" class="pln"><span class="n"><a href="#t5220">5220</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5221" class="pln"><span class="n"><a href="#t5221">5221</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5222" class="pln"><span class="n"><a href="#t5222">5222</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5223" class="pln"><span class="n"><a href="#t5223">5223</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5224" class="pln"><span class="n"><a href="#t5224">5224</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5225" class="pln"><span class="n"><a href="#t5225">5225</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5226" class="pln"><span class="n"><a href="#t5226">5226</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5227" class="pln"><span class="n"><a href="#t5227">5227</a></span><span class="t"><span class="str">    tensor([-2.0755,  1.0226,  0.0831,  0.4806])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5228" class="pln"><span class="n"><a href="#t5228">5228</a></span><span class="t"><span class="str">    >>> torch.sqrt(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5229" class="pln"><span class="n"><a href="#t5229">5229</a></span><span class="t"><span class="str">    tensor([    nan,  1.0112,  0.2883,  0.6933])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5230" class="pln"><span class="n"><a href="#t5230">5230</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5231" class="pln"><span class="n"><a href="#t5231">5231</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5232" class="run"><span class="n"><a href="#t5232">5232</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">square</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5233" class="pln"><span class="n"><a href="#t5233">5233</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5234" class="pln"><span class="n"><a href="#t5234">5234</a></span><span class="t"><span class="str">square(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5235" class="pln"><span class="n"><a href="#t5235">5235</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5236" class="pln"><span class="n"><a href="#t5236">5236</a></span><span class="t"><span class="str">Returns a new tensor with the square of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5237" class="pln"><span class="n"><a href="#t5237">5237</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5238" class="pln"><span class="n"><a href="#t5238">5238</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5239" class="pln"><span class="n"><a href="#t5239">5239</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5240" class="pln"><span class="n"><a href="#t5240">5240</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5241" class="pln"><span class="n"><a href="#t5241">5241</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5242" class="pln"><span class="n"><a href="#t5242">5242</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5243" class="pln"><span class="n"><a href="#t5243">5243</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5244" class="pln"><span class="n"><a href="#t5244">5244</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5245" class="pln"><span class="n"><a href="#t5245">5245</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5246" class="pln"><span class="n"><a href="#t5246">5246</a></span><span class="t"><span class="str">    tensor([-2.0755,  1.0226,  0.0831,  0.4806])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5247" class="pln"><span class="n"><a href="#t5247">5247</a></span><span class="t"><span class="str">    >>> torch.square(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5248" class="pln"><span class="n"><a href="#t5248">5248</a></span><span class="t"><span class="str">    tensor([ 4.3077,  1.0457,  0.0069,  0.2310])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5249" class="pln"><span class="n"><a href="#t5249">5249</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5250" class="pln"><span class="n"><a href="#t5250">5250</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5251" class="run"><span class="n"><a href="#t5251">5251</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">squeeze</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5252" class="pln"><span class="n"><a href="#t5252">5252</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5253" class="pln"><span class="n"><a href="#t5253">5253</a></span><span class="t"><span class="str">squeeze(input, dim=None, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5254" class="pln"><span class="n"><a href="#t5254">5254</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5255" class="pln"><span class="n"><a href="#t5255">5255</a></span><span class="t"><span class="str">Returns a tensor with all the dimensions of :attr:`input` of size `1` removed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5256" class="pln"><span class="n"><a href="#t5256">5256</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5257" class="pln"><span class="n"><a href="#t5257">5257</a></span><span class="t"><span class="str">For example, if `input` is of shape:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5258" class="pln"><span class="n"><a href="#t5258">5258</a></span><span class="t"><span class="str">:math:`(A \times 1 \times B \times C \times 1 \times D)` then the `out` tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5259" class="pln"><span class="n"><a href="#t5259">5259</a></span><span class="t"><span class="str">will be of shape: :math:`(A \times B \times C \times D)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5260" class="pln"><span class="n"><a href="#t5260">5260</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5261" class="pln"><span class="n"><a href="#t5261">5261</a></span><span class="t"><span class="str">When :attr:`dim` is given, a squeeze operation is done only in the given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5262" class="pln"><span class="n"><a href="#t5262">5262</a></span><span class="t"><span class="str">dimension. If `input` is of shape: :math:`(A \times 1 \times B)`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5263" class="pln"><span class="n"><a href="#t5263">5263</a></span><span class="t"><span class="str">``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5264" class="pln"><span class="n"><a href="#t5264">5264</a></span><span class="t"><span class="str">will squeeze the tensor to the shape :math:`(A \times B)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5265" class="pln"><span class="n"><a href="#t5265">5265</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5266" class="pln"><span class="n"><a href="#t5266">5266</a></span><span class="t"><span class="str">.. note:: The returned tensor shares the storage with the input tensor,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5267" class="pln"><span class="n"><a href="#t5267">5267</a></span><span class="t"><span class="str">          so changing the contents of one will change the contents of the other.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5268" class="pln"><span class="n"><a href="#t5268">5268</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5269" class="pln"><span class="n"><a href="#t5269">5269</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5270" class="pln"><span class="n"><a href="#t5270">5270</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5271" class="pln"><span class="n"><a href="#t5271">5271</a></span><span class="t"><span class="str">    dim (int, optional): if given, the input will be squeezed only in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5272" class="pln"><span class="n"><a href="#t5272">5272</a></span><span class="t"><span class="str">           this dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5273" class="pln"><span class="n"><a href="#t5273">5273</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5274" class="pln"><span class="n"><a href="#t5274">5274</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5275" class="pln"><span class="n"><a href="#t5275">5275</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5276" class="pln"><span class="n"><a href="#t5276">5276</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5277" class="pln"><span class="n"><a href="#t5277">5277</a></span><span class="t"><span class="str">    >>> x = torch.zeros(2, 1, 2, 1, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5278" class="pln"><span class="n"><a href="#t5278">5278</a></span><span class="t"><span class="str">    >>> x.size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5279" class="pln"><span class="n"><a href="#t5279">5279</a></span><span class="t"><span class="str">    torch.Size([2, 1, 2, 1, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5280" class="pln"><span class="n"><a href="#t5280">5280</a></span><span class="t"><span class="str">    >>> y = torch.squeeze(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5281" class="pln"><span class="n"><a href="#t5281">5281</a></span><span class="t"><span class="str">    >>> y.size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5282" class="pln"><span class="n"><a href="#t5282">5282</a></span><span class="t"><span class="str">    torch.Size([2, 2, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5283" class="pln"><span class="n"><a href="#t5283">5283</a></span><span class="t"><span class="str">    >>> y = torch.squeeze(x, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5284" class="pln"><span class="n"><a href="#t5284">5284</a></span><span class="t"><span class="str">    >>> y.size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5285" class="pln"><span class="n"><a href="#t5285">5285</a></span><span class="t"><span class="str">    torch.Size([2, 1, 2, 1, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5286" class="pln"><span class="n"><a href="#t5286">5286</a></span><span class="t"><span class="str">    >>> y = torch.squeeze(x, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5287" class="pln"><span class="n"><a href="#t5287">5287</a></span><span class="t"><span class="str">    >>> y.size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5288" class="pln"><span class="n"><a href="#t5288">5288</a></span><span class="t"><span class="str">    torch.Size([2, 2, 1, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5289" class="pln"><span class="n"><a href="#t5289">5289</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5290" class="pln"><span class="n"><a href="#t5290">5290</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5291" class="run"><span class="n"><a href="#t5291">5291</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">std</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5292" class="pln"><span class="n"><a href="#t5292">5292</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5293" class="pln"><span class="n"><a href="#t5293">5293</a></span><span class="t"><span class="str">.. function:: std(input, unbiased=True) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5294" class="pln"><span class="n"><a href="#t5294">5294</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5295" class="pln"><span class="n"><a href="#t5295">5295</a></span><span class="t"><span class="str">Returns the standard-deviation of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5296" class="pln"><span class="n"><a href="#t5296">5296</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5297" class="pln"><span class="n"><a href="#t5297">5297</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the standard-deviation will be calculated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5298" class="pln"><span class="n"><a href="#t5298">5298</a></span><span class="t"><span class="str">via the biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5299" class="pln"><span class="n"><a href="#t5299">5299</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5300" class="pln"><span class="n"><a href="#t5300">5300</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5301" class="pln"><span class="n"><a href="#t5301">5301</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5302" class="pln"><span class="n"><a href="#t5302">5302</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5303" class="pln"><span class="n"><a href="#t5303">5303</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5304" class="pln"><span class="n"><a href="#t5304">5304</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5305" class="pln"><span class="n"><a href="#t5305">5305</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5306" class="pln"><span class="n"><a href="#t5306">5306</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5307" class="pln"><span class="n"><a href="#t5307">5307</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5308" class="pln"><span class="n"><a href="#t5308">5308</a></span><span class="t"><span class="str">    tensor([[-0.8166, -1.3802, -0.3560]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5309" class="pln"><span class="n"><a href="#t5309">5309</a></span><span class="t"><span class="str">    >>> torch.std(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5310" class="pln"><span class="n"><a href="#t5310">5310</a></span><span class="t"><span class="str">    tensor(0.5130)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5311" class="pln"><span class="n"><a href="#t5311">5311</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5312" class="pln"><span class="n"><a href="#t5312">5312</a></span><span class="t"><span class="str">.. function:: std(input, dim, unbiased=True, keepdim=False, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5313" class="pln"><span class="n"><a href="#t5313">5313</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5314" class="pln"><span class="n"><a href="#t5314">5314</a></span><span class="t"><span class="str">Returns the standard-deviation of each row of the :attr:`input` tensor in the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5315" class="pln"><span class="n"><a href="#t5315">5315</a></span><span class="t"><span class="str">dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5316" class="pln"><span class="n"><a href="#t5316">5316</a></span><span class="t"><span class="str">reduce over all of them.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5317" class="pln"><span class="n"><a href="#t5317">5317</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5318" class="pln"><span class="n"><a href="#t5318">5318</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5319" class="pln"><span class="n"><a href="#t5319">5319</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5320" class="pln"><span class="n"><a href="#t5320">5320</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the standard-deviation will be calculated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5321" class="pln"><span class="n"><a href="#t5321">5321</a></span><span class="t"><span class="str">via the biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5322" class="pln"><span class="n"><a href="#t5322">5322</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5323" class="pln"><span class="n"><a href="#t5323">5323</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5324" class="pln"><span class="n"><a href="#t5324">5324</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5325" class="pln"><span class="n"><a href="#t5325">5325</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5326" class="pln"><span class="n"><a href="#t5326">5326</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5327" class="pln"><span class="n"><a href="#t5327">5327</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5328" class="pln"><span class="n"><a href="#t5328">5328</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5329" class="pln"><span class="n"><a href="#t5329">5329</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5330" class="pln"><span class="n"><a href="#t5330">5330</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5331" class="pln"><span class="n"><a href="#t5331">5331</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5332" class="pln"><span class="n"><a href="#t5332">5332</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5333" class="pln"><span class="n"><a href="#t5333">5333</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5334" class="pln"><span class="n"><a href="#t5334">5334</a></span><span class="t"><span class="str">    tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5335" class="pln"><span class="n"><a href="#t5335">5335</a></span><span class="t"><span class="str">            [ 1.5027, -0.3270,  0.5905,  0.6538],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5336" class="pln"><span class="n"><a href="#t5336">5336</a></span><span class="t"><span class="str">            [-1.5745,  1.3330, -0.5596, -0.6548],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5337" class="pln"><span class="n"><a href="#t5337">5337</a></span><span class="t"><span class="str">            [ 0.1264, -0.5080,  1.6420,  0.1992]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5338" class="pln"><span class="n"><a href="#t5338">5338</a></span><span class="t"><span class="str">    >>> torch.std(a, dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5339" class="pln"><span class="n"><a href="#t5339">5339</a></span><span class="t"><span class="str">    tensor([ 1.0311,  0.7477,  1.2204,  0.9087])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5340" class="pln"><span class="n"><a href="#t5340">5340</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5341" class="pln"><span class="n"><a href="#t5341">5341</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5342" class="run"><span class="n"><a href="#t5342">5342</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">std_mean</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5343" class="pln"><span class="n"><a href="#t5343">5343</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5344" class="pln"><span class="n"><a href="#t5344">5344</a></span><span class="t"><span class="str">.. function:: std_mean(input, unbiased=True) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5345" class="pln"><span class="n"><a href="#t5345">5345</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5346" class="pln"><span class="n"><a href="#t5346">5346</a></span><span class="t"><span class="str">Returns the standard-deviation and mean of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5347" class="pln"><span class="n"><a href="#t5347">5347</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5348" class="pln"><span class="n"><a href="#t5348">5348</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the standard-deviation will be calculated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5349" class="pln"><span class="n"><a href="#t5349">5349</a></span><span class="t"><span class="str">via the biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5350" class="pln"><span class="n"><a href="#t5350">5350</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5351" class="pln"><span class="n"><a href="#t5351">5351</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5352" class="pln"><span class="n"><a href="#t5352">5352</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5353" class="pln"><span class="n"><a href="#t5353">5353</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5354" class="pln"><span class="n"><a href="#t5354">5354</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5355" class="pln"><span class="n"><a href="#t5355">5355</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5356" class="pln"><span class="n"><a href="#t5356">5356</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5357" class="pln"><span class="n"><a href="#t5357">5357</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5358" class="pln"><span class="n"><a href="#t5358">5358</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5359" class="pln"><span class="n"><a href="#t5359">5359</a></span><span class="t"><span class="str">    tensor([[0.3364, 0.3591, 0.9462]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5360" class="pln"><span class="n"><a href="#t5360">5360</a></span><span class="t"><span class="str">    >>> torch.std_mean(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5361" class="pln"><span class="n"><a href="#t5361">5361</a></span><span class="t"><span class="str">    (tensor(0.3457), tensor(0.5472))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5362" class="pln"><span class="n"><a href="#t5362">5362</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5363" class="pln"><span class="n"><a href="#t5363">5363</a></span><span class="t"><span class="str">.. function:: std_mean(input, dim, unbiased=True, keepdim=False) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5364" class="pln"><span class="n"><a href="#t5364">5364</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5365" class="pln"><span class="n"><a href="#t5365">5365</a></span><span class="t"><span class="str">Returns the standard-deviation and mean of each row of the :attr:`input` tensor in the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5366" class="pln"><span class="n"><a href="#t5366">5366</a></span><span class="t"><span class="str">dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5367" class="pln"><span class="n"><a href="#t5367">5367</a></span><span class="t"><span class="str">reduce over all of them.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5368" class="pln"><span class="n"><a href="#t5368">5368</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5369" class="pln"><span class="n"><a href="#t5369">5369</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5370" class="pln"><span class="n"><a href="#t5370">5370</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5371" class="pln"><span class="n"><a href="#t5371">5371</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the standard-deviation will be calculated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5372" class="pln"><span class="n"><a href="#t5372">5372</a></span><span class="t"><span class="str">via the biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5373" class="pln"><span class="n"><a href="#t5373">5373</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5374" class="pln"><span class="n"><a href="#t5374">5374</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5375" class="pln"><span class="n"><a href="#t5375">5375</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5376" class="pln"><span class="n"><a href="#t5376">5376</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5377" class="pln"><span class="n"><a href="#t5377">5377</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5378" class="pln"><span class="n"><a href="#t5378">5378</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5379" class="pln"><span class="n"><a href="#t5379">5379</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5380" class="pln"><span class="n"><a href="#t5380">5380</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5381" class="pln"><span class="n"><a href="#t5381">5381</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5382" class="pln"><span class="n"><a href="#t5382">5382</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5383" class="pln"><span class="n"><a href="#t5383">5383</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5384" class="pln"><span class="n"><a href="#t5384">5384</a></span><span class="t"><span class="str">    tensor([[ 0.5648, -0.5984, -1.2676, -1.4471],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5385" class="pln"><span class="n"><a href="#t5385">5385</a></span><span class="t"><span class="str">            [ 0.9267,  1.0612,  1.1050, -0.6014],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5386" class="pln"><span class="n"><a href="#t5386">5386</a></span><span class="t"><span class="str">            [ 0.0154,  1.9301,  0.0125, -1.0904],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5387" class="pln"><span class="n"><a href="#t5387">5387</a></span><span class="t"><span class="str">            [-1.9711, -0.7748, -1.3840,  0.5067]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5388" class="pln"><span class="n"><a href="#t5388">5388</a></span><span class="t"><span class="str">    >>> torch.std_mean(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5389" class="pln"><span class="n"><a href="#t5389">5389</a></span><span class="t"><span class="str">    (tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5390" class="pln"><span class="n"><a href="#t5390">5390</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5391" class="pln"><span class="n"><a href="#t5391">5391</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5392" class="run"><span class="n"><a href="#t5392">5392</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">sum</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5393" class="pln"><span class="n"><a href="#t5393">5393</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5394" class="pln"><span class="n"><a href="#t5394">5394</a></span><span class="t"><span class="str">.. function:: sum(input, dtype=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5395" class="pln"><span class="n"><a href="#t5395">5395</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5396" class="pln"><span class="n"><a href="#t5396">5396</a></span><span class="t"><span class="str">Returns the sum of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5397" class="pln"><span class="n"><a href="#t5397">5397</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5398" class="pln"><span class="n"><a href="#t5398">5398</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5399" class="pln"><span class="n"><a href="#t5399">5399</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5400" class="pln"><span class="n"><a href="#t5400">5400</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5401" class="pln"><span class="n"><a href="#t5401">5401</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5402" class="pln"><span class="n"><a href="#t5402">5402</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5403" class="pln"><span class="n"><a href="#t5403">5403</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5404" class="pln"><span class="n"><a href="#t5404">5404</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5405" class="pln"><span class="n"><a href="#t5405">5405</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5406" class="pln"><span class="n"><a href="#t5406">5406</a></span><span class="t"><span class="str">    tensor([[ 0.1133, -0.9567,  0.2958]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5407" class="pln"><span class="n"><a href="#t5407">5407</a></span><span class="t"><span class="str">    >>> torch.sum(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5408" class="pln"><span class="n"><a href="#t5408">5408</a></span><span class="t"><span class="str">    tensor(-0.5475)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5409" class="pln"><span class="n"><a href="#t5409">5409</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5410" class="pln"><span class="n"><a href="#t5410">5410</a></span><span class="t"><span class="str">.. function:: sum(input, dim, keepdim=False, dtype=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5411" class="pln"><span class="n"><a href="#t5411">5411</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5412" class="pln"><span class="n"><a href="#t5412">5412</a></span><span class="t"><span class="str">Returns the sum of each row of the :attr:`input` tensor in the given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5413" class="pln"><span class="n"><a href="#t5413">5413</a></span><span class="t"><span class="str">dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5414" class="pln"><span class="n"><a href="#t5414">5414</a></span><span class="t"><span class="str">reduce over all of them.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5415" class="pln"><span class="n"><a href="#t5415">5415</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5416" class="pln"><span class="n"><a href="#t5416">5416</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5417" class="pln"><span class="n"><a href="#t5417">5417</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5418" class="pln"><span class="n"><a href="#t5418">5418</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5419" class="pln"><span class="n"><a href="#t5419">5419</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5420" class="pln"><span class="n"><a href="#t5420">5420</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5421" class="pln"><span class="n"><a href="#t5421">5421</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5422" class="pln"><span class="n"><a href="#t5422">5422</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5423" class="pln"><span class="n"><a href="#t5423">5423</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5424" class="pln"><span class="n"><a href="#t5424">5424</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5425" class="pln"><span class="n"><a href="#t5425">5425</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5426" class="pln"><span class="n"><a href="#t5426">5426</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5427" class="pln"><span class="n"><a href="#t5427">5427</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5428" class="pln"><span class="n"><a href="#t5428">5428</a></span><span class="t"><span class="str">    tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5429" class="pln"><span class="n"><a href="#t5429">5429</a></span><span class="t"><span class="str">            [-0.2993,  0.9138,  0.9337, -1.6864],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5430" class="pln"><span class="n"><a href="#t5430">5430</a></span><span class="t"><span class="str">            [ 0.1132,  0.7892, -0.1003,  0.5688],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5431" class="pln"><span class="n"><a href="#t5431">5431</a></span><span class="t"><span class="str">            [ 0.3637, -0.9906, -0.4752, -1.5197]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5432" class="pln"><span class="n"><a href="#t5432">5432</a></span><span class="t"><span class="str">    >>> torch.sum(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5433" class="pln"><span class="n"><a href="#t5433">5433</a></span><span class="t"><span class="str">    tensor([-0.4598, -0.1381,  1.3708, -2.6217])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5434" class="pln"><span class="n"><a href="#t5434">5434</a></span><span class="t"><span class="str">    >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5435" class="pln"><span class="n"><a href="#t5435">5435</a></span><span class="t"><span class="str">    >>> torch.sum(b, (2, 1))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5436" class="pln"><span class="n"><a href="#t5436">5436</a></span><span class="t"><span class="str">    tensor([  435.,  1335.,  2235.,  3135.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5437" class="pln"><span class="n"><a href="#t5437">5437</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5438" class="pln"><span class="n"><a href="#t5438">5438</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5439" class="run"><span class="n"><a href="#t5439">5439</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">svd</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5440" class="pln"><span class="n"><a href="#t5440">5440</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5441" class="pln"><span class="n"><a href="#t5441">5441</a></span><span class="t"><span class="str">svd(input, some=True, compute_uv=True, out=None) -> (Tensor, Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5442" class="pln"><span class="n"><a href="#t5442">5442</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5443" class="pln"><span class="n"><a href="#t5443">5443</a></span><span class="t"><span class="str">This function returns a namedtuple ``(U, S, V)`` which is the singular value</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5444" class="pln"><span class="n"><a href="#t5444">5444</a></span><span class="t"><span class="str">decomposition of a input real matrix or batches of real matrices :attr:`input` such that</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5445" class="pln"><span class="n"><a href="#t5445">5445</a></span><span class="t"><span class="str">:math:`input = U \times diag(S) \times V^T`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5446" class="pln"><span class="n"><a href="#t5446">5446</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5447" class="pln"><span class="n"><a href="#t5447">5447</a></span><span class="t"><span class="str">If :attr:`some` is ``True`` (default), the method returns the reduced singular value decomposition</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5448" class="pln"><span class="n"><a href="#t5448">5448</a></span><span class="t"><span class="str">i.e., if the last two dimensions of :attr:`input` are ``m`` and ``n``, then the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5449" class="pln"><span class="n"><a href="#t5449">5449</a></span><span class="t"><span class="str">`U` and `V` matrices will contain only :math:`min(n, m)` orthonormal columns.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5450" class="pln"><span class="n"><a href="#t5450">5450</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5451" class="pln"><span class="n"><a href="#t5451">5451</a></span><span class="t"><span class="str">If :attr:`compute_uv` is ``False``, the returned `U` and `V` matrices will be zero matrices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5452" class="pln"><span class="n"><a href="#t5452">5452</a></span><span class="t"><span class="str">of shape :math:`(m \times m)` and :math:`(n \times n)` respectively. :attr:`some` will be ignored here.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5453" class="pln"><span class="n"><a href="#t5453">5453</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5454" class="pln"><span class="n"><a href="#t5454">5454</a></span><span class="t"><span class="str">.. note:: The singular values are returned in descending order. If :attr:`input` is a batch of matrices,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5455" class="pln"><span class="n"><a href="#t5455">5455</a></span><span class="t"><span class="str">          then the singular values of each matrix in the batch is returned in descending order.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5456" class="pln"><span class="n"><a href="#t5456">5456</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5457" class="pln"><span class="n"><a href="#t5457">5457</a></span><span class="t"><span class="str">.. note:: The implementation of SVD on CPU uses the LAPACK routine `?gesdd` (a divide-and-conquer</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5458" class="pln"><span class="n"><a href="#t5458">5458</a></span><span class="t"><span class="str">          algorithm) instead of `?gesvd` for speed. Analogously, the SVD on GPU uses the MAGMA routine</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5459" class="pln"><span class="n"><a href="#t5459">5459</a></span><span class="t"><span class="str">          `gesdd` as well.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5460" class="pln"><span class="n"><a href="#t5460">5460</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5461" class="pln"><span class="n"><a href="#t5461">5461</a></span><span class="t"><span class="str">.. note:: Irrespective of the original strides, the returned matrix `U`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5462" class="pln"><span class="n"><a href="#t5462">5462</a></span><span class="t"><span class="str">          will be transposed, i.e. with strides :code:`U.contiguous().transpose(-2, -1).stride()`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5463" class="pln"><span class="n"><a href="#t5463">5463</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5464" class="pln"><span class="n"><a href="#t5464">5464</a></span><span class="t"><span class="str">.. note:: Extra care needs to be taken when backward through `U` and `V`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5465" class="pln"><span class="n"><a href="#t5465">5465</a></span><span class="t"><span class="str">          outputs. Such operation is really only stable when :attr:`input` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5466" class="pln"><span class="n"><a href="#t5466">5466</a></span><span class="t"><span class="str">          full rank with all distinct singular values. Otherwise, ``NaN`` can</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5467" class="pln"><span class="n"><a href="#t5467">5467</a></span><span class="t"><span class="str">          appear as the gradients are not properly defined. Also, notice that</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5468" class="pln"><span class="n"><a href="#t5468">5468</a></span><span class="t"><span class="str">          double backward will usually do an additional backward through `U` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5469" class="pln"><span class="n"><a href="#t5469">5469</a></span><span class="t"><span class="str">          `V` even if the original backward is only on `S`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5470" class="pln"><span class="n"><a href="#t5470">5470</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5471" class="pln"><span class="n"><a href="#t5471">5471</a></span><span class="t"><span class="str">.. note:: When :attr:`some` = ``False``, the gradients on :code:`U[..., :, min(m, n):]`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5472" class="pln"><span class="n"><a href="#t5472">5472</a></span><span class="t"><span class="str">          and :code:`V[..., :, min(m, n):]` will be ignored in backward as those vectors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5473" class="pln"><span class="n"><a href="#t5473">5473</a></span><span class="t"><span class="str">          can be arbitrary bases of the subspaces.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5474" class="pln"><span class="n"><a href="#t5474">5474</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5475" class="pln"><span class="n"><a href="#t5475">5475</a></span><span class="t"><span class="str">.. note:: When :attr:`compute_uv` = ``False``, backward cannot be performed since `U` and `V`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5476" class="pln"><span class="n"><a href="#t5476">5476</a></span><span class="t"><span class="str">          from the forward pass is required for the backward operation.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5477" class="pln"><span class="n"><a href="#t5477">5477</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5478" class="pln"><span class="n"><a href="#t5478">5478</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5479" class="pln"><span class="n"><a href="#t5479">5479</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size :math:`(*, m, n)` where `*` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5480" class="pln"><span class="n"><a href="#t5480">5480</a></span><span class="t"><span class="str">                    batch dimensions consisting of :math:`m \times n` matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5481" class="pln"><span class="n"><a href="#t5481">5481</a></span><span class="t"><span class="str">    some (bool, optional): controls the shape of returned `U` and `V`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5482" class="pln"><span class="n"><a href="#t5482">5482</a></span><span class="t"><span class="str">    compute_uv (bool, optional): option whether to compute `U` and `V` or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5483" class="pln"><span class="n"><a href="#t5483">5483</a></span><span class="t"><span class="str">    out (tuple, optional): the output tuple of tensors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5484" class="pln"><span class="n"><a href="#t5484">5484</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5485" class="pln"><span class="n"><a href="#t5485">5485</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5486" class="pln"><span class="n"><a href="#t5486">5486</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5487" class="pln"><span class="n"><a href="#t5487">5487</a></span><span class="t"><span class="str">    >>> a = torch.randn(5, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5488" class="pln"><span class="n"><a href="#t5488">5488</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5489" class="pln"><span class="n"><a href="#t5489">5489</a></span><span class="t"><span class="str">    tensor([[ 0.2364, -0.7752,  0.6372],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5490" class="pln"><span class="n"><a href="#t5490">5490</a></span><span class="t"><span class="str">            [ 1.7201,  0.7394, -0.0504],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5491" class="pln"><span class="n"><a href="#t5491">5491</a></span><span class="t"><span class="str">            [-0.3371, -1.0584,  0.5296],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5492" class="pln"><span class="n"><a href="#t5492">5492</a></span><span class="t"><span class="str">            [ 0.3550, -0.4022,  1.5569],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5493" class="pln"><span class="n"><a href="#t5493">5493</a></span><span class="t"><span class="str">            [ 0.2445, -0.0158,  1.1414]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5494" class="pln"><span class="n"><a href="#t5494">5494</a></span><span class="t"><span class="str">    >>> u, s, v = torch.svd(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5495" class="pln"><span class="n"><a href="#t5495">5495</a></span><span class="t"><span class="str">    >>> u</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5496" class="pln"><span class="n"><a href="#t5496">5496</a></span><span class="t"><span class="str">    tensor([[ 0.4027,  0.0287,  0.5434],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5497" class="pln"><span class="n"><a href="#t5497">5497</a></span><span class="t"><span class="str">            [-0.1946,  0.8833,  0.3679],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5498" class="pln"><span class="n"><a href="#t5498">5498</a></span><span class="t"><span class="str">            [ 0.4296, -0.2890,  0.5261],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5499" class="pln"><span class="n"><a href="#t5499">5499</a></span><span class="t"><span class="str">            [ 0.6604,  0.2717, -0.2618],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5500" class="pln"><span class="n"><a href="#t5500">5500</a></span><span class="t"><span class="str">            [ 0.4234,  0.2481, -0.4733]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5501" class="pln"><span class="n"><a href="#t5501">5501</a></span><span class="t"><span class="str">    >>> s</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5502" class="pln"><span class="n"><a href="#t5502">5502</a></span><span class="t"><span class="str">    tensor([2.3289, 2.0315, 0.7806])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5503" class="pln"><span class="n"><a href="#t5503">5503</a></span><span class="t"><span class="str">    >>> v</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5504" class="pln"><span class="n"><a href="#t5504">5504</a></span><span class="t"><span class="str">    tensor([[-0.0199,  0.8766,  0.4809],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5505" class="pln"><span class="n"><a href="#t5505">5505</a></span><span class="t"><span class="str">            [-0.5080,  0.4054, -0.7600],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5506" class="pln"><span class="n"><a href="#t5506">5506</a></span><span class="t"><span class="str">            [ 0.8611,  0.2594, -0.4373]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5507" class="pln"><span class="n"><a href="#t5507">5507</a></span><span class="t"><span class="str">    >>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5508" class="pln"><span class="n"><a href="#t5508">5508</a></span><span class="t"><span class="str">    tensor(8.6531e-07)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5509" class="pln"><span class="n"><a href="#t5509">5509</a></span><span class="t"><span class="str">    >>> a_big = torch.randn(7, 5, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5510" class="pln"><span class="n"><a href="#t5510">5510</a></span><span class="t"><span class="str">    >>> u, s, v = torch.svd(a_big)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5511" class="pln"><span class="n"><a href="#t5511">5511</a></span><span class="t"><span class="str">    >>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5512" class="pln"><span class="n"><a href="#t5512">5512</a></span><span class="t"><span class="str">    tensor(2.6503e-06)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5513" class="pln"><span class="n"><a href="#t5513">5513</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5514" class="pln"><span class="n"><a href="#t5514">5514</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5515" class="run"><span class="n"><a href="#t5515">5515</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">symeig</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5516" class="pln"><span class="n"><a href="#t5516">5516</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5517" class="pln"><span class="n"><a href="#t5517">5517</a></span><span class="t"><span class="str">symeig(input, eigenvectors=False, upper=True, out=None) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5518" class="pln"><span class="n"><a href="#t5518">5518</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5519" class="pln"><span class="n"><a href="#t5519">5519</a></span><span class="t"><span class="str">This function returns eigenvalues and eigenvectors</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5520" class="pln"><span class="n"><a href="#t5520">5520</a></span><span class="t"><span class="str">of a real symmetric matrix :attr:`input` or a batch of real symmetric matrices,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5521" class="pln"><span class="n"><a href="#t5521">5521</a></span><span class="t"><span class="str">represented by a namedtuple (eigenvalues, eigenvectors).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5522" class="pln"><span class="n"><a href="#t5522">5522</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5523" class="pln"><span class="n"><a href="#t5523">5523</a></span><span class="t"><span class="str">This function calculates all eigenvalues (and vectors) of :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5524" class="pln"><span class="n"><a href="#t5524">5524</a></span><span class="t"><span class="str">such that :math:`\text{input} = V \text{diag}(e) V^T`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5525" class="pln"><span class="n"><a href="#t5525">5525</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5526" class="pln"><span class="n"><a href="#t5526">5526</a></span><span class="t"><span class="str">The boolean argument :attr:`eigenvectors` defines computation of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5527" class="pln"><span class="n"><a href="#t5527">5527</a></span><span class="t"><span class="str">both eigenvectors and eigenvalues or eigenvalues only.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5528" class="pln"><span class="n"><a href="#t5528">5528</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5529" class="pln"><span class="n"><a href="#t5529">5529</a></span><span class="t"><span class="str">If it is ``False``, only eigenvalues are computed. If it is ``True``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5530" class="pln"><span class="n"><a href="#t5530">5530</a></span><span class="t"><span class="str">both eigenvalues and eigenvectors are computed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5531" class="pln"><span class="n"><a href="#t5531">5531</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5532" class="pln"><span class="n"><a href="#t5532">5532</a></span><span class="t"><span class="str">Since the input matrix :attr:`input` is supposed to be symmetric,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5533" class="pln"><span class="n"><a href="#t5533">5533</a></span><span class="t"><span class="str">only the upper triangular portion is used by default.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5534" class="pln"><span class="n"><a href="#t5534">5534</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5535" class="pln"><span class="n"><a href="#t5535">5535</a></span><span class="t"><span class="str">If :attr:`upper` is ``False``, then lower triangular portion is used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5536" class="pln"><span class="n"><a href="#t5536">5536</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5537" class="pln"><span class="n"><a href="#t5537">5537</a></span><span class="t"><span class="str">.. note:: The eigenvalues are returned in ascending order. If :attr:`input` is a batch of matrices,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5538" class="pln"><span class="n"><a href="#t5538">5538</a></span><span class="t"><span class="str">          then the eigenvalues of each matrix in the batch is returned in ascending order.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5539" class="pln"><span class="n"><a href="#t5539">5539</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5540" class="pln"><span class="n"><a href="#t5540">5540</a></span><span class="t"><span class="str">.. note:: Irrespective of the original strides, the returned matrix `V` will</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5541" class="pln"><span class="n"><a href="#t5541">5541</a></span><span class="t"><span class="str">          be transposed, i.e. with strides `V.contiguous().transpose(-1, -2).stride()`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5542" class="pln"><span class="n"><a href="#t5542">5542</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5543" class="pln"><span class="n"><a href="#t5543">5543</a></span><span class="t"><span class="str">.. note:: Extra care needs to be taken when backward through outputs. Such</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5544" class="pln"><span class="n"><a href="#t5544">5544</a></span><span class="t"><span class="str">          operation is really only stable when all eigenvalues are distinct.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5545" class="pln"><span class="n"><a href="#t5545">5545</a></span><span class="t"><span class="str">          Otherwise, ``NaN`` can appear as the gradients are not properly defined.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5546" class="pln"><span class="n"><a href="#t5546">5546</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5547" class="pln"><span class="n"><a href="#t5547">5547</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5548" class="pln"><span class="n"><a href="#t5548">5548</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size :math:`(*, n, n)` where `*` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5549" class="pln"><span class="n"><a href="#t5549">5549</a></span><span class="t"><span class="str">                    batch dimensions consisting of symmetric matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5550" class="pln"><span class="n"><a href="#t5550">5550</a></span><span class="t"><span class="str">    eigenvectors(boolean, optional): controls whether eigenvectors have to be computed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5551" class="pln"><span class="n"><a href="#t5551">5551</a></span><span class="t"><span class="str">    upper(boolean, optional): controls whether to consider upper-triangular or lower-triangular region</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5552" class="pln"><span class="n"><a href="#t5552">5552</a></span><span class="t"><span class="str">    out (tuple, optional): the output tuple of (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5553" class="pln"><span class="n"><a href="#t5553">5553</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5554" class="pln"><span class="n"><a href="#t5554">5554</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5555" class="pln"><span class="n"><a href="#t5555">5555</a></span><span class="t"><span class="str">    (Tensor, Tensor): A namedtuple (eigenvalues, eigenvectors) containing</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5556" class="pln"><span class="n"><a href="#t5556">5556</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5557" class="pln"><span class="n"><a href="#t5557">5557</a></span><span class="t"><span class="str">        - **eigenvalues** (*Tensor*): Shape :math:`(*, m)`. The eigenvalues in ascending order.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5558" class="pln"><span class="n"><a href="#t5558">5558</a></span><span class="t"><span class="str">        - **eigenvectors** (*Tensor*): Shape :math:`(*, m, m)`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5559" class="pln"><span class="n"><a href="#t5559">5559</a></span><span class="t"><span class="str">          If ``eigenvectors=False``, it's an empty tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5560" class="pln"><span class="n"><a href="#t5560">5560</a></span><span class="t"><span class="str">          Otherwise, this tensor contains the orthonormal eigenvectors of the ``input``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5561" class="pln"><span class="n"><a href="#t5561">5561</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5562" class="pln"><span class="n"><a href="#t5562">5562</a></span><span class="t"><span class="str">Examples::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5563" class="pln"><span class="n"><a href="#t5563">5563</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5564" class="pln"><span class="n"><a href="#t5564">5564</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5565" class="pln"><span class="n"><a href="#t5565">5565</a></span><span class="t"><span class="str">    >>> a = torch.randn(5, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5566" class="pln"><span class="n"><a href="#t5566">5566</a></span><span class="t"><span class="str">    >>> a = a + a.t()  # To make a symmetric</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5567" class="pln"><span class="n"><a href="#t5567">5567</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5568" class="pln"><span class="n"><a href="#t5568">5568</a></span><span class="t"><span class="str">    tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5569" class="pln"><span class="n"><a href="#t5569">5569</a></span><span class="t"><span class="str">            [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5570" class="pln"><span class="n"><a href="#t5570">5570</a></span><span class="t"><span class="str">            [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5571" class="pln"><span class="n"><a href="#t5571">5571</a></span><span class="t"><span class="str">            [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5572" class="pln"><span class="n"><a href="#t5572">5572</a></span><span class="t"><span class="str">            [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5573" class="pln"><span class="n"><a href="#t5573">5573</a></span><span class="t"><span class="str">    >>> e, v = torch.symeig(a, eigenvectors=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5574" class="pln"><span class="n"><a href="#t5574">5574</a></span><span class="t"><span class="str">    >>> e</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5575" class="pln"><span class="n"><a href="#t5575">5575</a></span><span class="t"><span class="str">    tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5576" class="pln"><span class="n"><a href="#t5576">5576</a></span><span class="t"><span class="str">    >>> v</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5577" class="pln"><span class="n"><a href="#t5577">5577</a></span><span class="t"><span class="str">    tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5578" class="pln"><span class="n"><a href="#t5578">5578</a></span><span class="t"><span class="str">            [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5579" class="pln"><span class="n"><a href="#t5579">5579</a></span><span class="t"><span class="str">            [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5580" class="pln"><span class="n"><a href="#t5580">5580</a></span><span class="t"><span class="str">            [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5581" class="pln"><span class="n"><a href="#t5581">5581</a></span><span class="t"><span class="str">            [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5582" class="pln"><span class="n"><a href="#t5582">5582</a></span><span class="t"><span class="str">    >>> a_big = torch.randn(5, 2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5583" class="pln"><span class="n"><a href="#t5583">5583</a></span><span class="t"><span class="str">    >>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5584" class="pln"><span class="n"><a href="#t5584">5584</a></span><span class="t"><span class="str">    >>> e, v = a_big.symeig(eigenvectors=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5585" class="pln"><span class="n"><a href="#t5585">5585</a></span><span class="t"><span class="str">    >>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5586" class="pln"><span class="n"><a href="#t5586">5586</a></span><span class="t"><span class="str">    True</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5587" class="pln"><span class="n"><a href="#t5587">5587</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5588" class="pln"><span class="n"><a href="#t5588">5588</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5589" class="run"><span class="n"><a href="#t5589">5589</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">t</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5590" class="pln"><span class="n"><a href="#t5590">5590</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5591" class="pln"><span class="n"><a href="#t5591">5591</a></span><span class="t"><span class="str">t(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5592" class="pln"><span class="n"><a href="#t5592">5592</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5593" class="pln"><span class="n"><a href="#t5593">5593</a></span><span class="t"><span class="str">Expects :attr:`input` to be &lt;= 2-D tensor and transposes dimensions 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5594" class="pln"><span class="n"><a href="#t5594">5594</a></span><span class="t"><span class="str">and 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5595" class="pln"><span class="n"><a href="#t5595">5595</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5596" class="pln"><span class="n"><a href="#t5596">5596</a></span><span class="t"><span class="str">0-D and 1-D tensors are returned as is. When input is a 2-D tensor this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5597" class="pln"><span class="n"><a href="#t5597">5597</a></span><span class="t"><span class="str">is equivalent to ``transpose(input, 0, 1)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5598" class="pln"><span class="n"><a href="#t5598">5598</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5599" class="pln"><span class="n"><a href="#t5599">5599</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5600" class="pln"><span class="n"><a href="#t5600">5600</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5601" class="pln"><span class="n"><a href="#t5601">5601</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5602" class="pln"><span class="n"><a href="#t5602">5602</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5603" class="pln"><span class="n"><a href="#t5603">5603</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5604" class="pln"><span class="n"><a href="#t5604">5604</a></span><span class="t"><span class="str">    >>> x = torch.randn(())</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5605" class="pln"><span class="n"><a href="#t5605">5605</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5606" class="pln"><span class="n"><a href="#t5606">5606</a></span><span class="t"><span class="str">    tensor(0.1995)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5607" class="pln"><span class="n"><a href="#t5607">5607</a></span><span class="t"><span class="str">    >>> torch.t(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5608" class="pln"><span class="n"><a href="#t5608">5608</a></span><span class="t"><span class="str">    tensor(0.1995)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5609" class="pln"><span class="n"><a href="#t5609">5609</a></span><span class="t"><span class="str">    >>> x = torch.randn(3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5610" class="pln"><span class="n"><a href="#t5610">5610</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5611" class="pln"><span class="n"><a href="#t5611">5611</a></span><span class="t"><span class="str">    tensor([ 2.4320, -0.4608,  0.7702])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5612" class="pln"><span class="n"><a href="#t5612">5612</a></span><span class="t"><span class="str">    >>> torch.t(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5613" class="pln"><span class="n"><a href="#t5613">5613</a></span><span class="t"><span class="str">    tensor([.2.4320,.-0.4608,..0.7702])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5614" class="pln"><span class="n"><a href="#t5614">5614</a></span><span class="t"><span class="str">    >>> x = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5615" class="pln"><span class="n"><a href="#t5615">5615</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5616" class="pln"><span class="n"><a href="#t5616">5616</a></span><span class="t"><span class="str">    tensor([[ 0.4875,  0.9158, -0.5872],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5617" class="pln"><span class="n"><a href="#t5617">5617</a></span><span class="t"><span class="str">            [ 0.3938, -0.6929,  0.6932]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5618" class="pln"><span class="n"><a href="#t5618">5618</a></span><span class="t"><span class="str">    >>> torch.t(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5619" class="pln"><span class="n"><a href="#t5619">5619</a></span><span class="t"><span class="str">    tensor([[ 0.4875,  0.3938],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5620" class="pln"><span class="n"><a href="#t5620">5620</a></span><span class="t"><span class="str">            [ 0.9158, -0.6929],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5621" class="pln"><span class="n"><a href="#t5621">5621</a></span><span class="t"><span class="str">            [-0.5872,  0.6932]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5622" class="pln"><span class="n"><a href="#t5622">5622</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5623" class="pln"><span class="n"><a href="#t5623">5623</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5624" class="run"><span class="n"><a href="#t5624">5624</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">flip</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5625" class="pln"><span class="n"><a href="#t5625">5625</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5626" class="pln"><span class="n"><a href="#t5626">5626</a></span><span class="t"><span class="str">flip(input, dims) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5627" class="pln"><span class="n"><a href="#t5627">5627</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5628" class="pln"><span class="n"><a href="#t5628">5628</a></span><span class="t"><span class="str">Reverse the order of a n-D tensor along given axis in dims.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5629" class="pln"><span class="n"><a href="#t5629">5629</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5630" class="pln"><span class="n"><a href="#t5630">5630</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5631" class="pln"><span class="n"><a href="#t5631">5631</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5632" class="pln"><span class="n"><a href="#t5632">5632</a></span><span class="t"><span class="str">    dims (a list or tuple): axis to flip on</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5633" class="pln"><span class="n"><a href="#t5633">5633</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5634" class="pln"><span class="n"><a href="#t5634">5634</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5635" class="pln"><span class="n"><a href="#t5635">5635</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5636" class="pln"><span class="n"><a href="#t5636">5636</a></span><span class="t"><span class="str">    >>> x = torch.arange(8).view(2, 2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5637" class="pln"><span class="n"><a href="#t5637">5637</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5638" class="pln"><span class="n"><a href="#t5638">5638</a></span><span class="t"><span class="str">    tensor([[[ 0,  1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5639" class="pln"><span class="n"><a href="#t5639">5639</a></span><span class="t"><span class="str">             [ 2,  3]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5640" class="pln"><span class="n"><a href="#t5640">5640</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5641" class="pln"><span class="n"><a href="#t5641">5641</a></span><span class="t"><span class="str">            [[ 4,  5],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5642" class="pln"><span class="n"><a href="#t5642">5642</a></span><span class="t"><span class="str">             [ 6,  7]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5643" class="pln"><span class="n"><a href="#t5643">5643</a></span><span class="t"><span class="str">    >>> torch.flip(x, [0, 1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5644" class="pln"><span class="n"><a href="#t5644">5644</a></span><span class="t"><span class="str">    tensor([[[ 6,  7],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5645" class="pln"><span class="n"><a href="#t5645">5645</a></span><span class="t"><span class="str">             [ 4,  5]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5646" class="pln"><span class="n"><a href="#t5646">5646</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5647" class="pln"><span class="n"><a href="#t5647">5647</a></span><span class="t"><span class="str">            [[ 2,  3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5648" class="pln"><span class="n"><a href="#t5648">5648</a></span><span class="t"><span class="str">             [ 0,  1]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5649" class="pln"><span class="n"><a href="#t5649">5649</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5650" class="pln"><span class="n"><a href="#t5650">5650</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5651" class="run"><span class="n"><a href="#t5651">5651</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">roll</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5652" class="pln"><span class="n"><a href="#t5652">5652</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5653" class="pln"><span class="n"><a href="#t5653">5653</a></span><span class="t"><span class="str">roll(input, shifts, dims=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5654" class="pln"><span class="n"><a href="#t5654">5654</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5655" class="pln"><span class="n"><a href="#t5655">5655</a></span><span class="t"><span class="str">Roll the tensor along the given dimension(s). Elements that are shifted beyond the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5656" class="pln"><span class="n"><a href="#t5656">5656</a></span><span class="t"><span class="str">last position are re-introduced at the first position. If a dimension is not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5657" class="pln"><span class="n"><a href="#t5657">5657</a></span><span class="t"><span class="str">specified, the tensor will be flattened before rolling and then restored</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5658" class="pln"><span class="n"><a href="#t5658">5658</a></span><span class="t"><span class="str">to the original shape.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5659" class="pln"><span class="n"><a href="#t5659">5659</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5660" class="pln"><span class="n"><a href="#t5660">5660</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5661" class="pln"><span class="n"><a href="#t5661">5661</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5662" class="pln"><span class="n"><a href="#t5662">5662</a></span><span class="t"><span class="str">    shifts (int or tuple of ints): The number of places by which the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5663" class="pln"><span class="n"><a href="#t5663">5663</a></span><span class="t"><span class="str">        of the tensor are shifted. If shifts is a tuple, dims must be a tuple of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5664" class="pln"><span class="n"><a href="#t5664">5664</a></span><span class="t"><span class="str">        the same size, and each dimension will be rolled by the corresponding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5665" class="pln"><span class="n"><a href="#t5665">5665</a></span><span class="t"><span class="str">        value</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5666" class="pln"><span class="n"><a href="#t5666">5666</a></span><span class="t"><span class="str">    dims (int or tuple of ints): Axis along which to roll</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5667" class="pln"><span class="n"><a href="#t5667">5667</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5668" class="pln"><span class="n"><a href="#t5668">5668</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5669" class="pln"><span class="n"><a href="#t5669">5669</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5670" class="pln"><span class="n"><a href="#t5670">5670</a></span><span class="t"><span class="str">    >>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5671" class="pln"><span class="n"><a href="#t5671">5671</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5672" class="pln"><span class="n"><a href="#t5672">5672</a></span><span class="t"><span class="str">    tensor([[1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5673" class="pln"><span class="n"><a href="#t5673">5673</a></span><span class="t"><span class="str">            [3, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5674" class="pln"><span class="n"><a href="#t5674">5674</a></span><span class="t"><span class="str">            [5, 6],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5675" class="pln"><span class="n"><a href="#t5675">5675</a></span><span class="t"><span class="str">            [7, 8]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5676" class="pln"><span class="n"><a href="#t5676">5676</a></span><span class="t"><span class="str">    >>> torch.roll(x, 1, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5677" class="pln"><span class="n"><a href="#t5677">5677</a></span><span class="t"><span class="str">    tensor([[7, 8],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5678" class="pln"><span class="n"><a href="#t5678">5678</a></span><span class="t"><span class="str">            [1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5679" class="pln"><span class="n"><a href="#t5679">5679</a></span><span class="t"><span class="str">            [3, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5680" class="pln"><span class="n"><a href="#t5680">5680</a></span><span class="t"><span class="str">            [5, 6]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5681" class="pln"><span class="n"><a href="#t5681">5681</a></span><span class="t"><span class="str">    >>> torch.roll(x, -1, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5682" class="pln"><span class="n"><a href="#t5682">5682</a></span><span class="t"><span class="str">    tensor([[3, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5683" class="pln"><span class="n"><a href="#t5683">5683</a></span><span class="t"><span class="str">            [5, 6],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5684" class="pln"><span class="n"><a href="#t5684">5684</a></span><span class="t"><span class="str">            [7, 8],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5685" class="pln"><span class="n"><a href="#t5685">5685</a></span><span class="t"><span class="str">            [1, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5686" class="pln"><span class="n"><a href="#t5686">5686</a></span><span class="t"><span class="str">    >>> torch.roll(x, shifts=(2, 1), dims=(0, 1))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5687" class="pln"><span class="n"><a href="#t5687">5687</a></span><span class="t"><span class="str">    tensor([[6, 5],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5688" class="pln"><span class="n"><a href="#t5688">5688</a></span><span class="t"><span class="str">            [8, 7],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5689" class="pln"><span class="n"><a href="#t5689">5689</a></span><span class="t"><span class="str">            [2, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5690" class="pln"><span class="n"><a href="#t5690">5690</a></span><span class="t"><span class="str">            [4, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5691" class="pln"><span class="n"><a href="#t5691">5691</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5692" class="pln"><span class="n"><a href="#t5692">5692</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5693" class="run"><span class="n"><a href="#t5693">5693</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">rot90</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5694" class="pln"><span class="n"><a href="#t5694">5694</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5695" class="pln"><span class="n"><a href="#t5695">5695</a></span><span class="t"><span class="str">rot90(input, k, dims) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5696" class="pln"><span class="n"><a href="#t5696">5696</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5697" class="pln"><span class="n"><a href="#t5697">5697</a></span><span class="t"><span class="str">Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5698" class="pln"><span class="n"><a href="#t5698">5698</a></span><span class="t"><span class="str">Rotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k &lt; 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5699" class="pln"><span class="n"><a href="#t5699">5699</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5700" class="pln"><span class="n"><a href="#t5700">5700</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5701" class="pln"><span class="n"><a href="#t5701">5701</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5702" class="pln"><span class="n"><a href="#t5702">5702</a></span><span class="t"><span class="str">    k (int): number of times to rotate</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5703" class="pln"><span class="n"><a href="#t5703">5703</a></span><span class="t"><span class="str">    dims (a list or tuple): axis to rotate</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5704" class="pln"><span class="n"><a href="#t5704">5704</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5705" class="pln"><span class="n"><a href="#t5705">5705</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5706" class="pln"><span class="n"><a href="#t5706">5706</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5707" class="pln"><span class="n"><a href="#t5707">5707</a></span><span class="t"><span class="str">    >>> x = torch.arange(4).view(2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5708" class="pln"><span class="n"><a href="#t5708">5708</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5709" class="pln"><span class="n"><a href="#t5709">5709</a></span><span class="t"><span class="str">    tensor([[0, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5710" class="pln"><span class="n"><a href="#t5710">5710</a></span><span class="t"><span class="str">            [2, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5711" class="pln"><span class="n"><a href="#t5711">5711</a></span><span class="t"><span class="str">    >>> torch.rot90(x, 1, [0, 1])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5712" class="pln"><span class="n"><a href="#t5712">5712</a></span><span class="t"><span class="str">    tensor([[1, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5713" class="pln"><span class="n"><a href="#t5713">5713</a></span><span class="t"><span class="str">            [0, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5714" class="pln"><span class="n"><a href="#t5714">5714</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5715" class="pln"><span class="n"><a href="#t5715">5715</a></span><span class="t"><span class="str">    >>> x = torch.arange(8).view(2, 2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5716" class="pln"><span class="n"><a href="#t5716">5716</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5717" class="pln"><span class="n"><a href="#t5717">5717</a></span><span class="t"><span class="str">    tensor([[[0, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5718" class="pln"><span class="n"><a href="#t5718">5718</a></span><span class="t"><span class="str">             [2, 3]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5719" class="pln"><span class="n"><a href="#t5719">5719</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5720" class="pln"><span class="n"><a href="#t5720">5720</a></span><span class="t"><span class="str">            [[4, 5],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5721" class="pln"><span class="n"><a href="#t5721">5721</a></span><span class="t"><span class="str">             [6, 7]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5722" class="pln"><span class="n"><a href="#t5722">5722</a></span><span class="t"><span class="str">    >>> torch.rot90(x, 1, [1, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5723" class="pln"><span class="n"><a href="#t5723">5723</a></span><span class="t"><span class="str">    tensor([[[1, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5724" class="pln"><span class="n"><a href="#t5724">5724</a></span><span class="t"><span class="str">             [0, 2]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5725" class="pln"><span class="n"><a href="#t5725">5725</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5726" class="pln"><span class="n"><a href="#t5726">5726</a></span><span class="t"><span class="str">            [[5, 7],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5727" class="pln"><span class="n"><a href="#t5727">5727</a></span><span class="t"><span class="str">             [4, 6]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5728" class="pln"><span class="n"><a href="#t5728">5728</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5729" class="pln"><span class="n"><a href="#t5729">5729</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5730" class="run"><span class="n"><a href="#t5730">5730</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">take</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5731" class="pln"><span class="n"><a href="#t5731">5731</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5732" class="pln"><span class="n"><a href="#t5732">5732</a></span><span class="t"><span class="str">take(input, index) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5733" class="pln"><span class="n"><a href="#t5733">5733</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5734" class="pln"><span class="n"><a href="#t5734">5734</a></span><span class="t"><span class="str">Returns a new tensor with the elements of :attr:`input` at the given indices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5735" class="pln"><span class="n"><a href="#t5735">5735</a></span><span class="t"><span class="str">The input tensor is treated as if it were viewed as a 1-D tensor. The result</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5736" class="pln"><span class="n"><a href="#t5736">5736</a></span><span class="t"><span class="str">takes the same shape as the indices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5737" class="pln"><span class="n"><a href="#t5737">5737</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5738" class="pln"><span class="n"><a href="#t5738">5738</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5739" class="pln"><span class="n"><a href="#t5739">5739</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5740" class="pln"><span class="n"><a href="#t5740">5740</a></span><span class="t"><span class="str">    indices (LongTensor): the indices into tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5741" class="pln"><span class="n"><a href="#t5741">5741</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5742" class="pln"><span class="n"><a href="#t5742">5742</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5743" class="pln"><span class="n"><a href="#t5743">5743</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5744" class="pln"><span class="n"><a href="#t5744">5744</a></span><span class="t"><span class="str">    >>> src = torch.tensor([[4, 3, 5],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5745" class="pln"><span class="n"><a href="#t5745">5745</a></span><span class="t"><span class="str">                            [6, 7, 8]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5746" class="pln"><span class="n"><a href="#t5746">5746</a></span><span class="t"><span class="str">    >>> torch.take(src, torch.tensor([0, 2, 5]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5747" class="pln"><span class="n"><a href="#t5747">5747</a></span><span class="t"><span class="str">    tensor([ 4,  5,  8])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5748" class="pln"><span class="n"><a href="#t5748">5748</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5749" class="pln"><span class="n"><a href="#t5749">5749</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5750" class="run"><span class="n"><a href="#t5750">5750</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">tan</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5751" class="pln"><span class="n"><a href="#t5751">5751</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5752" class="pln"><span class="n"><a href="#t5752">5752</a></span><span class="t"><span class="str">tan(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5753" class="pln"><span class="n"><a href="#t5753">5753</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5754" class="pln"><span class="n"><a href="#t5754">5754</a></span><span class="t"><span class="str">Returns a new tensor with the tangent of the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5755" class="pln"><span class="n"><a href="#t5755">5755</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5756" class="pln"><span class="n"><a href="#t5756">5756</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5757" class="pln"><span class="n"><a href="#t5757">5757</a></span><span class="t"><span class="str">    \text{out}_{i} = \tan(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5758" class="pln"><span class="n"><a href="#t5758">5758</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5759" class="pln"><span class="n"><a href="#t5759">5759</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5760" class="pln"><span class="n"><a href="#t5760">5760</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5761" class="pln"><span class="n"><a href="#t5761">5761</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5762" class="pln"><span class="n"><a href="#t5762">5762</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5763" class="pln"><span class="n"><a href="#t5763">5763</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5764" class="pln"><span class="n"><a href="#t5764">5764</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5765" class="pln"><span class="n"><a href="#t5765">5765</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5766" class="pln"><span class="n"><a href="#t5766">5766</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5767" class="pln"><span class="n"><a href="#t5767">5767</a></span><span class="t"><span class="str">    tensor([-1.2027, -1.7687,  0.4412, -1.3856])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5768" class="pln"><span class="n"><a href="#t5768">5768</a></span><span class="t"><span class="str">    >>> torch.tan(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5769" class="pln"><span class="n"><a href="#t5769">5769</a></span><span class="t"><span class="str">    tensor([-2.5930,  4.9859,  0.4722, -5.3366])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5770" class="pln"><span class="n"><a href="#t5770">5770</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5771" class="pln"><span class="n"><a href="#t5771">5771</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5772" class="run"><span class="n"><a href="#t5772">5772</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">tanh</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5773" class="pln"><span class="n"><a href="#t5773">5773</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5774" class="pln"><span class="n"><a href="#t5774">5774</a></span><span class="t"><span class="str">tanh(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5775" class="pln"><span class="n"><a href="#t5775">5775</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5776" class="pln"><span class="n"><a href="#t5776">5776</a></span><span class="t"><span class="str">Returns a new tensor with the hyperbolic tangent of the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5777" class="pln"><span class="n"><a href="#t5777">5777</a></span><span class="t"><span class="str">of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5778" class="pln"><span class="n"><a href="#t5778">5778</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5779" class="pln"><span class="n"><a href="#t5779">5779</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5780" class="pln"><span class="n"><a href="#t5780">5780</a></span><span class="t"><span class="str">    \text{out}_{i} = \tanh(\text{input}_{i})</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5781" class="pln"><span class="n"><a href="#t5781">5781</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5782" class="pln"><span class="n"><a href="#t5782">5782</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5783" class="pln"><span class="n"><a href="#t5783">5783</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5784" class="pln"><span class="n"><a href="#t5784">5784</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5785" class="pln"><span class="n"><a href="#t5785">5785</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5786" class="pln"><span class="n"><a href="#t5786">5786</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5787" class="pln"><span class="n"><a href="#t5787">5787</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5788" class="pln"><span class="n"><a href="#t5788">5788</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5789" class="pln"><span class="n"><a href="#t5789">5789</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5790" class="pln"><span class="n"><a href="#t5790">5790</a></span><span class="t"><span class="str">    tensor([ 0.8986, -0.7279,  1.1745,  0.2611])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5791" class="pln"><span class="n"><a href="#t5791">5791</a></span><span class="t"><span class="str">    >>> torch.tanh(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5792" class="pln"><span class="n"><a href="#t5792">5792</a></span><span class="t"><span class="str">    tensor([ 0.7156, -0.6218,  0.8257,  0.2553])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5793" class="pln"><span class="n"><a href="#t5793">5793</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5794" class="pln"><span class="n"><a href="#t5794">5794</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5795" class="run"><span class="n"><a href="#t5795">5795</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">topk</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5796" class="pln"><span class="n"><a href="#t5796">5796</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5797" class="pln"><span class="n"><a href="#t5797">5797</a></span><span class="t"><span class="str">topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5798" class="pln"><span class="n"><a href="#t5798">5798</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5799" class="pln"><span class="n"><a href="#t5799">5799</a></span><span class="t"><span class="str">Returns the :attr:`k` largest elements of the given :attr:`input` tensor along</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5800" class="pln"><span class="n"><a href="#t5800">5800</a></span><span class="t"><span class="str">a given dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5801" class="pln"><span class="n"><a href="#t5801">5801</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5802" class="pln"><span class="n"><a href="#t5802">5802</a></span><span class="t"><span class="str">If :attr:`dim` is not given, the last dimension of the `input` is chosen.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5803" class="pln"><span class="n"><a href="#t5803">5803</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5804" class="pln"><span class="n"><a href="#t5804">5804</a></span><span class="t"><span class="str">If :attr:`largest` is ``False`` then the `k` smallest elements are returned.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5805" class="pln"><span class="n"><a href="#t5805">5805</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5806" class="pln"><span class="n"><a href="#t5806">5806</a></span><span class="t"><span class="str">A namedtuple of `(values, indices)` is returned, where the `indices` are the indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5807" class="pln"><span class="n"><a href="#t5807">5807</a></span><span class="t"><span class="str">of the elements in the original `input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5808" class="pln"><span class="n"><a href="#t5808">5808</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5809" class="pln"><span class="n"><a href="#t5809">5809</a></span><span class="t"><span class="str">The boolean option :attr:`sorted` if ``True``, will make sure that the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5810" class="pln"><span class="n"><a href="#t5810">5810</a></span><span class="t"><span class="str">`k` elements are themselves sorted</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5811" class="pln"><span class="n"><a href="#t5811">5811</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5812" class="pln"><span class="n"><a href="#t5812">5812</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5813" class="pln"><span class="n"><a href="#t5813">5813</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5814" class="pln"><span class="n"><a href="#t5814">5814</a></span><span class="t"><span class="str">    k (int): the k in "top-k"</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5815" class="pln"><span class="n"><a href="#t5815">5815</a></span><span class="t"><span class="str">    dim (int, optional): the dimension to sort along</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5816" class="pln"><span class="n"><a href="#t5816">5816</a></span><span class="t"><span class="str">    largest (bool, optional): controls whether to return largest or</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5817" class="pln"><span class="n"><a href="#t5817">5817</a></span><span class="t"><span class="str">           smallest elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5818" class="pln"><span class="n"><a href="#t5818">5818</a></span><span class="t"><span class="str">    sorted (bool, optional): controls whether to return the elements</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5819" class="pln"><span class="n"><a href="#t5819">5819</a></span><span class="t"><span class="str">           in sorted order</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5820" class="pln"><span class="n"><a href="#t5820">5820</a></span><span class="t"><span class="str">    out (tuple, optional): the output tuple of (Tensor, LongTensor) that can be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5821" class="pln"><span class="n"><a href="#t5821">5821</a></span><span class="t"><span class="str">        optionally given to be used as output buffers</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5822" class="pln"><span class="n"><a href="#t5822">5822</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5823" class="pln"><span class="n"><a href="#t5823">5823</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5824" class="pln"><span class="n"><a href="#t5824">5824</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5825" class="pln"><span class="n"><a href="#t5825">5825</a></span><span class="t"><span class="str">    >>> x = torch.arange(1., 6.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5826" class="pln"><span class="n"><a href="#t5826">5826</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5827" class="pln"><span class="n"><a href="#t5827">5827</a></span><span class="t"><span class="str">    tensor([ 1.,  2.,  3.,  4.,  5.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5828" class="pln"><span class="n"><a href="#t5828">5828</a></span><span class="t"><span class="str">    >>> torch.topk(x, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5829" class="pln"><span class="n"><a href="#t5829">5829</a></span><span class="t"><span class="str">    torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5830" class="pln"><span class="n"><a href="#t5830">5830</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5831" class="pln"><span class="n"><a href="#t5831">5831</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5832" class="run"><span class="n"><a href="#t5832">5832</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">trace</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5833" class="pln"><span class="n"><a href="#t5833">5833</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5834" class="pln"><span class="n"><a href="#t5834">5834</a></span><span class="t"><span class="str">trace(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5835" class="pln"><span class="n"><a href="#t5835">5835</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5836" class="pln"><span class="n"><a href="#t5836">5836</a></span><span class="t"><span class="str">Returns the sum of the elements of the diagonal of the input 2-D matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5837" class="pln"><span class="n"><a href="#t5837">5837</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5838" class="pln"><span class="n"><a href="#t5838">5838</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5839" class="pln"><span class="n"><a href="#t5839">5839</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5840" class="pln"><span class="n"><a href="#t5840">5840</a></span><span class="t"><span class="str">    >>> x = torch.arange(1., 10.).view(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5841" class="pln"><span class="n"><a href="#t5841">5841</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5842" class="pln"><span class="n"><a href="#t5842">5842</a></span><span class="t"><span class="str">    tensor([[ 1.,  2.,  3.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5843" class="pln"><span class="n"><a href="#t5843">5843</a></span><span class="t"><span class="str">            [ 4.,  5.,  6.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5844" class="pln"><span class="n"><a href="#t5844">5844</a></span><span class="t"><span class="str">            [ 7.,  8.,  9.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5845" class="pln"><span class="n"><a href="#t5845">5845</a></span><span class="t"><span class="str">    >>> torch.trace(x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5846" class="pln"><span class="n"><a href="#t5846">5846</a></span><span class="t"><span class="str">    tensor(15.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5847" class="pln"><span class="n"><a href="#t5847">5847</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5848" class="pln"><span class="n"><a href="#t5848">5848</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5849" class="run"><span class="n"><a href="#t5849">5849</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">transpose</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5850" class="pln"><span class="n"><a href="#t5850">5850</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5851" class="pln"><span class="n"><a href="#t5851">5851</a></span><span class="t"><span class="str">transpose(input, dim0, dim1) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5852" class="pln"><span class="n"><a href="#t5852">5852</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5853" class="pln"><span class="n"><a href="#t5853">5853</a></span><span class="t"><span class="str">Returns a tensor that is a transposed version of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5854" class="pln"><span class="n"><a href="#t5854">5854</a></span><span class="t"><span class="str">The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5855" class="pln"><span class="n"><a href="#t5855">5855</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5856" class="pln"><span class="n"><a href="#t5856">5856</a></span><span class="t"><span class="str">The resulting :attr:`out` tensor shares it's underlying storage with the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5857" class="pln"><span class="n"><a href="#t5857">5857</a></span><span class="t"><span class="str">:attr:`input` tensor, so changing the content of one would change the content</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5858" class="pln"><span class="n"><a href="#t5858">5858</a></span><span class="t"><span class="str">of the other.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5859" class="pln"><span class="n"><a href="#t5859">5859</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5860" class="pln"><span class="n"><a href="#t5860">5860</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5861" class="pln"><span class="n"><a href="#t5861">5861</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5862" class="pln"><span class="n"><a href="#t5862">5862</a></span><span class="t"><span class="str">    dim0 (int): the first dimension to be transposed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5863" class="pln"><span class="n"><a href="#t5863">5863</a></span><span class="t"><span class="str">    dim1 (int): the second dimension to be transposed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5864" class="pln"><span class="n"><a href="#t5864">5864</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5865" class="pln"><span class="n"><a href="#t5865">5865</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5866" class="pln"><span class="n"><a href="#t5866">5866</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5867" class="pln"><span class="n"><a href="#t5867">5867</a></span><span class="t"><span class="str">    >>> x = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5868" class="pln"><span class="n"><a href="#t5868">5868</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5869" class="pln"><span class="n"><a href="#t5869">5869</a></span><span class="t"><span class="str">    tensor([[ 1.0028, -0.9893,  0.5809],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5870" class="pln"><span class="n"><a href="#t5870">5870</a></span><span class="t"><span class="str">            [-0.1669,  0.7299,  0.4942]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5871" class="pln"><span class="n"><a href="#t5871">5871</a></span><span class="t"><span class="str">    >>> torch.transpose(x, 0, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5872" class="pln"><span class="n"><a href="#t5872">5872</a></span><span class="t"><span class="str">    tensor([[ 1.0028, -0.1669],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5873" class="pln"><span class="n"><a href="#t5873">5873</a></span><span class="t"><span class="str">            [-0.9893,  0.7299],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5874" class="pln"><span class="n"><a href="#t5874">5874</a></span><span class="t"><span class="str">            [ 0.5809,  0.4942]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5875" class="pln"><span class="n"><a href="#t5875">5875</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5876" class="pln"><span class="n"><a href="#t5876">5876</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5877" class="run"><span class="n"><a href="#t5877">5877</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">triangular_solve</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5878" class="pln"><span class="n"><a href="#t5878">5878</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5879" class="pln"><span class="n"><a href="#t5879">5879</a></span><span class="t"><span class="str">triangular_solve(input, A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5880" class="pln"><span class="n"><a href="#t5880">5880</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5881" class="pln"><span class="n"><a href="#t5881">5881</a></span><span class="t"><span class="str">Solves a system of equations with a triangular coefficient matrix :math:`A`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5882" class="pln"><span class="n"><a href="#t5882">5882</a></span><span class="t"><span class="str">and multiple right-hand sides :math:`b`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5883" class="pln"><span class="n"><a href="#t5883">5883</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5884" class="pln"><span class="n"><a href="#t5884">5884</a></span><span class="t"><span class="str">In particular, solves :math:`AX = b` and assumes :math:`A` is upper-triangular</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5885" class="pln"><span class="n"><a href="#t5885">5885</a></span><span class="t"><span class="str">with the default keyword arguments.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5886" class="pln"><span class="n"><a href="#t5886">5886</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5887" class="pln"><span class="n"><a href="#t5887">5887</a></span><span class="t"><span class="str">`torch.triangular_solve(b, A)` can take in 2D inputs `b, A` or inputs that are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5888" class="pln"><span class="n"><a href="#t5888">5888</a></span><span class="t"><span class="str">batches of 2D matrices. If the inputs are batches, then returns</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5889" class="pln"><span class="n"><a href="#t5889">5889</a></span><span class="t"><span class="str">batched outputs `X`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5890" class="pln"><span class="n"><a href="#t5890">5890</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5891" class="pln"><span class="n"><a href="#t5891">5891</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5892" class="pln"><span class="n"><a href="#t5892">5892</a></span><span class="t"><span class="str">    input (Tensor): multiple right-hand sides of size :math:`(*, m, k)` where</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5893" class="pln"><span class="n"><a href="#t5893">5893</a></span><span class="t"><span class="str">                :math:`*` is zero of more batch dimensions (:math:`b`)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5894" class="pln"><span class="n"><a href="#t5894">5894</a></span><span class="t"><span class="str">    A (Tensor): the input triangular coefficient matrix of size :math:`(*, m, m)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5895" class="pln"><span class="n"><a href="#t5895">5895</a></span><span class="t"><span class="str">                where :math:`*` is zero or more batch dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5896" class="pln"><span class="n"><a href="#t5896">5896</a></span><span class="t"><span class="str">    upper (bool, optional): whether to solve the upper-triangular system</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5897" class="pln"><span class="n"><a href="#t5897">5897</a></span><span class="t"><span class="str">        of equations (default) or the lower-triangular system of equations. Default: ``True``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5898" class="pln"><span class="n"><a href="#t5898">5898</a></span><span class="t"><span class="str">    transpose (bool, optional): whether :math:`A` should be transposed before</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5899" class="pln"><span class="n"><a href="#t5899">5899</a></span><span class="t"><span class="str">        being sent into the solver. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5900" class="pln"><span class="n"><a href="#t5900">5900</a></span><span class="t"><span class="str">    unitriangular (bool, optional): whether :math:`A` is unit triangular.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5901" class="pln"><span class="n"><a href="#t5901">5901</a></span><span class="t"><span class="str">        If True, the diagonal elements of :math:`A` are assumed to be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5902" class="pln"><span class="n"><a href="#t5902">5902</a></span><span class="t"><span class="str">        1 and not referenced from :math:`A`. Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5903" class="pln"><span class="n"><a href="#t5903">5903</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5904" class="pln"><span class="n"><a href="#t5904">5904</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5905" class="pln"><span class="n"><a href="#t5905">5905</a></span><span class="t"><span class="str">    A namedtuple `(solution, cloned_coefficient)` where `cloned_coefficient`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5906" class="pln"><span class="n"><a href="#t5906">5906</a></span><span class="t"><span class="str">    is a clone of :math:`A` and `solution` is the solution :math:`X` to :math:`AX = b`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5907" class="pln"><span class="n"><a href="#t5907">5907</a></span><span class="t"><span class="str">    (or whatever variant of the system of equations, depending on the keyword arguments.)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5908" class="pln"><span class="n"><a href="#t5908">5908</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5909" class="pln"><span class="n"><a href="#t5909">5909</a></span><span class="t"><span class="str">Examples::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5910" class="pln"><span class="n"><a href="#t5910">5910</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5911" class="pln"><span class="n"><a href="#t5911">5911</a></span><span class="t"><span class="str">    >>> A = torch.randn(2, 2).triu()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5912" class="pln"><span class="n"><a href="#t5912">5912</a></span><span class="t"><span class="str">    >>> A</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5913" class="pln"><span class="n"><a href="#t5913">5913</a></span><span class="t"><span class="str">    tensor([[ 1.1527, -1.0753],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5914" class="pln"><span class="n"><a href="#t5914">5914</a></span><span class="t"><span class="str">            [ 0.0000,  0.7986]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5915" class="pln"><span class="n"><a href="#t5915">5915</a></span><span class="t"><span class="str">    >>> b = torch.randn(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5916" class="pln"><span class="n"><a href="#t5916">5916</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5917" class="pln"><span class="n"><a href="#t5917">5917</a></span><span class="t"><span class="str">    tensor([[-0.0210,  2.3513, -1.5492],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5918" class="pln"><span class="n"><a href="#t5918">5918</a></span><span class="t"><span class="str">            [ 1.5429,  0.7403, -1.0243]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5919" class="pln"><span class="n"><a href="#t5919">5919</a></span><span class="t"><span class="str">    >>> torch.triangular_solve(b, A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5920" class="pln"><span class="n"><a href="#t5920">5920</a></span><span class="t"><span class="str">    torch.return_types.triangular_solve(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5921" class="pln"><span class="n"><a href="#t5921">5921</a></span><span class="t"><span class="str">    solution=tensor([[ 1.7841,  2.9046, -2.5405],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5922" class="pln"><span class="n"><a href="#t5922">5922</a></span><span class="t"><span class="str">            [ 1.9320,  0.9270, -1.2826]]),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5923" class="pln"><span class="n"><a href="#t5923">5923</a></span><span class="t"><span class="str">    cloned_coefficient=tensor([[ 1.1527, -1.0753],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5924" class="pln"><span class="n"><a href="#t5924">5924</a></span><span class="t"><span class="str">            [ 0.0000,  0.7986]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5925" class="pln"><span class="n"><a href="#t5925">5925</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5926" class="pln"><span class="n"><a href="#t5926">5926</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5927" class="run"><span class="n"><a href="#t5927">5927</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">tril</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5928" class="pln"><span class="n"><a href="#t5928">5928</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5929" class="pln"><span class="n"><a href="#t5929">5929</a></span><span class="t"><span class="str">tril(input, diagonal=0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5930" class="pln"><span class="n"><a href="#t5930">5930</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5931" class="pln"><span class="n"><a href="#t5931">5931</a></span><span class="t"><span class="str">Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5932" class="pln"><span class="n"><a href="#t5932">5932</a></span><span class="t"><span class="str">:attr:`input`, the other elements of the result tensor :attr:`out` are set to 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5933" class="pln"><span class="n"><a href="#t5933">5933</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5934" class="pln"><span class="n"><a href="#t5934">5934</a></span><span class="t"><span class="str">The lower triangular part of the matrix is defined as the elements on and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5935" class="pln"><span class="n"><a href="#t5935">5935</a></span><span class="t"><span class="str">below the diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5936" class="pln"><span class="n"><a href="#t5936">5936</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5937" class="pln"><span class="n"><a href="#t5937">5937</a></span><span class="t"><span class="str">The argument :attr:`diagonal` controls which diagonal to consider. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5938" class="pln"><span class="n"><a href="#t5938">5938</a></span><span class="t"><span class="str">:attr:`diagonal` = 0, all elements on and below the main diagonal are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5939" class="pln"><span class="n"><a href="#t5939">5939</a></span><span class="t"><span class="str">retained. A positive value includes just as many diagonals above the main</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5940" class="pln"><span class="n"><a href="#t5940">5940</a></span><span class="t"><span class="str">diagonal, and similarly a negative value excludes just as many diagonals below</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5941" class="pln"><span class="n"><a href="#t5941">5941</a></span><span class="t"><span class="str">the main diagonal. The main diagonal are the set of indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5942" class="pln"><span class="n"><a href="#t5942">5942</a></span><span class="t"><span class="str">:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5943" class="pln"><span class="n"><a href="#t5943">5943</a></span><span class="t"><span class="str">:math:`d_{1}, d_{2}` are the dimensions of the matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5944" class="pln"><span class="n"><a href="#t5944">5944</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5945" class="pln"><span class="n"><a href="#t5945">5945</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5946" class="pln"><span class="n"><a href="#t5946">5946</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5947" class="pln"><span class="n"><a href="#t5947">5947</a></span><span class="t"><span class="str">    diagonal (int, optional): the diagonal to consider</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5948" class="pln"><span class="n"><a href="#t5948">5948</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5949" class="pln"><span class="n"><a href="#t5949">5949</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5950" class="pln"><span class="n"><a href="#t5950">5950</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5951" class="pln"><span class="n"><a href="#t5951">5951</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5952" class="pln"><span class="n"><a href="#t5952">5952</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5953" class="pln"><span class="n"><a href="#t5953">5953</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5954" class="pln"><span class="n"><a href="#t5954">5954</a></span><span class="t"><span class="str">    tensor([[-1.0813, -0.8619,  0.7105],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5955" class="pln"><span class="n"><a href="#t5955">5955</a></span><span class="t"><span class="str">            [ 0.0935,  0.1380,  2.2112],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5956" class="pln"><span class="n"><a href="#t5956">5956</a></span><span class="t"><span class="str">            [-0.3409, -0.9828,  0.0289]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5957" class="pln"><span class="n"><a href="#t5957">5957</a></span><span class="t"><span class="str">    >>> torch.tril(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5958" class="pln"><span class="n"><a href="#t5958">5958</a></span><span class="t"><span class="str">    tensor([[-1.0813,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5959" class="pln"><span class="n"><a href="#t5959">5959</a></span><span class="t"><span class="str">            [ 0.0935,  0.1380,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5960" class="pln"><span class="n"><a href="#t5960">5960</a></span><span class="t"><span class="str">            [-0.3409, -0.9828,  0.0289]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5961" class="pln"><span class="n"><a href="#t5961">5961</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5962" class="pln"><span class="n"><a href="#t5962">5962</a></span><span class="t"><span class="str">    >>> b = torch.randn(4, 6)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5963" class="pln"><span class="n"><a href="#t5963">5963</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5964" class="pln"><span class="n"><a href="#t5964">5964</a></span><span class="t"><span class="str">    tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5965" class="pln"><span class="n"><a href="#t5965">5965</a></span><span class="t"><span class="str">            [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5966" class="pln"><span class="n"><a href="#t5966">5966</a></span><span class="t"><span class="str">            [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5967" class="pln"><span class="n"><a href="#t5967">5967</a></span><span class="t"><span class="str">            [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5968" class="pln"><span class="n"><a href="#t5968">5968</a></span><span class="t"><span class="str">    >>> torch.tril(b, diagonal=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5969" class="pln"><span class="n"><a href="#t5969">5969</a></span><span class="t"><span class="str">    tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5970" class="pln"><span class="n"><a href="#t5970">5970</a></span><span class="t"><span class="str">            [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5971" class="pln"><span class="n"><a href="#t5971">5971</a></span><span class="t"><span class="str">            [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5972" class="pln"><span class="n"><a href="#t5972">5972</a></span><span class="t"><span class="str">            [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5973" class="pln"><span class="n"><a href="#t5973">5973</a></span><span class="t"><span class="str">    >>> torch.tril(b, diagonal=-1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5974" class="pln"><span class="n"><a href="#t5974">5974</a></span><span class="t"><span class="str">    tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5975" class="pln"><span class="n"><a href="#t5975">5975</a></span><span class="t"><span class="str">            [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5976" class="pln"><span class="n"><a href="#t5976">5976</a></span><span class="t"><span class="str">            [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5977" class="pln"><span class="n"><a href="#t5977">5977</a></span><span class="t"><span class="str">            [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5978" class="pln"><span class="n"><a href="#t5978">5978</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5979" class="pln"><span class="n"><a href="#t5979">5979</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5980" class="pln"><span class="n"><a href="#t5980">5980</a></span><span class="t"><span class="com"># docstr is split in two parts to avoid format mis-captureing :math: braces '{}'</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5981" class="pln"><span class="n"><a href="#t5981">5981</a></span><span class="t"><span class="com"># as common args.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5982" class="run"><span class="n"><a href="#t5982">5982</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">tril_indices</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5983" class="pln"><span class="n"><a href="#t5983">5983</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5984" class="pln"><span class="n"><a href="#t5984">5984</a></span><span class="t"><span class="str">tril_indices(row, col, offset=0, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5985" class="pln"><span class="n"><a href="#t5985">5985</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5986" class="pln"><span class="n"><a href="#t5986">5986</a></span><span class="t"><span class="str">Returns the indices of the lower triangular part of a :attr:`row`-by-</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5987" class="pln"><span class="n"><a href="#t5987">5987</a></span><span class="t"><span class="str">:attr:`col` matrix in a 2-by-N Tensor, where the first row contains row</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5988" class="pln"><span class="n"><a href="#t5988">5988</a></span><span class="t"><span class="str">coordinates of all indices and the second row contains column coordinates.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5989" class="pln"><span class="n"><a href="#t5989">5989</a></span><span class="t"><span class="str">Indices are ordered based on rows and then columns.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5990" class="pln"><span class="n"><a href="#t5990">5990</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5991" class="pln"><span class="n"><a href="#t5991">5991</a></span><span class="t"><span class="str">The lower triangular part of the matrix is defined as the elements on and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5992" class="pln"><span class="n"><a href="#t5992">5992</a></span><span class="t"><span class="str">below the diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5993" class="pln"><span class="n"><a href="#t5993">5993</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t5994" class="pln"><span class="n"><a href="#t5994">5994</a></span><span class="t"><span class="str">The argument :attr:`offset` controls which diagonal to consider. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5995" class="pln"><span class="n"><a href="#t5995">5995</a></span><span class="t"><span class="str">:attr:`offset` = 0, all elements on and below the main diagonal are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5996" class="pln"><span class="n"><a href="#t5996">5996</a></span><span class="t"><span class="str">retained. A positive value includes just as many diagonals above the main</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5997" class="pln"><span class="n"><a href="#t5997">5997</a></span><span class="t"><span class="str">diagonal, and similarly a negative value excludes just as many diagonals below</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5998" class="pln"><span class="n"><a href="#t5998">5998</a></span><span class="t"><span class="str">the main diagonal. The main diagonal are the set of indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5999" class="pln"><span class="n"><a href="#t5999">5999</a></span><span class="t"><span class="str">:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6000" class="pln"><span class="n"><a href="#t6000">6000</a></span><span class="t"><span class="str">where :math:`d_{1}, d_{2}` are the dimensions of the matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6001" class="pln"><span class="n"><a href="#t6001">6001</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6002" class="pln"><span class="n"><a href="#t6002">6002</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6003" class="pln"><span class="n"><a href="#t6003">6003</a></span><span class="t"><span class="str">    When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6004" class="pln"><span class="n"><a href="#t6004">6004</a></span><span class="t"><span class="str">    prevent overflow during calculation.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6005" class="pln"><span class="n"><a href="#t6005">6005</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6006" class="pln"><span class="n"><a href="#t6006">6006</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6007" class="pln"><span class="n"><a href="#t6007">6007</a></span><span class="t"><span class="str">    row (``int``): number of rows in the 2-D matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6008" class="pln"><span class="n"><a href="#t6008">6008</a></span><span class="t"><span class="str">    col (``int``): number of columns in the 2-D matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6009" class="pln"><span class="n"><a href="#t6009">6009</a></span><span class="t"><span class="str">    offset (``int``): diagonal offset from the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6010" class="pln"><span class="n"><a href="#t6010">6010</a></span><span class="t"><span class="str">        Default: if not provided, 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6011" class="pln"><span class="n"><a href="#t6011">6011</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6012" class="pln"><span class="n"><a href="#t6012">6012</a></span><span class="t"><span class="str">        Default: if ``None``, ``torch.long``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6013" class="pln"><span class="n"><a href="#t6013">6013</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6014" class="pln"><span class="n"><a href="#t6014">6014</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): currently only support ``torch.strided``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6015" class="pln"><span class="n"><a href="#t6015">6015</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6016" class="pln"><span class="n"><a href="#t6016">6016</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6017" class="pln"><span class="n"><a href="#t6017">6017</a></span><span class="t"><span class="str">    >>> a = torch.tril_indices(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6018" class="pln"><span class="n"><a href="#t6018">6018</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6019" class="pln"><span class="n"><a href="#t6019">6019</a></span><span class="t"><span class="str">    tensor([[0, 1, 1, 2, 2, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6020" class="pln"><span class="n"><a href="#t6020">6020</a></span><span class="t"><span class="str">            [0, 0, 1, 0, 1, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6021" class="pln"><span class="n"><a href="#t6021">6021</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6022" class="pln"><span class="n"><a href="#t6022">6022</a></span><span class="t"><span class="str">    >>> a = torch.tril_indices(4, 3, -1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6023" class="pln"><span class="n"><a href="#t6023">6023</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6024" class="pln"><span class="n"><a href="#t6024">6024</a></span><span class="t"><span class="str">    tensor([[1, 2, 2, 3, 3, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6025" class="pln"><span class="n"><a href="#t6025">6025</a></span><span class="t"><span class="str">            [0, 0, 1, 0, 1, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6026" class="pln"><span class="n"><a href="#t6026">6026</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6027" class="pln"><span class="n"><a href="#t6027">6027</a></span><span class="t"><span class="str">    >>> a = torch.tril_indices(4, 3, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6028" class="pln"><span class="n"><a href="#t6028">6028</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6029" class="pln"><span class="n"><a href="#t6029">6029</a></span><span class="t"><span class="str">    tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6030" class="pln"><span class="n"><a href="#t6030">6030</a></span><span class="t"><span class="str">            [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6031" class="pln"><span class="n"><a href="#t6031">6031</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6032" class="pln"><span class="n"><a href="#t6032">6032</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6033" class="run"><span class="n"><a href="#t6033">6033</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">triu</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6034" class="pln"><span class="n"><a href="#t6034">6034</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6035" class="pln"><span class="n"><a href="#t6035">6035</a></span><span class="t"><span class="str">triu(input, diagonal=0, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6036" class="pln"><span class="n"><a href="#t6036">6036</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6037" class="pln"><span class="n"><a href="#t6037">6037</a></span><span class="t"><span class="str">Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6038" class="pln"><span class="n"><a href="#t6038">6038</a></span><span class="t"><span class="str">:attr:`input`, the other elements of the result tensor :attr:`out` are set to 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6039" class="pln"><span class="n"><a href="#t6039">6039</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6040" class="pln"><span class="n"><a href="#t6040">6040</a></span><span class="t"><span class="str">The upper triangular part of the matrix is defined as the elements on and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6041" class="pln"><span class="n"><a href="#t6041">6041</a></span><span class="t"><span class="str">above the diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6042" class="pln"><span class="n"><a href="#t6042">6042</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6043" class="pln"><span class="n"><a href="#t6043">6043</a></span><span class="t"><span class="str">The argument :attr:`diagonal` controls which diagonal to consider. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6044" class="pln"><span class="n"><a href="#t6044">6044</a></span><span class="t"><span class="str">:attr:`diagonal` = 0, all elements on and above the main diagonal are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6045" class="pln"><span class="n"><a href="#t6045">6045</a></span><span class="t"><span class="str">retained. A positive value excludes just as many diagonals above the main</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6046" class="pln"><span class="n"><a href="#t6046">6046</a></span><span class="t"><span class="str">diagonal, and similarly a negative value includes just as many diagonals below</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6047" class="pln"><span class="n"><a href="#t6047">6047</a></span><span class="t"><span class="str">the main diagonal. The main diagonal are the set of indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6048" class="pln"><span class="n"><a href="#t6048">6048</a></span><span class="t"><span class="str">:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6049" class="pln"><span class="n"><a href="#t6049">6049</a></span><span class="t"><span class="str">:math:`d_{1}, d_{2}` are the dimensions of the matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6050" class="pln"><span class="n"><a href="#t6050">6050</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6051" class="pln"><span class="n"><a href="#t6051">6051</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6052" class="pln"><span class="n"><a href="#t6052">6052</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6053" class="pln"><span class="n"><a href="#t6053">6053</a></span><span class="t"><span class="str">    diagonal (int, optional): the diagonal to consider</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6054" class="pln"><span class="n"><a href="#t6054">6054</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6055" class="pln"><span class="n"><a href="#t6055">6055</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6056" class="pln"><span class="n"><a href="#t6056">6056</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6057" class="pln"><span class="n"><a href="#t6057">6057</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6058" class="pln"><span class="n"><a href="#t6058">6058</a></span><span class="t"><span class="str">    >>> a = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6059" class="pln"><span class="n"><a href="#t6059">6059</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6060" class="pln"><span class="n"><a href="#t6060">6060</a></span><span class="t"><span class="str">    tensor([[ 0.2309,  0.5207,  2.0049],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6061" class="pln"><span class="n"><a href="#t6061">6061</a></span><span class="t"><span class="str">            [ 0.2072, -1.0680,  0.6602],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6062" class="pln"><span class="n"><a href="#t6062">6062</a></span><span class="t"><span class="str">            [ 0.3480, -0.5211, -0.4573]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6063" class="pln"><span class="n"><a href="#t6063">6063</a></span><span class="t"><span class="str">    >>> torch.triu(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6064" class="pln"><span class="n"><a href="#t6064">6064</a></span><span class="t"><span class="str">    tensor([[ 0.2309,  0.5207,  2.0049],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6065" class="pln"><span class="n"><a href="#t6065">6065</a></span><span class="t"><span class="str">            [ 0.0000, -1.0680,  0.6602],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6066" class="pln"><span class="n"><a href="#t6066">6066</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000, -0.4573]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6067" class="pln"><span class="n"><a href="#t6067">6067</a></span><span class="t"><span class="str">    >>> torch.triu(a, diagonal=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6068" class="pln"><span class="n"><a href="#t6068">6068</a></span><span class="t"><span class="str">    tensor([[ 0.0000,  0.5207,  2.0049],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6069" class="pln"><span class="n"><a href="#t6069">6069</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.6602],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6070" class="pln"><span class="n"><a href="#t6070">6070</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6071" class="pln"><span class="n"><a href="#t6071">6071</a></span><span class="t"><span class="str">    >>> torch.triu(a, diagonal=-1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6072" class="pln"><span class="n"><a href="#t6072">6072</a></span><span class="t"><span class="str">    tensor([[ 0.2309,  0.5207,  2.0049],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6073" class="pln"><span class="n"><a href="#t6073">6073</a></span><span class="t"><span class="str">            [ 0.2072, -1.0680,  0.6602],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6074" class="pln"><span class="n"><a href="#t6074">6074</a></span><span class="t"><span class="str">            [ 0.0000, -0.5211, -0.4573]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6075" class="pln"><span class="n"><a href="#t6075">6075</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6076" class="pln"><span class="n"><a href="#t6076">6076</a></span><span class="t"><span class="str">    >>> b = torch.randn(4, 6)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6077" class="pln"><span class="n"><a href="#t6077">6077</a></span><span class="t"><span class="str">    >>> b</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6078" class="pln"><span class="n"><a href="#t6078">6078</a></span><span class="t"><span class="str">    tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6079" class="pln"><span class="n"><a href="#t6079">6079</a></span><span class="t"><span class="str">            [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6080" class="pln"><span class="n"><a href="#t6080">6080</a></span><span class="t"><span class="str">            [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6081" class="pln"><span class="n"><a href="#t6081">6081</a></span><span class="t"><span class="str">            [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6082" class="pln"><span class="n"><a href="#t6082">6082</a></span><span class="t"><span class="str">    >>> torch.triu(b, diagonal=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6083" class="pln"><span class="n"><a href="#t6083">6083</a></span><span class="t"><span class="str">    tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6084" class="pln"><span class="n"><a href="#t6084">6084</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6085" class="pln"><span class="n"><a href="#t6085">6085</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6086" class="pln"><span class="n"><a href="#t6086">6086</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6087" class="pln"><span class="n"><a href="#t6087">6087</a></span><span class="t"><span class="str">    >>> torch.triu(b, diagonal=-1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6088" class="pln"><span class="n"><a href="#t6088">6088</a></span><span class="t"><span class="str">    tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6089" class="pln"><span class="n"><a href="#t6089">6089</a></span><span class="t"><span class="str">            [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6090" class="pln"><span class="n"><a href="#t6090">6090</a></span><span class="t"><span class="str">            [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6091" class="pln"><span class="n"><a href="#t6091">6091</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6092" class="pln"><span class="n"><a href="#t6092">6092</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6093" class="pln"><span class="n"><a href="#t6093">6093</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6094" class="pln"><span class="n"><a href="#t6094">6094</a></span><span class="t"><span class="com"># docstr is split in two parts to avoid format mis-capturing :math: braces '{}'</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6095" class="pln"><span class="n"><a href="#t6095">6095</a></span><span class="t"><span class="com"># as common args.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6096" class="run"><span class="n"><a href="#t6096">6096</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">triu_indices</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6097" class="pln"><span class="n"><a href="#t6097">6097</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6098" class="pln"><span class="n"><a href="#t6098">6098</a></span><span class="t"><span class="str">triu_indices(row, col, offset=0, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6099" class="pln"><span class="n"><a href="#t6099">6099</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6100" class="pln"><span class="n"><a href="#t6100">6100</a></span><span class="t"><span class="str">Returns the indices of the upper triangular part of a :attr:`row` by</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6101" class="pln"><span class="n"><a href="#t6101">6101</a></span><span class="t"><span class="str">:attr:`col` matrix in a 2-by-N Tensor, where the first row contains row</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6102" class="pln"><span class="n"><a href="#t6102">6102</a></span><span class="t"><span class="str">coordinates of all indices and the second row contains column coordinates.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6103" class="pln"><span class="n"><a href="#t6103">6103</a></span><span class="t"><span class="str">Indices are ordered based on rows and then columns.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6104" class="pln"><span class="n"><a href="#t6104">6104</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6105" class="pln"><span class="n"><a href="#t6105">6105</a></span><span class="t"><span class="str">The upper triangular part of the matrix is defined as the elements on and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6106" class="pln"><span class="n"><a href="#t6106">6106</a></span><span class="t"><span class="str">above the diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6107" class="pln"><span class="n"><a href="#t6107">6107</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6108" class="pln"><span class="n"><a href="#t6108">6108</a></span><span class="t"><span class="str">The argument :attr:`offset` controls which diagonal to consider. If</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6109" class="pln"><span class="n"><a href="#t6109">6109</a></span><span class="t"><span class="str">:attr:`offset` = 0, all elements on and above the main diagonal are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6110" class="pln"><span class="n"><a href="#t6110">6110</a></span><span class="t"><span class="str">retained. A positive value excludes just as many diagonals above the main</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6111" class="pln"><span class="n"><a href="#t6111">6111</a></span><span class="t"><span class="str">diagonal, and similarly a negative value includes just as many diagonals below</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6112" class="pln"><span class="n"><a href="#t6112">6112</a></span><span class="t"><span class="str">the main diagonal. The main diagonal are the set of indices</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6113" class="pln"><span class="n"><a href="#t6113">6113</a></span><span class="t"><span class="str">:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6114" class="pln"><span class="n"><a href="#t6114">6114</a></span><span class="t"><span class="str">where :math:`d_{1}, d_{2}` are the dimensions of the matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6115" class="pln"><span class="n"><a href="#t6115">6115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6116" class="pln"><span class="n"><a href="#t6116">6116</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6117" class="pln"><span class="n"><a href="#t6117">6117</a></span><span class="t"><span class="str">    When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6118" class="pln"><span class="n"><a href="#t6118">6118</a></span><span class="t"><span class="str">    prevent overflow during calculation.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6119" class="pln"><span class="n"><a href="#t6119">6119</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6120" class="pln"><span class="n"><a href="#t6120">6120</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6121" class="pln"><span class="n"><a href="#t6121">6121</a></span><span class="t"><span class="str">    row (``int``): number of rows in the 2-D matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6122" class="pln"><span class="n"><a href="#t6122">6122</a></span><span class="t"><span class="str">    col (``int``): number of columns in the 2-D matrix.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6123" class="pln"><span class="n"><a href="#t6123">6123</a></span><span class="t"><span class="str">    offset (``int``): diagonal offset from the main diagonal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6124" class="pln"><span class="n"><a href="#t6124">6124</a></span><span class="t"><span class="str">        Default: if not provided, 0.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6125" class="pln"><span class="n"><a href="#t6125">6125</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6126" class="pln"><span class="n"><a href="#t6126">6126</a></span><span class="t"><span class="str">        Default: if ``None``, ``torch.long``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6127" class="pln"><span class="n"><a href="#t6127">6127</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6128" class="pln"><span class="n"><a href="#t6128">6128</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): currently only support ``torch.strided``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6129" class="pln"><span class="n"><a href="#t6129">6129</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6130" class="pln"><span class="n"><a href="#t6130">6130</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6131" class="pln"><span class="n"><a href="#t6131">6131</a></span><span class="t"><span class="str">    >>> a = torch.triu_indices(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6132" class="pln"><span class="n"><a href="#t6132">6132</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6133" class="pln"><span class="n"><a href="#t6133">6133</a></span><span class="t"><span class="str">    tensor([[0, 0, 0, 1, 1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6134" class="pln"><span class="n"><a href="#t6134">6134</a></span><span class="t"><span class="str">            [0, 1, 2, 1, 2, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6135" class="pln"><span class="n"><a href="#t6135">6135</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6136" class="pln"><span class="n"><a href="#t6136">6136</a></span><span class="t"><span class="str">    >>> a = torch.triu_indices(4, 3, -1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6137" class="pln"><span class="n"><a href="#t6137">6137</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6138" class="pln"><span class="n"><a href="#t6138">6138</a></span><span class="t"><span class="str">    tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6139" class="pln"><span class="n"><a href="#t6139">6139</a></span><span class="t"><span class="str">            [0, 1, 2, 0, 1, 2, 1, 2, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6140" class="pln"><span class="n"><a href="#t6140">6140</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6141" class="pln"><span class="n"><a href="#t6141">6141</a></span><span class="t"><span class="str">    >>> a = torch.triu_indices(4, 3, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6142" class="pln"><span class="n"><a href="#t6142">6142</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6143" class="pln"><span class="n"><a href="#t6143">6143</a></span><span class="t"><span class="str">    tensor([[0, 0, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6144" class="pln"><span class="n"><a href="#t6144">6144</a></span><span class="t"><span class="str">            [1, 2, 2]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6145" class="pln"><span class="n"><a href="#t6145">6145</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6146" class="pln"><span class="n"><a href="#t6146">6146</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6147" class="run"><span class="n"><a href="#t6147">6147</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">trunc</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6148" class="pln"><span class="n"><a href="#t6148">6148</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6149" class="pln"><span class="n"><a href="#t6149">6149</a></span><span class="t"><span class="str">trunc(input, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6150" class="pln"><span class="n"><a href="#t6150">6150</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6151" class="pln"><span class="n"><a href="#t6151">6151</a></span><span class="t"><span class="str">Returns a new tensor with the truncated integer values of</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6152" class="pln"><span class="n"><a href="#t6152">6152</a></span><span class="t"><span class="str">the elements of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6153" class="pln"><span class="n"><a href="#t6153">6153</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6154" class="pln"><span class="n"><a href="#t6154">6154</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6155" class="pln"><span class="n"><a href="#t6155">6155</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6156" class="pln"><span class="n"><a href="#t6156">6156</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6157" class="pln"><span class="n"><a href="#t6157">6157</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6158" class="pln"><span class="n"><a href="#t6158">6158</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6159" class="pln"><span class="n"><a href="#t6159">6159</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6160" class="pln"><span class="n"><a href="#t6160">6160</a></span><span class="t"><span class="str">    >>> a = torch.randn(4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6161" class="pln"><span class="n"><a href="#t6161">6161</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6162" class="pln"><span class="n"><a href="#t6162">6162</a></span><span class="t"><span class="str">    tensor([ 3.4742,  0.5466, -0.8008, -0.9079])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6163" class="pln"><span class="n"><a href="#t6163">6163</a></span><span class="t"><span class="str">    >>> torch.trunc(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6164" class="pln"><span class="n"><a href="#t6164">6164</a></span><span class="t"><span class="str">    tensor([ 3.,  0., -0., -0.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6165" class="pln"><span class="n"><a href="#t6165">6165</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6166" class="pln"><span class="n"><a href="#t6166">6166</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6167" class="run"><span class="n"><a href="#t6167">6167</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">unsqueeze</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6168" class="pln"><span class="n"><a href="#t6168">6168</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6169" class="pln"><span class="n"><a href="#t6169">6169</a></span><span class="t"><span class="str">unsqueeze(input, dim, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6170" class="pln"><span class="n"><a href="#t6170">6170</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6171" class="pln"><span class="n"><a href="#t6171">6171</a></span><span class="t"><span class="str">Returns a new tensor with a dimension of size one inserted at the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6172" class="pln"><span class="n"><a href="#t6172">6172</a></span><span class="t"><span class="str">specified position.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6173" class="pln"><span class="n"><a href="#t6173">6173</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6174" class="pln"><span class="n"><a href="#t6174">6174</a></span><span class="t"><span class="str">The returned tensor shares the same underlying data with this tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6175" class="pln"><span class="n"><a href="#t6175">6175</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6176" class="pln"><span class="n"><a href="#t6176">6176</a></span><span class="t"><span class="str">A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6177" class="pln"><span class="n"><a href="#t6177">6177</a></span><span class="t"><span class="str">can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6178" class="pln"><span class="n"><a href="#t6178">6178</a></span><span class="t"><span class="str">applied at :attr:`dim` = ``dim + input.dim() + 1``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6179" class="pln"><span class="n"><a href="#t6179">6179</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6180" class="pln"><span class="n"><a href="#t6180">6180</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6181" class="pln"><span class="n"><a href="#t6181">6181</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6182" class="pln"><span class="n"><a href="#t6182">6182</a></span><span class="t"><span class="str">    dim (int): the index at which to insert the singleton dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6183" class="pln"><span class="n"><a href="#t6183">6183</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6184" class="pln"><span class="n"><a href="#t6184">6184</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6185" class="pln"><span class="n"><a href="#t6185">6185</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6186" class="pln"><span class="n"><a href="#t6186">6186</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6187" class="pln"><span class="n"><a href="#t6187">6187</a></span><span class="t"><span class="str">    >>> x = torch.tensor([1, 2, 3, 4])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6188" class="pln"><span class="n"><a href="#t6188">6188</a></span><span class="t"><span class="str">    >>> torch.unsqueeze(x, 0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6189" class="pln"><span class="n"><a href="#t6189">6189</a></span><span class="t"><span class="str">    tensor([[ 1,  2,  3,  4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6190" class="pln"><span class="n"><a href="#t6190">6190</a></span><span class="t"><span class="str">    >>> torch.unsqueeze(x, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6191" class="pln"><span class="n"><a href="#t6191">6191</a></span><span class="t"><span class="str">    tensor([[ 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6192" class="pln"><span class="n"><a href="#t6192">6192</a></span><span class="t"><span class="str">            [ 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6193" class="pln"><span class="n"><a href="#t6193">6193</a></span><span class="t"><span class="str">            [ 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6194" class="pln"><span class="n"><a href="#t6194">6194</a></span><span class="t"><span class="str">            [ 4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6195" class="pln"><span class="n"><a href="#t6195">6195</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6196" class="pln"><span class="n"><a href="#t6196">6196</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6197" class="run"><span class="n"><a href="#t6197">6197</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">var</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6198" class="pln"><span class="n"><a href="#t6198">6198</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6199" class="pln"><span class="n"><a href="#t6199">6199</a></span><span class="t"><span class="str">.. function:: var(input, unbiased=True) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6200" class="pln"><span class="n"><a href="#t6200">6200</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6201" class="pln"><span class="n"><a href="#t6201">6201</a></span><span class="t"><span class="str">Returns the variance of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6202" class="pln"><span class="n"><a href="#t6202">6202</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6203" class="pln"><span class="n"><a href="#t6203">6203</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the variance will be calculated via the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6204" class="pln"><span class="n"><a href="#t6204">6204</a></span><span class="t"><span class="str">biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6205" class="pln"><span class="n"><a href="#t6205">6205</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6206" class="pln"><span class="n"><a href="#t6206">6206</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6207" class="pln"><span class="n"><a href="#t6207">6207</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6208" class="pln"><span class="n"><a href="#t6208">6208</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6209" class="pln"><span class="n"><a href="#t6209">6209</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6210" class="pln"><span class="n"><a href="#t6210">6210</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6211" class="pln"><span class="n"><a href="#t6211">6211</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6212" class="pln"><span class="n"><a href="#t6212">6212</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6213" class="pln"><span class="n"><a href="#t6213">6213</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6214" class="pln"><span class="n"><a href="#t6214">6214</a></span><span class="t"><span class="str">    tensor([[-0.3425, -1.2636, -0.4864]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6215" class="pln"><span class="n"><a href="#t6215">6215</a></span><span class="t"><span class="str">    >>> torch.var(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6216" class="pln"><span class="n"><a href="#t6216">6216</a></span><span class="t"><span class="str">    tensor(0.2455)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6217" class="pln"><span class="n"><a href="#t6217">6217</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6218" class="pln"><span class="n"><a href="#t6218">6218</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6219" class="pln"><span class="n"><a href="#t6219">6219</a></span><span class="t"><span class="str">.. function:: var(input, dim, keepdim=False, unbiased=True, out=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6220" class="pln"><span class="n"><a href="#t6220">6220</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6221" class="pln"><span class="n"><a href="#t6221">6221</a></span><span class="t"><span class="str">Returns the variance of each row of the :attr:`input` tensor in the given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6222" class="pln"><span class="n"><a href="#t6222">6222</a></span><span class="t"><span class="str">dimension :attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6223" class="pln"><span class="n"><a href="#t6223">6223</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6224" class="pln"><span class="n"><a href="#t6224">6224</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6225" class="pln"><span class="n"><a href="#t6225">6225</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6226" class="pln"><span class="n"><a href="#t6226">6226</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the variance will be calculated via the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6227" class="pln"><span class="n"><a href="#t6227">6227</a></span><span class="t"><span class="str">biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6228" class="pln"><span class="n"><a href="#t6228">6228</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6229" class="pln"><span class="n"><a href="#t6229">6229</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6230" class="pln"><span class="n"><a href="#t6230">6230</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6231" class="pln"><span class="n"><a href="#t6231">6231</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6232" class="pln"><span class="n"><a href="#t6232">6232</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6233" class="pln"><span class="n"><a href="#t6233">6233</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6234" class="pln"><span class="n"><a href="#t6234">6234</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6235" class="pln"><span class="n"><a href="#t6235">6235</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6236" class="pln"><span class="n"><a href="#t6236">6236</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6237" class="pln"><span class="n"><a href="#t6237">6237</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6238" class="pln"><span class="n"><a href="#t6238">6238</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6239" class="pln"><span class="n"><a href="#t6239">6239</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6240" class="pln"><span class="n"><a href="#t6240">6240</a></span><span class="t"><span class="str">    tensor([[-0.3567,  1.7385, -1.3042,  0.7423],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6241" class="pln"><span class="n"><a href="#t6241">6241</a></span><span class="t"><span class="str">            [ 1.3436, -0.1015, -0.9834, -0.8438],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6242" class="pln"><span class="n"><a href="#t6242">6242</a></span><span class="t"><span class="str">            [ 0.6056,  0.1089, -0.3112, -1.4085],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6243" class="pln"><span class="n"><a href="#t6243">6243</a></span><span class="t"><span class="str">            [-0.7700,  0.6074, -0.1469,  0.7777]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6244" class="pln"><span class="n"><a href="#t6244">6244</a></span><span class="t"><span class="str">    >>> torch.var(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6245" class="pln"><span class="n"><a href="#t6245">6245</a></span><span class="t"><span class="str">    tensor([ 1.7444,  1.1363,  0.7356,  0.5112])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6246" class="pln"><span class="n"><a href="#t6246">6246</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6247" class="pln"><span class="n"><a href="#t6247">6247</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6248" class="run"><span class="n"><a href="#t6248">6248</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">var_mean</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6249" class="pln"><span class="n"><a href="#t6249">6249</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6250" class="pln"><span class="n"><a href="#t6250">6250</a></span><span class="t"><span class="str">.. function:: var_mean(input, unbiased=True) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6251" class="pln"><span class="n"><a href="#t6251">6251</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6252" class="pln"><span class="n"><a href="#t6252">6252</a></span><span class="t"><span class="str">Returns the variance and mean of all elements in the :attr:`input` tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6253" class="pln"><span class="n"><a href="#t6253">6253</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6254" class="pln"><span class="n"><a href="#t6254">6254</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the variance will be calculated via the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6255" class="pln"><span class="n"><a href="#t6255">6255</a></span><span class="t"><span class="str">biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6256" class="pln"><span class="n"><a href="#t6256">6256</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6257" class="pln"><span class="n"><a href="#t6257">6257</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6258" class="pln"><span class="n"><a href="#t6258">6258</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6259" class="pln"><span class="n"><a href="#t6259">6259</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6260" class="pln"><span class="n"><a href="#t6260">6260</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6261" class="pln"><span class="n"><a href="#t6261">6261</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6262" class="pln"><span class="n"><a href="#t6262">6262</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6263" class="pln"><span class="n"><a href="#t6263">6263</a></span><span class="t"><span class="str">    >>> a = torch.randn(1, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6264" class="pln"><span class="n"><a href="#t6264">6264</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6265" class="pln"><span class="n"><a href="#t6265">6265</a></span><span class="t"><span class="str">    tensor([[0.0146, 0.4258, 0.2211]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6266" class="pln"><span class="n"><a href="#t6266">6266</a></span><span class="t"><span class="str">    >>> torch.var_mean(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6267" class="pln"><span class="n"><a href="#t6267">6267</a></span><span class="t"><span class="str">    (tensor(0.0423), tensor(0.2205))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6268" class="pln"><span class="n"><a href="#t6268">6268</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6269" class="pln"><span class="n"><a href="#t6269">6269</a></span><span class="t"><span class="str">.. function:: var_mean(input, dim, keepdim=False, unbiased=True) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6270" class="pln"><span class="n"><a href="#t6270">6270</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6271" class="pln"><span class="n"><a href="#t6271">6271</a></span><span class="t"><span class="str">Returns the variance and mean of each row of the :attr:`input` tensor in the given</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6272" class="pln"><span class="n"><a href="#t6272">6272</a></span><span class="t"><span class="str">dimension :attr:`dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6273" class="pln"><span class="n"><a href="#t6273">6273</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6274" class="pln"><span class="n"><a href="#t6274">6274</a></span><span class="t"><span class="str">{keepdim_details}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6275" class="pln"><span class="n"><a href="#t6275">6275</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6276" class="pln"><span class="n"><a href="#t6276">6276</a></span><span class="t"><span class="str">If :attr:`unbiased` is ``False``, then the variance will be calculated via the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6277" class="pln"><span class="n"><a href="#t6277">6277</a></span><span class="t"><span class="str">biased estimator. Otherwise, Bessel's correction will be used.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6278" class="pln"><span class="n"><a href="#t6278">6278</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6279" class="pln"><span class="n"><a href="#t6279">6279</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6280" class="pln"><span class="n"><a href="#t6280">6280</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6281" class="pln"><span class="n"><a href="#t6281">6281</a></span><span class="t"><span class="str">    {dim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6282" class="pln"><span class="n"><a href="#t6282">6282</a></span><span class="t"><span class="str">    {keepdim}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6283" class="pln"><span class="n"><a href="#t6283">6283</a></span><span class="t"><span class="str">    unbiased (bool): whether to use the unbiased estimation or not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6284" class="pln"><span class="n"><a href="#t6284">6284</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6285" class="pln"><span class="n"><a href="#t6285">6285</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6286" class="pln"><span class="n"><a href="#t6286">6286</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6287" class="pln"><span class="n"><a href="#t6287">6287</a></span><span class="t"><span class="str">    >>> a = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6288" class="pln"><span class="n"><a href="#t6288">6288</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6289" class="pln"><span class="n"><a href="#t6289">6289</a></span><span class="t"><span class="str">    tensor([[-1.5650,  2.0415, -0.1024, -0.5790],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6290" class="pln"><span class="n"><a href="#t6290">6290</a></span><span class="t"><span class="str">            [ 0.2325, -2.6145, -1.6428, -0.3537],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6291" class="pln"><span class="n"><a href="#t6291">6291</a></span><span class="t"><span class="str">            [-0.2159, -1.1069,  1.2882, -1.3265],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6292" class="pln"><span class="n"><a href="#t6292">6292</a></span><span class="t"><span class="str">            [-0.6706, -1.5893,  0.6827,  1.6727]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6293" class="pln"><span class="n"><a href="#t6293">6293</a></span><span class="t"><span class="str">    >>> torch.var_mean(a, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6294" class="pln"><span class="n"><a href="#t6294">6294</a></span><span class="t"><span class="str">    (tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6295" class="pln"><span class="n"><a href="#t6295">6295</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">multi_dim_common</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6296" class="pln"><span class="n"><a href="#t6296">6296</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6297" class="run"><span class="n"><a href="#t6297">6297</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">zeros</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6298" class="pln"><span class="n"><a href="#t6298">6298</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6299" class="pln"><span class="n"><a href="#t6299">6299</a></span><span class="t"><span class="str">zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6300" class="pln"><span class="n"><a href="#t6300">6300</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6301" class="pln"><span class="n"><a href="#t6301">6301</a></span><span class="t"><span class="str">Returns a tensor filled with the scalar value `0`, with the shape defined</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6302" class="pln"><span class="n"><a href="#t6302">6302</a></span><span class="t"><span class="str">by the variable argument :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6303" class="pln"><span class="n"><a href="#t6303">6303</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6304" class="pln"><span class="n"><a href="#t6304">6304</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6305" class="pln"><span class="n"><a href="#t6305">6305</a></span><span class="t"><span class="str">    size (int...): a sequence of integers defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6306" class="pln"><span class="n"><a href="#t6306">6306</a></span><span class="t"><span class="str">        Can be a variable number of arguments or a collection like a list or tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6307" class="pln"><span class="n"><a href="#t6307">6307</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6308" class="pln"><span class="n"><a href="#t6308">6308</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6309" class="pln"><span class="n"><a href="#t6309">6309</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6310" class="pln"><span class="n"><a href="#t6310">6310</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6311" class="pln"><span class="n"><a href="#t6311">6311</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6312" class="pln"><span class="n"><a href="#t6312">6312</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6313" class="pln"><span class="n"><a href="#t6313">6313</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6314" class="pln"><span class="n"><a href="#t6314">6314</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6315" class="pln"><span class="n"><a href="#t6315">6315</a></span><span class="t"><span class="str">    >>> torch.zeros(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6316" class="pln"><span class="n"><a href="#t6316">6316</a></span><span class="t"><span class="str">    tensor([[ 0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6317" class="pln"><span class="n"><a href="#t6317">6317</a></span><span class="t"><span class="str">            [ 0.,  0.,  0.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6318" class="pln"><span class="n"><a href="#t6318">6318</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6319" class="pln"><span class="n"><a href="#t6319">6319</a></span><span class="t"><span class="str">    >>> torch.zeros(5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6320" class="pln"><span class="n"><a href="#t6320">6320</a></span><span class="t"><span class="str">    tensor([ 0.,  0.,  0.,  0.,  0.])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6321" class="pln"><span class="n"><a href="#t6321">6321</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6322" class="pln"><span class="n"><a href="#t6322">6322</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6323" class="run"><span class="n"><a href="#t6323">6323</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6324" class="pln"><span class="n"><a href="#t6324">6324</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6325" class="pln"><span class="n"><a href="#t6325">6325</a></span><span class="t"><span class="str">zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6326" class="pln"><span class="n"><a href="#t6326">6326</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6327" class="pln"><span class="n"><a href="#t6327">6327</a></span><span class="t"><span class="str">Returns a tensor filled with the scalar value `0`, with the same size as</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6328" class="pln"><span class="n"><a href="#t6328">6328</a></span><span class="t"><span class="str">:attr:`input`. ``torch.zeros_like(input)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6329" class="pln"><span class="n"><a href="#t6329">6329</a></span><span class="t"><span class="str">``torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6330" class="pln"><span class="n"><a href="#t6330">6330</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6331" class="pln"><span class="n"><a href="#t6331">6331</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6332" class="pln"><span class="n"><a href="#t6332">6332</a></span><span class="t"><span class="str">    As of 0.4, this function does not support an :attr:`out` keyword. As an alternative,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6333" class="pln"><span class="n"><a href="#t6333">6333</a></span><span class="t"><span class="str">    the old ``torch.zeros_like(input, out=output)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6334" class="pln"><span class="n"><a href="#t6334">6334</a></span><span class="t"><span class="str">    ``torch.zeros(input.size(), out=output)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6335" class="pln"><span class="n"><a href="#t6335">6335</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6336" class="pln"><span class="n"><a href="#t6336">6336</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6337" class="pln"><span class="n"><a href="#t6337">6337</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6338" class="pln"><span class="n"><a href="#t6338">6338</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6339" class="pln"><span class="n"><a href="#t6339">6339</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6340" class="pln"><span class="n"><a href="#t6340">6340</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6341" class="pln"><span class="n"><a href="#t6341">6341</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6342" class="pln"><span class="n"><a href="#t6342">6342</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6343" class="pln"><span class="n"><a href="#t6343">6343</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6344" class="pln"><span class="n"><a href="#t6344">6344</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6345" class="pln"><span class="n"><a href="#t6345">6345</a></span><span class="t"><span class="str">    >>> input = torch.empty(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6346" class="pln"><span class="n"><a href="#t6346">6346</a></span><span class="t"><span class="str">    >>> torch.zeros_like(input)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6347" class="pln"><span class="n"><a href="#t6347">6347</a></span><span class="t"><span class="str">    tensor([[ 0.,  0.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6348" class="pln"><span class="n"><a href="#t6348">6348</a></span><span class="t"><span class="str">            [ 0.,  0.,  0.]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6349" class="pln"><span class="n"><a href="#t6349">6349</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6350" class="pln"><span class="n"><a href="#t6350">6350</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6351" class="run"><span class="n"><a href="#t6351">6351</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">empty</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6352" class="pln"><span class="n"><a href="#t6352">6352</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6353" class="pln"><span class="n"><a href="#t6353">6353</a></span><span class="t"><span class="str">empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6354" class="pln"><span class="n"><a href="#t6354">6354</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6355" class="pln"><span class="n"><a href="#t6355">6355</a></span><span class="t"><span class="str">Returns a tensor filled with uninitialized data. The shape of the tensor is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6356" class="pln"><span class="n"><a href="#t6356">6356</a></span><span class="t"><span class="str">defined by the variable argument :attr:`size`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6357" class="pln"><span class="n"><a href="#t6357">6357</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6358" class="pln"><span class="n"><a href="#t6358">6358</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6359" class="pln"><span class="n"><a href="#t6359">6359</a></span><span class="t"><span class="str">    size (int...): a sequence of integers defining the shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6360" class="pln"><span class="n"><a href="#t6360">6360</a></span><span class="t"><span class="str">        Can be a variable number of arguments or a collection like a list or tuple.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6361" class="pln"><span class="n"><a href="#t6361">6361</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6362" class="pln"><span class="n"><a href="#t6362">6362</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6363" class="pln"><span class="n"><a href="#t6363">6363</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6364" class="pln"><span class="n"><a href="#t6364">6364</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6365" class="pln"><span class="n"><a href="#t6365">6365</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6366" class="pln"><span class="n"><a href="#t6366">6366</a></span><span class="t"><span class="str">    {pin_memory}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6367" class="pln"><span class="n"><a href="#t6367">6367</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6368" class="pln"><span class="n"><a href="#t6368">6368</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6369" class="pln"><span class="n"><a href="#t6369">6369</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6370" class="pln"><span class="n"><a href="#t6370">6370</a></span><span class="t"><span class="str">    >>> torch.empty(2, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6371" class="pln"><span class="n"><a href="#t6371">6371</a></span><span class="t"><span class="str">    tensor(1.00000e-08 *</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6372" class="pln"><span class="n"><a href="#t6372">6372</a></span><span class="t"><span class="str">           [[ 6.3984,  0.0000,  0.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6373" class="pln"><span class="n"><a href="#t6373">6373</a></span><span class="t"><span class="str">            [ 0.0000,  0.0000,  0.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6374" class="pln"><span class="n"><a href="#t6374">6374</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6375" class="pln"><span class="n"><a href="#t6375">6375</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6376" class="pln"><span class="n"><a href="#t6376">6376</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6377" class="run"><span class="n"><a href="#t6377">6377</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">empty_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6378" class="pln"><span class="n"><a href="#t6378">6378</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6379" class="pln"><span class="n"><a href="#t6379">6379</a></span><span class="t"><span class="str">empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6380" class="pln"><span class="n"><a href="#t6380">6380</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6381" class="pln"><span class="n"><a href="#t6381">6381</a></span><span class="t"><span class="str">Returns an uninitialized tensor with the same size as :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6382" class="pln"><span class="n"><a href="#t6382">6382</a></span><span class="t"><span class="str">``torch.empty_like(input)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6383" class="pln"><span class="n"><a href="#t6383">6383</a></span><span class="t"><span class="str">``torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6384" class="pln"><span class="n"><a href="#t6384">6384</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6385" class="pln"><span class="n"><a href="#t6385">6385</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6386" class="pln"><span class="n"><a href="#t6386">6386</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6387" class="pln"><span class="n"><a href="#t6387">6387</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6388" class="pln"><span class="n"><a href="#t6388">6388</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6389" class="pln"><span class="n"><a href="#t6389">6389</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6390" class="pln"><span class="n"><a href="#t6390">6390</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6391" class="pln"><span class="n"><a href="#t6391">6391</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6392" class="pln"><span class="n"><a href="#t6392">6392</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6393" class="pln"><span class="n"><a href="#t6393">6393</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6394" class="pln"><span class="n"><a href="#t6394">6394</a></span><span class="t"><span class="str">    >>> torch.empty((2,3), dtype=torch.int64)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6395" class="pln"><span class="n"><a href="#t6395">6395</a></span><span class="t"><span class="str">    tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6396" class="pln"><span class="n"><a href="#t6396">6396</a></span><span class="t"><span class="str">            [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6397" class="pln"><span class="n"><a href="#t6397">6397</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6398" class="pln"><span class="n"><a href="#t6398">6398</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6399" class="run"><span class="n"><a href="#t6399">6399</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">empty_strided</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6400" class="pln"><span class="n"><a href="#t6400">6400</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6401" class="pln"><span class="n"><a href="#t6401">6401</a></span><span class="t"><span class="str">empty_strided(size, stride, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6402" class="pln"><span class="n"><a href="#t6402">6402</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6403" class="pln"><span class="n"><a href="#t6403">6403</a></span><span class="t"><span class="str">Returns a tensor filled with uninitialized data. The shape and strides of the tensor is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6404" class="pln"><span class="n"><a href="#t6404">6404</a></span><span class="t"><span class="str">defined by the variable argument :attr:`size` and :attr:`stride` respectively.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6405" class="pln"><span class="n"><a href="#t6405">6405</a></span><span class="t"><span class="str">``torch.empty_strided(size, stride)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6406" class="pln"><span class="n"><a href="#t6406">6406</a></span><span class="t"><span class="str">``torch.empty(size).as_strided(size, stride)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6407" class="pln"><span class="n"><a href="#t6407">6407</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6408" class="pln"><span class="n"><a href="#t6408">6408</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6409" class="pln"><span class="n"><a href="#t6409">6409</a></span><span class="t"><span class="str">    More than one element of the created tensor may refer to a single memory</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6410" class="pln"><span class="n"><a href="#t6410">6410</a></span><span class="t"><span class="str">    location. As a result, in-place operations (especially ones that are</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6411" class="pln"><span class="n"><a href="#t6411">6411</a></span><span class="t"><span class="str">    vectorized) may result in incorrect behavior. If you need to write to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6412" class="pln"><span class="n"><a href="#t6412">6412</a></span><span class="t"><span class="str">    the tensors, please clone them first.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6413" class="pln"><span class="n"><a href="#t6413">6413</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6414" class="pln"><span class="n"><a href="#t6414">6414</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6415" class="pln"><span class="n"><a href="#t6415">6415</a></span><span class="t"><span class="str">    size (tuple of ints): the shape of the output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6416" class="pln"><span class="n"><a href="#t6416">6416</a></span><span class="t"><span class="str">    stride (tuple of ints): the strides of the output tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6417" class="pln"><span class="n"><a href="#t6417">6417</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6418" class="pln"><span class="n"><a href="#t6418">6418</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6419" class="pln"><span class="n"><a href="#t6419">6419</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6420" class="pln"><span class="n"><a href="#t6420">6420</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6421" class="pln"><span class="n"><a href="#t6421">6421</a></span><span class="t"><span class="str">    {pin_memory}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6422" class="pln"><span class="n"><a href="#t6422">6422</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6423" class="pln"><span class="n"><a href="#t6423">6423</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6424" class="pln"><span class="n"><a href="#t6424">6424</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6425" class="pln"><span class="n"><a href="#t6425">6425</a></span><span class="t"><span class="str">    >>> a = torch.empty_strided((2, 3), (1, 2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6426" class="pln"><span class="n"><a href="#t6426">6426</a></span><span class="t"><span class="str">    >>> a</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6427" class="pln"><span class="n"><a href="#t6427">6427</a></span><span class="t"><span class="str">    tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6428" class="pln"><span class="n"><a href="#t6428">6428</a></span><span class="t"><span class="str">            [0.0000e+00, 0.0000e+00, 3.0705e-41]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6429" class="pln"><span class="n"><a href="#t6429">6429</a></span><span class="t"><span class="str">    >>> a.stride()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6430" class="pln"><span class="n"><a href="#t6430">6430</a></span><span class="t"><span class="str">    (1, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6431" class="pln"><span class="n"><a href="#t6431">6431</a></span><span class="t"><span class="str">    >>> a.size()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6432" class="pln"><span class="n"><a href="#t6432">6432</a></span><span class="t"><span class="str">    torch.Size([2, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6433" class="pln"><span class="n"><a href="#t6433">6433</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6434" class="pln"><span class="n"><a href="#t6434">6434</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6435" class="run"><span class="n"><a href="#t6435">6435</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">full</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6436" class="pln"><span class="n"><a href="#t6436">6436</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6437" class="pln"><span class="n"><a href="#t6437">6437</a></span><span class="t"><span class="str">full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6438" class="pln"><span class="n"><a href="#t6438">6438</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6439" class="pln"><span class="n"><a href="#t6439">6439</a></span><span class="t"><span class="str">Returns a tensor of size :attr:`size` filled with :attr:`fill_value`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6440" class="pln"><span class="n"><a href="#t6440">6440</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6441" class="pln"><span class="n"><a href="#t6441">6441</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6442" class="pln"><span class="n"><a href="#t6442">6442</a></span><span class="t"><span class="str">    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6443" class="pln"><span class="n"><a href="#t6443">6443</a></span><span class="t"><span class="str">        shape of the output tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6444" class="pln"><span class="n"><a href="#t6444">6444</a></span><span class="t"><span class="str">    fill_value: the number to fill the output tensor with.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6445" class="pln"><span class="n"><a href="#t6445">6445</a></span><span class="t"><span class="str">    {out}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6446" class="pln"><span class="n"><a href="#t6446">6446</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6447" class="pln"><span class="n"><a href="#t6447">6447</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6448" class="pln"><span class="n"><a href="#t6448">6448</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6449" class="pln"><span class="n"><a href="#t6449">6449</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6450" class="pln"><span class="n"><a href="#t6450">6450</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6451" class="pln"><span class="n"><a href="#t6451">6451</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6452" class="pln"><span class="n"><a href="#t6452">6452</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6453" class="pln"><span class="n"><a href="#t6453">6453</a></span><span class="t"><span class="str">    >>> torch.full((2, 3), 3.141592)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6454" class="pln"><span class="n"><a href="#t6454">6454</a></span><span class="t"><span class="str">    tensor([[ 3.1416,  3.1416,  3.1416],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6455" class="pln"><span class="n"><a href="#t6455">6455</a></span><span class="t"><span class="str">            [ 3.1416,  3.1416,  3.1416]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6456" class="pln"><span class="n"><a href="#t6456">6456</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6457" class="pln"><span class="n"><a href="#t6457">6457</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6458" class="pln"><span class="n"><a href="#t6458">6458</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6459" class="run"><span class="n"><a href="#t6459">6459</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">full_like</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6460" class="pln"><span class="n"><a href="#t6460">6460</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6461" class="pln"><span class="n"><a href="#t6461">6461</a></span><span class="t"><span class="str">full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6462" class="pln"><span class="n"><a href="#t6462">6462</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6463" class="pln"><span class="n"><a href="#t6463">6463</a></span><span class="t"><span class="str">Returns a tensor with the same size as :attr:`input` filled with :attr:`fill_value`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6464" class="pln"><span class="n"><a href="#t6464">6464</a></span><span class="t"><span class="str">``torch.full_like(input, fill_value)`` is equivalent to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6465" class="pln"><span class="n"><a href="#t6465">6465</a></span><span class="t"><span class="str">``torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6466" class="pln"><span class="n"><a href="#t6466">6466</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6467" class="pln"><span class="n"><a href="#t6467">6467</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6468" class="pln"><span class="n"><a href="#t6468">6468</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6469" class="pln"><span class="n"><a href="#t6469">6469</a></span><span class="t"><span class="str">    fill_value: the number to fill the output tensor with.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6470" class="pln"><span class="n"><a href="#t6470">6470</a></span><span class="t"><span class="str">    {dtype}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6471" class="pln"><span class="n"><a href="#t6471">6471</a></span><span class="t"><span class="str">    {layout}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6472" class="pln"><span class="n"><a href="#t6472">6472</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6473" class="pln"><span class="n"><a href="#t6473">6473</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6474" class="pln"><span class="n"><a href="#t6474">6474</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_like_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6475" class="pln"><span class="n"><a href="#t6475">6475</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6476" class="run"><span class="n"><a href="#t6476">6476</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">det</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6477" class="pln"><span class="n"><a href="#t6477">6477</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6478" class="pln"><span class="n"><a href="#t6478">6478</a></span><span class="t"><span class="str">det(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6479" class="pln"><span class="n"><a href="#t6479">6479</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6480" class="pln"><span class="n"><a href="#t6480">6480</a></span><span class="t"><span class="str">Calculates determinant of a square matrix or batches of square matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6481" class="pln"><span class="n"><a href="#t6481">6481</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6482" class="pln"><span class="n"><a href="#t6482">6482</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6483" class="pln"><span class="n"><a href="#t6483">6483</a></span><span class="t"><span class="str">    Backward through :meth:`det` internally uses SVD results when :attr:`input` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6484" class="pln"><span class="n"><a href="#t6484">6484</a></span><span class="t"><span class="str">    not invertible. In this case, double backward through :meth:`det` will be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6485" class="pln"><span class="n"><a href="#t6485">6485</a></span><span class="t"><span class="str">    unstable in when :attr:`input` doesn't have distinct singular values. See</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6486" class="pln"><span class="n"><a href="#t6486">6486</a></span><span class="t"><span class="str">    :meth:`~torch.svd` for details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6487" class="pln"><span class="n"><a href="#t6487">6487</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6488" class="pln"><span class="n"><a href="#t6488">6488</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6489" class="pln"><span class="n"><a href="#t6489">6489</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size ``(*, n, n)`` where ``*`` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6490" class="pln"><span class="n"><a href="#t6490">6490</a></span><span class="t"><span class="str">                batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6491" class="pln"><span class="n"><a href="#t6491">6491</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6492" class="pln"><span class="n"><a href="#t6492">6492</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6493" class="pln"><span class="n"><a href="#t6493">6493</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6494" class="pln"><span class="n"><a href="#t6494">6494</a></span><span class="t"><span class="str">    >>> A = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6495" class="pln"><span class="n"><a href="#t6495">6495</a></span><span class="t"><span class="str">    >>> torch.det(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6496" class="pln"><span class="n"><a href="#t6496">6496</a></span><span class="t"><span class="str">    tensor(3.7641)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6497" class="pln"><span class="n"><a href="#t6497">6497</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6498" class="pln"><span class="n"><a href="#t6498">6498</a></span><span class="t"><span class="str">    >>> A = torch.randn(3, 2, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6499" class="pln"><span class="n"><a href="#t6499">6499</a></span><span class="t"><span class="str">    >>> A</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6500" class="pln"><span class="n"><a href="#t6500">6500</a></span><span class="t"><span class="str">    tensor([[[ 0.9254, -0.6213],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6501" class="pln"><span class="n"><a href="#t6501">6501</a></span><span class="t"><span class="str">             [-0.5787,  1.6843]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6502" class="pln"><span class="n"><a href="#t6502">6502</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6503" class="pln"><span class="n"><a href="#t6503">6503</a></span><span class="t"><span class="str">            [[ 0.3242, -0.9665],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6504" class="pln"><span class="n"><a href="#t6504">6504</a></span><span class="t"><span class="str">             [ 0.4539, -0.0887]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6505" class="pln"><span class="n"><a href="#t6505">6505</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6506" class="pln"><span class="n"><a href="#t6506">6506</a></span><span class="t"><span class="str">            [[ 1.1336, -0.4025],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6507" class="pln"><span class="n"><a href="#t6507">6507</a></span><span class="t"><span class="str">             [-0.7089,  0.9032]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6508" class="pln"><span class="n"><a href="#t6508">6508</a></span><span class="t"><span class="str">    >>> A.det()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6509" class="pln"><span class="n"><a href="#t6509">6509</a></span><span class="t"><span class="str">    tensor([1.1990, 0.4099, 0.7386])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6510" class="pln"><span class="n"><a href="#t6510">6510</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6511" class="pln"><span class="n"><a href="#t6511">6511</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6512" class="run"><span class="n"><a href="#t6512">6512</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">where</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6513" class="pln"><span class="n"><a href="#t6513">6513</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6514" class="pln"><span class="n"><a href="#t6514">6514</a></span><span class="t"><span class="str">.. function:: where(condition, x, y) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6515" class="pln"><span class="n"><a href="#t6515">6515</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6516" class="pln"><span class="n"><a href="#t6516">6516</a></span><span class="t"><span class="str">Return a tensor of elements selected from either :attr:`x` or :attr:`y`, depending on :attr:`condition`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6517" class="pln"><span class="n"><a href="#t6517">6517</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6518" class="pln"><span class="n"><a href="#t6518">6518</a></span><span class="t"><span class="str">The operation is defined as:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6519" class="pln"><span class="n"><a href="#t6519">6519</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6520" class="pln"><span class="n"><a href="#t6520">6520</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6521" class="pln"><span class="n"><a href="#t6521">6521</a></span><span class="t"><span class="str">    \text{out}_i = \begin{cases}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6522" class="pln"><span class="n"><a href="#t6522">6522</a></span><span class="t"><span class="str">        \text{x}_i &amp; \text{if } \text{condition}_i \\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6523" class="pln"><span class="n"><a href="#t6523">6523</a></span><span class="t"><span class="str">        \text{y}_i &amp; \text{otherwise} \\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6524" class="pln"><span class="n"><a href="#t6524">6524</a></span><span class="t"><span class="str">    \end{cases}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6525" class="pln"><span class="n"><a href="#t6525">6525</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6526" class="pln"><span class="n"><a href="#t6526">6526</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6527" class="pln"><span class="n"><a href="#t6527">6527</a></span><span class="t"><span class="str">    The tensors :attr:`condition`, :attr:`x`, :attr:`y` must be :ref:`broadcastable &lt;broadcasting-semantics>`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6528" class="pln"><span class="n"><a href="#t6528">6528</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6529" class="pln"><span class="n"><a href="#t6529">6529</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6530" class="pln"><span class="n"><a href="#t6530">6530</a></span><span class="t"><span class="str">    condition (BoolTensor): When True (nonzero), yield x, otherwise yield y</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6531" class="pln"><span class="n"><a href="#t6531">6531</a></span><span class="t"><span class="str">    x (Tensor): values selected at indices where :attr:`condition` is ``True``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6532" class="pln"><span class="n"><a href="#t6532">6532</a></span><span class="t"><span class="str">    y (Tensor): values selected at indices where :attr:`condition` is ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6533" class="pln"><span class="n"><a href="#t6533">6533</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6534" class="pln"><span class="n"><a href="#t6534">6534</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6535" class="pln"><span class="n"><a href="#t6535">6535</a></span><span class="t"><span class="str">    Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`x`, :attr:`y`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6536" class="pln"><span class="n"><a href="#t6536">6536</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6537" class="pln"><span class="n"><a href="#t6537">6537</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6538" class="pln"><span class="n"><a href="#t6538">6538</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6539" class="pln"><span class="n"><a href="#t6539">6539</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6540" class="pln"><span class="n"><a href="#t6540">6540</a></span><span class="t"><span class="str">    >>> y = torch.ones(3, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6541" class="pln"><span class="n"><a href="#t6541">6541</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6542" class="pln"><span class="n"><a href="#t6542">6542</a></span><span class="t"><span class="str">    tensor([[-0.4620,  0.3139],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6543" class="pln"><span class="n"><a href="#t6543">6543</a></span><span class="t"><span class="str">            [ 0.3898, -0.7197],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6544" class="pln"><span class="n"><a href="#t6544">6544</a></span><span class="t"><span class="str">            [ 0.0478, -0.1657]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6545" class="pln"><span class="n"><a href="#t6545">6545</a></span><span class="t"><span class="str">    >>> torch.where(x > 0, x, y)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6546" class="pln"><span class="n"><a href="#t6546">6546</a></span><span class="t"><span class="str">    tensor([[ 1.0000,  0.3139],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6547" class="pln"><span class="n"><a href="#t6547">6547</a></span><span class="t"><span class="str">            [ 0.3898,  1.0000],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6548" class="pln"><span class="n"><a href="#t6548">6548</a></span><span class="t"><span class="str">            [ 0.0478,  1.0000]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6549" class="pln"><span class="n"><a href="#t6549">6549</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6550" class="pln"><span class="n"><a href="#t6550">6550</a></span><span class="t"><span class="str">.. function:: where(condition) -> tuple of LongTensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6551" class="pln"><span class="n"><a href="#t6551">6551</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6552" class="pln"><span class="n"><a href="#t6552">6552</a></span><span class="t"><span class="str">``torch.where(condition)`` is identical to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6553" class="pln"><span class="n"><a href="#t6553">6553</a></span><span class="t"><span class="str">``torch.nonzero(condition, as_tuple=True)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6554" class="pln"><span class="n"><a href="#t6554">6554</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6555" class="pln"><span class="n"><a href="#t6555">6555</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6556" class="pln"><span class="n"><a href="#t6556">6556</a></span><span class="t"><span class="str">    See also :func:`torch.nonzero`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6557" class="pln"><span class="n"><a href="#t6557">6557</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6558" class="pln"><span class="n"><a href="#t6558">6558</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6559" class="run"><span class="n"><a href="#t6559">6559</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">logdet</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6560" class="pln"><span class="n"><a href="#t6560">6560</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6561" class="pln"><span class="n"><a href="#t6561">6561</a></span><span class="t"><span class="str">logdet(input) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6562" class="pln"><span class="n"><a href="#t6562">6562</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6563" class="pln"><span class="n"><a href="#t6563">6563</a></span><span class="t"><span class="str">Calculates log determinant of a square matrix or batches of square matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6564" class="pln"><span class="n"><a href="#t6564">6564</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6565" class="pln"><span class="n"><a href="#t6565">6565</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6566" class="pln"><span class="n"><a href="#t6566">6566</a></span><span class="t"><span class="str">    Result is ``-inf`` if :attr:`input` has zero log determinant, and is ``nan`` if</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6567" class="pln"><span class="n"><a href="#t6567">6567</a></span><span class="t"><span class="str">    :attr:`input` has negative determinant.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6568" class="pln"><span class="n"><a href="#t6568">6568</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6569" class="pln"><span class="n"><a href="#t6569">6569</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6570" class="pln"><span class="n"><a href="#t6570">6570</a></span><span class="t"><span class="str">    Backward through :meth:`logdet` internally uses SVD results when :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6571" class="pln"><span class="n"><a href="#t6571">6571</a></span><span class="t"><span class="str">    is not invertible. In this case, double backward through :meth:`logdet` will</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6572" class="pln"><span class="n"><a href="#t6572">6572</a></span><span class="t"><span class="str">    be unstable in when :attr:`input` doesn't have distinct singular values. See</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6573" class="pln"><span class="n"><a href="#t6573">6573</a></span><span class="t"><span class="str">    :meth:`~torch.svd` for details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6574" class="pln"><span class="n"><a href="#t6574">6574</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6575" class="pln"><span class="n"><a href="#t6575">6575</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6576" class="pln"><span class="n"><a href="#t6576">6576</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size ``(*, n, n)`` where ``*`` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6577" class="pln"><span class="n"><a href="#t6577">6577</a></span><span class="t"><span class="str">                batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6578" class="pln"><span class="n"><a href="#t6578">6578</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6579" class="pln"><span class="n"><a href="#t6579">6579</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6580" class="pln"><span class="n"><a href="#t6580">6580</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6581" class="pln"><span class="n"><a href="#t6581">6581</a></span><span class="t"><span class="str">    >>> A = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6582" class="pln"><span class="n"><a href="#t6582">6582</a></span><span class="t"><span class="str">    >>> torch.det(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6583" class="pln"><span class="n"><a href="#t6583">6583</a></span><span class="t"><span class="str">    tensor(0.2611)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6584" class="pln"><span class="n"><a href="#t6584">6584</a></span><span class="t"><span class="str">    >>> torch.logdet(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6585" class="pln"><span class="n"><a href="#t6585">6585</a></span><span class="t"><span class="str">    tensor(-1.3430)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6586" class="pln"><span class="n"><a href="#t6586">6586</a></span><span class="t"><span class="str">    >>> A</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6587" class="pln"><span class="n"><a href="#t6587">6587</a></span><span class="t"><span class="str">    tensor([[[ 0.9254, -0.6213],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6588" class="pln"><span class="n"><a href="#t6588">6588</a></span><span class="t"><span class="str">             [-0.5787,  1.6843]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6589" class="pln"><span class="n"><a href="#t6589">6589</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6590" class="pln"><span class="n"><a href="#t6590">6590</a></span><span class="t"><span class="str">            [[ 0.3242, -0.9665],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6591" class="pln"><span class="n"><a href="#t6591">6591</a></span><span class="t"><span class="str">             [ 0.4539, -0.0887]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6592" class="pln"><span class="n"><a href="#t6592">6592</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6593" class="pln"><span class="n"><a href="#t6593">6593</a></span><span class="t"><span class="str">            [[ 1.1336, -0.4025],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6594" class="pln"><span class="n"><a href="#t6594">6594</a></span><span class="t"><span class="str">             [-0.7089,  0.9032]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6595" class="pln"><span class="n"><a href="#t6595">6595</a></span><span class="t"><span class="str">    >>> A.det()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6596" class="pln"><span class="n"><a href="#t6596">6596</a></span><span class="t"><span class="str">    tensor([1.1990, 0.4099, 0.7386])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6597" class="pln"><span class="n"><a href="#t6597">6597</a></span><span class="t"><span class="str">    >>> A.det().log()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6598" class="pln"><span class="n"><a href="#t6598">6598</a></span><span class="t"><span class="str">    tensor([ 0.1815, -0.8917, -0.3031])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6599" class="pln"><span class="n"><a href="#t6599">6599</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6600" class="pln"><span class="n"><a href="#t6600">6600</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6601" class="run"><span class="n"><a href="#t6601">6601</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">slogdet</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6602" class="pln"><span class="n"><a href="#t6602">6602</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6603" class="pln"><span class="n"><a href="#t6603">6603</a></span><span class="t"><span class="str">slogdet(input) -> (Tensor, Tensor)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6604" class="pln"><span class="n"><a href="#t6604">6604</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6605" class="pln"><span class="n"><a href="#t6605">6605</a></span><span class="t"><span class="str">Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6606" class="pln"><span class="n"><a href="#t6606">6606</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6607" class="pln"><span class="n"><a href="#t6607">6607</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6608" class="pln"><span class="n"><a href="#t6608">6608</a></span><span class="t"><span class="str">    If ``input`` has zero determinant, this returns ``(0, -inf)``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6609" class="pln"><span class="n"><a href="#t6609">6609</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6610" class="pln"><span class="n"><a href="#t6610">6610</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6611" class="pln"><span class="n"><a href="#t6611">6611</a></span><span class="t"><span class="str">    Backward through :meth:`slogdet` internally uses SVD results when :attr:`input`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6612" class="pln"><span class="n"><a href="#t6612">6612</a></span><span class="t"><span class="str">    is not invertible. In this case, double backward through :meth:`slogdet`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6613" class="pln"><span class="n"><a href="#t6613">6613</a></span><span class="t"><span class="str">    will be unstable in when :attr:`input` doesn't have distinct singular values.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6614" class="pln"><span class="n"><a href="#t6614">6614</a></span><span class="t"><span class="str">    See :meth:`~torch.svd` for details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6615" class="pln"><span class="n"><a href="#t6615">6615</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6616" class="pln"><span class="n"><a href="#t6616">6616</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6617" class="pln"><span class="n"><a href="#t6617">6617</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of size ``(*, n, n)`` where ``*`` is zero or more</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6618" class="pln"><span class="n"><a href="#t6618">6618</a></span><span class="t"><span class="str">                batch dimensions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6619" class="pln"><span class="n"><a href="#t6619">6619</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6620" class="pln"><span class="n"><a href="#t6620">6620</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6621" class="pln"><span class="n"><a href="#t6621">6621</a></span><span class="t"><span class="str">    A namedtuple (sign, logabsdet) containing the sign of the determinant, and the log</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6622" class="pln"><span class="n"><a href="#t6622">6622</a></span><span class="t"><span class="str">    value of the absolute determinant.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6623" class="pln"><span class="n"><a href="#t6623">6623</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6624" class="pln"><span class="n"><a href="#t6624">6624</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6625" class="pln"><span class="n"><a href="#t6625">6625</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6626" class="pln"><span class="n"><a href="#t6626">6626</a></span><span class="t"><span class="str">    >>> A = torch.randn(3, 3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6627" class="pln"><span class="n"><a href="#t6627">6627</a></span><span class="t"><span class="str">    >>> A</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6628" class="pln"><span class="n"><a href="#t6628">6628</a></span><span class="t"><span class="str">    tensor([[ 0.0032, -0.2239, -1.1219],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6629" class="pln"><span class="n"><a href="#t6629">6629</a></span><span class="t"><span class="str">            [-0.6690,  0.1161,  0.4053],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6630" class="pln"><span class="n"><a href="#t6630">6630</a></span><span class="t"><span class="str">            [-1.6218, -0.9273, -0.0082]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6631" class="pln"><span class="n"><a href="#t6631">6631</a></span><span class="t"><span class="str">    >>> torch.det(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6632" class="pln"><span class="n"><a href="#t6632">6632</a></span><span class="t"><span class="str">    tensor(-0.7576)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6633" class="pln"><span class="n"><a href="#t6633">6633</a></span><span class="t"><span class="str">    >>> torch.logdet(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6634" class="pln"><span class="n"><a href="#t6634">6634</a></span><span class="t"><span class="str">    tensor(nan)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6635" class="pln"><span class="n"><a href="#t6635">6635</a></span><span class="t"><span class="str">    >>> torch.slogdet(A)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6636" class="pln"><span class="n"><a href="#t6636">6636</a></span><span class="t"><span class="str">    torch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6637" class="pln"><span class="n"><a href="#t6637">6637</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6638" class="pln"><span class="n"><a href="#t6638">6638</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6639" class="run"><span class="n"><a href="#t6639">6639</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">pinverse</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6640" class="pln"><span class="n"><a href="#t6640">6640</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6641" class="pln"><span class="n"><a href="#t6641">6641</a></span><span class="t"><span class="str">pinverse(input, rcond=1e-15) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6642" class="pln"><span class="n"><a href="#t6642">6642</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6643" class="pln"><span class="n"><a href="#t6643">6643</a></span><span class="t"><span class="str">Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6644" class="pln"><span class="n"><a href="#t6644">6644</a></span><span class="t"><span class="str">Please look at `Moore-Penrose inverse`_ for more details</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6645" class="pln"><span class="n"><a href="#t6645">6645</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6646" class="pln"><span class="n"><a href="#t6646">6646</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6647" class="pln"><span class="n"><a href="#t6647">6647</a></span><span class="t"><span class="str">    This method is implemented using the Singular Value Decomposition.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6648" class="pln"><span class="n"><a href="#t6648">6648</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6649" class="pln"><span class="n"><a href="#t6649">6649</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6650" class="pln"><span class="n"><a href="#t6650">6650</a></span><span class="t"><span class="str">    The pseudo-inverse is not necessarily a continuous function in the elements of the matrix `[1]`_.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6651" class="pln"><span class="n"><a href="#t6651">6651</a></span><span class="t"><span class="str">    Therefore, derivatives are not always existent, and exist for a constant rank only `[2]`_.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6652" class="pln"><span class="n"><a href="#t6652">6652</a></span><span class="t"><span class="str">    However, this method is backprop-able due to the implementation by using SVD results, and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6653" class="pln"><span class="n"><a href="#t6653">6653</a></span><span class="t"><span class="str">    could be unstable. Double-backward will also be unstable due to the usage of SVD internally.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6654" class="pln"><span class="n"><a href="#t6654">6654</a></span><span class="t"><span class="str">    See :meth:`~torch.svd` for more details.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6655" class="pln"><span class="n"><a href="#t6655">6655</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6656" class="pln"><span class="n"><a href="#t6656">6656</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6657" class="pln"><span class="n"><a href="#t6657">6657</a></span><span class="t"><span class="str">    input (Tensor): The input tensor of size :math:`(*, m, n)` where :math:`*` is zero or more batch dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6658" class="pln"><span class="n"><a href="#t6658">6658</a></span><span class="t"><span class="str">    rcond (float): A floating point value to determine the cutoff for small singular values.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6659" class="pln"><span class="n"><a href="#t6659">6659</a></span><span class="t"><span class="str">                   Default: 1e-15</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6660" class="pln"><span class="n"><a href="#t6660">6660</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6661" class="pln"><span class="n"><a href="#t6661">6661</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6662" class="pln"><span class="n"><a href="#t6662">6662</a></span><span class="t"><span class="str">    The pseudo-inverse of :attr:`input` of dimensions :math:`(*, n, m)`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6663" class="pln"><span class="n"><a href="#t6663">6663</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6664" class="pln"><span class="n"><a href="#t6664">6664</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6665" class="pln"><span class="n"><a href="#t6665">6665</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6666" class="pln"><span class="n"><a href="#t6666">6666</a></span><span class="t"><span class="str">    >>> input = torch.randn(3, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6667" class="pln"><span class="n"><a href="#t6667">6667</a></span><span class="t"><span class="str">    >>> input</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6668" class="pln"><span class="n"><a href="#t6668">6668</a></span><span class="t"><span class="str">    tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6669" class="pln"><span class="n"><a href="#t6669">6669</a></span><span class="t"><span class="str">            [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6670" class="pln"><span class="n"><a href="#t6670">6670</a></span><span class="t"><span class="str">            [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6671" class="pln"><span class="n"><a href="#t6671">6671</a></span><span class="t"><span class="str">    >>> torch.pinverse(input)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6672" class="pln"><span class="n"><a href="#t6672">6672</a></span><span class="t"><span class="str">    tensor([[ 0.0600, -0.1933, -0.2090],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6673" class="pln"><span class="n"><a href="#t6673">6673</a></span><span class="t"><span class="str">            [-0.0903, -0.0817, -0.4752],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6674" class="pln"><span class="n"><a href="#t6674">6674</a></span><span class="t"><span class="str">            [-0.7124, -0.1631, -0.2272],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6675" class="pln"><span class="n"><a href="#t6675">6675</a></span><span class="t"><span class="str">            [ 0.1356,  0.3933, -0.5023],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6676" class="pln"><span class="n"><a href="#t6676">6676</a></span><span class="t"><span class="str">            [-0.0308, -0.1725, -0.5216]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6677" class="pln"><span class="n"><a href="#t6677">6677</a></span><span class="t"><span class="str">    >>> # Batched pinverse example</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6678" class="pln"><span class="n"><a href="#t6678">6678</a></span><span class="t"><span class="str">    >>> a = torch.randn(2,6,3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6679" class="pln"><span class="n"><a href="#t6679">6679</a></span><span class="t"><span class="str">    >>> b = torch.pinverse(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6680" class="pln"><span class="n"><a href="#t6680">6680</a></span><span class="t"><span class="str">    >>> torch.matmul(b, a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6681" class="pln"><span class="n"><a href="#t6681">6681</a></span><span class="t"><span class="str">    tensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6682" class="pln"><span class="n"><a href="#t6682">6682</a></span><span class="t"><span class="str">            [ 8.3121e-08,  1.0000e+00, -2.7567e-07],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6683" class="pln"><span class="n"><a href="#t6683">6683</a></span><span class="t"><span class="str">            [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6684" class="pln"><span class="n"><a href="#t6684">6684</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6685" class="pln"><span class="n"><a href="#t6685">6685</a></span><span class="t"><span class="str">            [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6686" class="pln"><span class="n"><a href="#t6686">6686</a></span><span class="t"><span class="str">            [-2.2352e-07,  1.0000e+00,  1.1921e-07],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6687" class="pln"><span class="n"><a href="#t6687">6687</a></span><span class="t"><span class="str">            [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6688" class="pln"><span class="n"><a href="#t6688">6688</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6689" class="pln"><span class="n"><a href="#t6689">6689</a></span><span class="t"><span class="str">.. _Moore-Penrose inverse: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6690" class="pln"><span class="n"><a href="#t6690">6690</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6691" class="pln"><span class="n"><a href="#t6691">6691</a></span><span class="t"><span class="str">.. _[1]: https://epubs.siam.org/doi/10.1137/0117004</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6692" class="pln"><span class="n"><a href="#t6692">6692</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6693" class="pln"><span class="n"><a href="#t6693">6693</a></span><span class="t"><span class="str">.. _[2]: https://www.jstor.org/stable/2156365</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6694" class="pln"><span class="n"><a href="#t6694">6694</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6695" class="pln"><span class="n"><a href="#t6695">6695</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6696" class="run"><span class="n"><a href="#t6696">6696</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">fft</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6697" class="pln"><span class="n"><a href="#t6697">6697</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6698" class="pln"><span class="n"><a href="#t6698">6698</a></span><span class="t"><span class="str">fft(input, signal_ndim, normalized=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6699" class="pln"><span class="n"><a href="#t6699">6699</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6700" class="pln"><span class="n"><a href="#t6700">6700</a></span><span class="t"><span class="str">Complex-to-complex Discrete Fourier Transform</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6701" class="pln"><span class="n"><a href="#t6701">6701</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6702" class="pln"><span class="n"><a href="#t6702">6702</a></span><span class="t"><span class="str">This method computes the complex-to-complex discrete Fourier transform.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6703" class="pln"><span class="n"><a href="#t6703">6703</a></span><span class="t"><span class="str">Ignoring the batch dimensions, it computes the following expression:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6704" class="pln"><span class="n"><a href="#t6704">6704</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6705" class="pln"><span class="n"><a href="#t6705">6705</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6706" class="pln"><span class="n"><a href="#t6706">6706</a></span><span class="t"><span class="str">    X[\omega_1, \dots, \omega_d] =</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6707" class="pln"><span class="n"><a href="#t6707">6707</a></span><span class="t"><span class="str">        \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6708" class="pln"><span class="n"><a href="#t6708">6708</a></span><span class="t"><span class="str">         e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6709" class="pln"><span class="n"><a href="#t6709">6709</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6710" class="pln"><span class="n"><a href="#t6710">6710</a></span><span class="t"><span class="str">where :math:`d` = :attr:`signal_ndim` is number of dimensions for the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6711" class="pln"><span class="n"><a href="#t6711">6711</a></span><span class="t"><span class="str">signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6712" class="pln"><span class="n"><a href="#t6712">6712</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6713" class="pln"><span class="n"><a href="#t6713">6713</a></span><span class="t"><span class="str">This method supports 1D, 2D and 3D complex-to-complex transforms, indicated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6714" class="pln"><span class="n"><a href="#t6714">6714</a></span><span class="t"><span class="str">by :attr:`signal_ndim`. :attr:`input` must be a tensor with last dimension</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6715" class="pln"><span class="n"><a href="#t6715">6715</a></span><span class="t"><span class="str">of size 2, representing the real and imaginary components of complex</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6716" class="pln"><span class="n"><a href="#t6716">6716</a></span><span class="t"><span class="str">numbers, and should have at least ``signal_ndim + 1`` dimensions with optionally</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6717" class="pln"><span class="n"><a href="#t6717">6717</a></span><span class="t"><span class="str">arbitrary number of leading batch dimensions. If :attr:`normalized` is set to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6718" class="pln"><span class="n"><a href="#t6718">6718</a></span><span class="t"><span class="str">``True``, this normalizes the result by dividing it with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6719" class="pln"><span class="n"><a href="#t6719">6719</a></span><span class="t"><span class="str">:math:`\sqrt{\prod_{i=1}^K N_i}` so that the operator is unitary.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6720" class="pln"><span class="n"><a href="#t6720">6720</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6721" class="pln"><span class="n"><a href="#t6721">6721</a></span><span class="t"><span class="str">Returns the real and the imaginary parts together as one tensor of the same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6722" class="pln"><span class="n"><a href="#t6722">6722</a></span><span class="t"><span class="str">shape of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6723" class="pln"><span class="n"><a href="#t6723">6723</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6724" class="pln"><span class="n"><a href="#t6724">6724</a></span><span class="t"><span class="str">The inverse of this function is :func:`~torch.ifft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6725" class="pln"><span class="n"><a href="#t6725">6725</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6726" class="pln"><span class="n"><a href="#t6726">6726</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6727" class="pln"><span class="n"><a href="#t6727">6727</a></span><span class="t"><span class="str">    For CUDA tensors, an LRU cache is used for cuFFT plans to speed up</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6728" class="pln"><span class="n"><a href="#t6728">6728</a></span><span class="t"><span class="str">    repeatedly running FFT methods on tensors of same geometry with same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6729" class="pln"><span class="n"><a href="#t6729">6729</a></span><span class="t"><span class="str">    configuration. See :ref:`cufft-plan-cache` for more details on how to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6730" class="pln"><span class="n"><a href="#t6730">6730</a></span><span class="t"><span class="str">    monitor and control the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6731" class="pln"><span class="n"><a href="#t6731">6731</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6732" class="pln"><span class="n"><a href="#t6732">6732</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6733" class="pln"><span class="n"><a href="#t6733">6733</a></span><span class="t"><span class="str">    For CPU tensors, this method is currently only available with MKL. Use</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6734" class="pln"><span class="n"><a href="#t6734">6734</a></span><span class="t"><span class="str">    :func:`torch.backends.mkl.is_available` to check if MKL is installed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6735" class="pln"><span class="n"><a href="#t6735">6735</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6736" class="pln"><span class="n"><a href="#t6736">6736</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6737" class="pln"><span class="n"><a href="#t6737">6737</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of at least :attr:`signal_ndim` ``+ 1``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6738" class="pln"><span class="n"><a href="#t6738">6738</a></span><span class="t"><span class="str">        dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6739" class="pln"><span class="n"><a href="#t6739">6739</a></span><span class="t"><span class="str">    signal_ndim (int): the number of dimensions in each signal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6740" class="pln"><span class="n"><a href="#t6740">6740</a></span><span class="t"><span class="str">        :attr:`signal_ndim` can only be 1, 2 or 3</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6741" class="pln"><span class="n"><a href="#t6741">6741</a></span><span class="t"><span class="str">    normalized (bool, optional): controls whether to return normalized results.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6742" class="pln"><span class="n"><a href="#t6742">6742</a></span><span class="t"><span class="str">        Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6743" class="pln"><span class="n"><a href="#t6743">6743</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6744" class="pln"><span class="n"><a href="#t6744">6744</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6745" class="pln"><span class="n"><a href="#t6745">6745</a></span><span class="t"><span class="str">    Tensor: A tensor containing the complex-to-complex Fourier transform result</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6746" class="pln"><span class="n"><a href="#t6746">6746</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6747" class="pln"><span class="n"><a href="#t6747">6747</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6748" class="pln"><span class="n"><a href="#t6748">6748</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6749" class="pln"><span class="n"><a href="#t6749">6749</a></span><span class="t"><span class="str">    >>> # unbatched 2D FFT</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6750" class="pln"><span class="n"><a href="#t6750">6750</a></span><span class="t"><span class="str">    >>> x = torch.randn(4, 3, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6751" class="pln"><span class="n"><a href="#t6751">6751</a></span><span class="t"><span class="str">    >>> torch.fft(x, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6752" class="pln"><span class="n"><a href="#t6752">6752</a></span><span class="t"><span class="str">    tensor([[[-0.0876,  1.7835],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6753" class="pln"><span class="n"><a href="#t6753">6753</a></span><span class="t"><span class="str">             [-2.0399, -2.9754],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6754" class="pln"><span class="n"><a href="#t6754">6754</a></span><span class="t"><span class="str">             [ 4.4773, -5.0119]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6755" class="pln"><span class="n"><a href="#t6755">6755</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6756" class="pln"><span class="n"><a href="#t6756">6756</a></span><span class="t"><span class="str">            [[-1.5716,  2.7631],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6757" class="pln"><span class="n"><a href="#t6757">6757</a></span><span class="t"><span class="str">             [-3.8846,  5.2652],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6758" class="pln"><span class="n"><a href="#t6758">6758</a></span><span class="t"><span class="str">             [ 0.2046, -0.7088]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6759" class="pln"><span class="n"><a href="#t6759">6759</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6760" class="pln"><span class="n"><a href="#t6760">6760</a></span><span class="t"><span class="str">            [[ 1.9938, -0.5901],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6761" class="pln"><span class="n"><a href="#t6761">6761</a></span><span class="t"><span class="str">             [ 6.5637,  6.4556],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6762" class="pln"><span class="n"><a href="#t6762">6762</a></span><span class="t"><span class="str">             [ 2.9865,  4.9318]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6763" class="pln"><span class="n"><a href="#t6763">6763</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6764" class="pln"><span class="n"><a href="#t6764">6764</a></span><span class="t"><span class="str">            [[ 7.0193,  1.1742],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6765" class="pln"><span class="n"><a href="#t6765">6765</a></span><span class="t"><span class="str">             [-1.3717, -2.1084],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6766" class="pln"><span class="n"><a href="#t6766">6766</a></span><span class="t"><span class="str">             [ 2.0289,  2.9357]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6767" class="pln"><span class="n"><a href="#t6767">6767</a></span><span class="t"><span class="str">    >>> # batched 1D FFT</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6768" class="pln"><span class="n"><a href="#t6768">6768</a></span><span class="t"><span class="str">    >>> torch.fft(x, 1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6769" class="pln"><span class="n"><a href="#t6769">6769</a></span><span class="t"><span class="str">    tensor([[[ 1.8385,  1.2827],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6770" class="pln"><span class="n"><a href="#t6770">6770</a></span><span class="t"><span class="str">             [-0.1831,  1.6593],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6771" class="pln"><span class="n"><a href="#t6771">6771</a></span><span class="t"><span class="str">             [ 2.4243,  0.5367]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6772" class="pln"><span class="n"><a href="#t6772">6772</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6773" class="pln"><span class="n"><a href="#t6773">6773</a></span><span class="t"><span class="str">            [[-0.9176, -1.5543],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6774" class="pln"><span class="n"><a href="#t6774">6774</a></span><span class="t"><span class="str">             [-3.9943, -2.9860],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6775" class="pln"><span class="n"><a href="#t6775">6775</a></span><span class="t"><span class="str">             [ 1.2838, -2.9420]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6776" class="pln"><span class="n"><a href="#t6776">6776</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6777" class="pln"><span class="n"><a href="#t6777">6777</a></span><span class="t"><span class="str">            [[-0.8854, -0.6860],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6778" class="pln"><span class="n"><a href="#t6778">6778</a></span><span class="t"><span class="str">             [ 2.4450,  0.0808],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6779" class="pln"><span class="n"><a href="#t6779">6779</a></span><span class="t"><span class="str">             [ 1.3076, -0.5768]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6780" class="pln"><span class="n"><a href="#t6780">6780</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6781" class="pln"><span class="n"><a href="#t6781">6781</a></span><span class="t"><span class="str">            [[-0.1231,  2.7411],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6782" class="pln"><span class="n"><a href="#t6782">6782</a></span><span class="t"><span class="str">             [-0.3075, -1.7295],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6783" class="pln"><span class="n"><a href="#t6783">6783</a></span><span class="t"><span class="str">             [-0.5384, -2.0299]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6784" class="pln"><span class="n"><a href="#t6784">6784</a></span><span class="t"><span class="str">    >>> # arbitrary number of batch dimensions, 2D FFT</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6785" class="pln"><span class="n"><a href="#t6785">6785</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 3, 5, 5, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6786" class="pln"><span class="n"><a href="#t6786">6786</a></span><span class="t"><span class="str">    >>> y = torch.fft(x, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6787" class="pln"><span class="n"><a href="#t6787">6787</a></span><span class="t"><span class="str">    >>> y.shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6788" class="pln"><span class="n"><a href="#t6788">6788</a></span><span class="t"><span class="str">    torch.Size([3, 3, 5, 5, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6789" class="pln"><span class="n"><a href="#t6789">6789</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6790" class="pln"><span class="n"><a href="#t6790">6790</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6791" class="pln"><span class="n"><a href="#t6791">6791</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6792" class="run"><span class="n"><a href="#t6792">6792</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ifft</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6793" class="pln"><span class="n"><a href="#t6793">6793</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6794" class="pln"><span class="n"><a href="#t6794">6794</a></span><span class="t"><span class="str">ifft(input, signal_ndim, normalized=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6795" class="pln"><span class="n"><a href="#t6795">6795</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6796" class="pln"><span class="n"><a href="#t6796">6796</a></span><span class="t"><span class="str">Complex-to-complex Inverse Discrete Fourier Transform</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6797" class="pln"><span class="n"><a href="#t6797">6797</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6798" class="pln"><span class="n"><a href="#t6798">6798</a></span><span class="t"><span class="str">This method computes the complex-to-complex inverse discrete Fourier</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6799" class="pln"><span class="n"><a href="#t6799">6799</a></span><span class="t"><span class="str">transform. Ignoring the batch dimensions, it computes the following</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6800" class="pln"><span class="n"><a href="#t6800">6800</a></span><span class="t"><span class="str">expression:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6801" class="pln"><span class="n"><a href="#t6801">6801</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6802" class="pln"><span class="n"><a href="#t6802">6802</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6803" class="pln"><span class="n"><a href="#t6803">6803</a></span><span class="t"><span class="str">    X[\omega_1, \dots, \omega_d] =</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6804" class="pln"><span class="n"><a href="#t6804">6804</a></span><span class="t"><span class="str">        \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6805" class="pln"><span class="n"><a href="#t6805">6805</a></span><span class="t"><span class="str">         e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6806" class="pln"><span class="n"><a href="#t6806">6806</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6807" class="pln"><span class="n"><a href="#t6807">6807</a></span><span class="t"><span class="str">where :math:`d` = :attr:`signal_ndim` is number of dimensions for the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6808" class="pln"><span class="n"><a href="#t6808">6808</a></span><span class="t"><span class="str">signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6809" class="pln"><span class="n"><a href="#t6809">6809</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6810" class="pln"><span class="n"><a href="#t6810">6810</a></span><span class="t"><span class="str">The argument specifications are almost identical with :func:`~torch.fft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6811" class="pln"><span class="n"><a href="#t6811">6811</a></span><span class="t"><span class="str">However, if :attr:`normalized` is set to ``True``, this instead returns the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6812" class="pln"><span class="n"><a href="#t6812">6812</a></span><span class="t"><span class="str">results multiplied by :math:`\sqrt{\prod_{i=1}^d N_i}`, to become a unitary</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6813" class="pln"><span class="n"><a href="#t6813">6813</a></span><span class="t"><span class="str">operator. Therefore, to invert a :func:`~torch.fft`, the :attr:`normalized`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6814" class="pln"><span class="n"><a href="#t6814">6814</a></span><span class="t"><span class="str">argument should be set identically for :func:`~torch.fft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6815" class="pln"><span class="n"><a href="#t6815">6815</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6816" class="pln"><span class="n"><a href="#t6816">6816</a></span><span class="t"><span class="str">Returns the real and the imaginary parts together as one tensor of the same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6817" class="pln"><span class="n"><a href="#t6817">6817</a></span><span class="t"><span class="str">shape of :attr:`input`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6818" class="pln"><span class="n"><a href="#t6818">6818</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6819" class="pln"><span class="n"><a href="#t6819">6819</a></span><span class="t"><span class="str">The inverse of this function is :func:`~torch.fft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6820" class="pln"><span class="n"><a href="#t6820">6820</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6821" class="pln"><span class="n"><a href="#t6821">6821</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6822" class="pln"><span class="n"><a href="#t6822">6822</a></span><span class="t"><span class="str">    For CUDA tensors, an LRU cache is used for cuFFT plans to speed up</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6823" class="pln"><span class="n"><a href="#t6823">6823</a></span><span class="t"><span class="str">    repeatedly running FFT methods on tensors of same geometry with same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6824" class="pln"><span class="n"><a href="#t6824">6824</a></span><span class="t"><span class="str">    configuration. See :ref:`cufft-plan-cache` for more details on how to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6825" class="pln"><span class="n"><a href="#t6825">6825</a></span><span class="t"><span class="str">    monitor and control the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6826" class="pln"><span class="n"><a href="#t6826">6826</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6827" class="pln"><span class="n"><a href="#t6827">6827</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6828" class="pln"><span class="n"><a href="#t6828">6828</a></span><span class="t"><span class="str">    For CPU tensors, this method is currently only available with MKL. Use</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6829" class="pln"><span class="n"><a href="#t6829">6829</a></span><span class="t"><span class="str">    :func:`torch.backends.mkl.is_available` to check if MKL is installed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6830" class="pln"><span class="n"><a href="#t6830">6830</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6831" class="pln"><span class="n"><a href="#t6831">6831</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6832" class="pln"><span class="n"><a href="#t6832">6832</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of at least :attr:`signal_ndim` ``+ 1``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6833" class="pln"><span class="n"><a href="#t6833">6833</a></span><span class="t"><span class="str">        dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6834" class="pln"><span class="n"><a href="#t6834">6834</a></span><span class="t"><span class="str">    signal_ndim (int): the number of dimensions in each signal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6835" class="pln"><span class="n"><a href="#t6835">6835</a></span><span class="t"><span class="str">        :attr:`signal_ndim` can only be 1, 2 or 3</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6836" class="pln"><span class="n"><a href="#t6836">6836</a></span><span class="t"><span class="str">    normalized (bool, optional): controls whether to return normalized results.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6837" class="pln"><span class="n"><a href="#t6837">6837</a></span><span class="t"><span class="str">        Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6838" class="pln"><span class="n"><a href="#t6838">6838</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6839" class="pln"><span class="n"><a href="#t6839">6839</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6840" class="pln"><span class="n"><a href="#t6840">6840</a></span><span class="t"><span class="str">    Tensor: A tensor containing the complex-to-complex inverse Fourier transform result</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6841" class="pln"><span class="n"><a href="#t6841">6841</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6842" class="pln"><span class="n"><a href="#t6842">6842</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6843" class="pln"><span class="n"><a href="#t6843">6843</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6844" class="pln"><span class="n"><a href="#t6844">6844</a></span><span class="t"><span class="str">    >>> x = torch.randn(3, 3, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6845" class="pln"><span class="n"><a href="#t6845">6845</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6846" class="pln"><span class="n"><a href="#t6846">6846</a></span><span class="t"><span class="str">    tensor([[[ 1.2766,  1.3680],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6847" class="pln"><span class="n"><a href="#t6847">6847</a></span><span class="t"><span class="str">             [-0.8337,  2.0251],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6848" class="pln"><span class="n"><a href="#t6848">6848</a></span><span class="t"><span class="str">             [ 0.9465, -1.4390]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6849" class="pln"><span class="n"><a href="#t6849">6849</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6850" class="pln"><span class="n"><a href="#t6850">6850</a></span><span class="t"><span class="str">            [[-0.1890,  1.6010],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6851" class="pln"><span class="n"><a href="#t6851">6851</a></span><span class="t"><span class="str">             [ 1.1034, -1.9230],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6852" class="pln"><span class="n"><a href="#t6852">6852</a></span><span class="t"><span class="str">             [-0.9482,  1.0775]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6853" class="pln"><span class="n"><a href="#t6853">6853</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6854" class="pln"><span class="n"><a href="#t6854">6854</a></span><span class="t"><span class="str">            [[-0.7708, -0.8176],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6855" class="pln"><span class="n"><a href="#t6855">6855</a></span><span class="t"><span class="str">             [-0.1843, -0.2287],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6856" class="pln"><span class="n"><a href="#t6856">6856</a></span><span class="t"><span class="str">             [-1.9034, -0.2196]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6857" class="pln"><span class="n"><a href="#t6857">6857</a></span><span class="t"><span class="str">    >>> y = torch.fft(x, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6858" class="pln"><span class="n"><a href="#t6858">6858</a></span><span class="t"><span class="str">    >>> torch.ifft(y, 2)  # recover x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6859" class="pln"><span class="n"><a href="#t6859">6859</a></span><span class="t"><span class="str">    tensor([[[ 1.2766,  1.3680],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6860" class="pln"><span class="n"><a href="#t6860">6860</a></span><span class="t"><span class="str">             [-0.8337,  2.0251],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6861" class="pln"><span class="n"><a href="#t6861">6861</a></span><span class="t"><span class="str">             [ 0.9465, -1.4390]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6862" class="pln"><span class="n"><a href="#t6862">6862</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6863" class="pln"><span class="n"><a href="#t6863">6863</a></span><span class="t"><span class="str">            [[-0.1890,  1.6010],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6864" class="pln"><span class="n"><a href="#t6864">6864</a></span><span class="t"><span class="str">             [ 1.1034, -1.9230],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6865" class="pln"><span class="n"><a href="#t6865">6865</a></span><span class="t"><span class="str">             [-0.9482,  1.0775]],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6866" class="pln"><span class="n"><a href="#t6866">6866</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6867" class="pln"><span class="n"><a href="#t6867">6867</a></span><span class="t"><span class="str">            [[-0.7708, -0.8176],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6868" class="pln"><span class="n"><a href="#t6868">6868</a></span><span class="t"><span class="str">             [-0.1843, -0.2287],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6869" class="pln"><span class="n"><a href="#t6869">6869</a></span><span class="t"><span class="str">             [-1.9034, -0.2196]]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6870" class="pln"><span class="n"><a href="#t6870">6870</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6871" class="pln"><span class="n"><a href="#t6871">6871</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6872" class="pln"><span class="n"><a href="#t6872">6872</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6873" class="run"><span class="n"><a href="#t6873">6873</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">rfft</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6874" class="pln"><span class="n"><a href="#t6874">6874</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6875" class="pln"><span class="n"><a href="#t6875">6875</a></span><span class="t"><span class="str">rfft(input, signal_ndim, normalized=False, onesided=True) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6876" class="pln"><span class="n"><a href="#t6876">6876</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6877" class="pln"><span class="n"><a href="#t6877">6877</a></span><span class="t"><span class="str">Real-to-complex Discrete Fourier Transform</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6878" class="pln"><span class="n"><a href="#t6878">6878</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6879" class="pln"><span class="n"><a href="#t6879">6879</a></span><span class="t"><span class="str">This method computes the real-to-complex discrete Fourier transform. It is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6880" class="pln"><span class="n"><a href="#t6880">6880</a></span><span class="t"><span class="str">mathematically equivalent with :func:`~torch.fft` with differences only in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6881" class="pln"><span class="n"><a href="#t6881">6881</a></span><span class="t"><span class="str">formats of the input and output.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6882" class="pln"><span class="n"><a href="#t6882">6882</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6883" class="pln"><span class="n"><a href="#t6883">6883</a></span><span class="t"><span class="str">This method supports 1D, 2D and 3D real-to-complex transforms, indicated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6884" class="pln"><span class="n"><a href="#t6884">6884</a></span><span class="t"><span class="str">by :attr:`signal_ndim`. :attr:`input` must be a tensor with at least</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6885" class="pln"><span class="n"><a href="#t6885">6885</a></span><span class="t"><span class="str">``signal_ndim`` dimensions with optionally arbitrary number of leading batch</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6886" class="pln"><span class="n"><a href="#t6886">6886</a></span><span class="t"><span class="str">dimensions. If :attr:`normalized` is set to ``True``, this normalizes the result</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6887" class="pln"><span class="n"><a href="#t6887">6887</a></span><span class="t"><span class="str">by dividing it with :math:`\sqrt{\prod_{i=1}^K N_i}` so that the operator is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6888" class="pln"><span class="n"><a href="#t6888">6888</a></span><span class="t"><span class="str">unitary, where :math:`N_i` is the size of signal dimension :math:`i`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6889" class="pln"><span class="n"><a href="#t6889">6889</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6890" class="pln"><span class="n"><a href="#t6890">6890</a></span><span class="t"><span class="str">The real-to-complex Fourier transform results follow conjugate symmetry:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6891" class="pln"><span class="n"><a href="#t6891">6891</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6892" class="pln"><span class="n"><a href="#t6892">6892</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6893" class="pln"><span class="n"><a href="#t6893">6893</a></span><span class="t"><span class="str">    X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6894" class="pln"><span class="n"><a href="#t6894">6894</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6895" class="pln"><span class="n"><a href="#t6895">6895</a></span><span class="t"><span class="str">where the index arithmetic is computed modulus the size of the corresponding</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6896" class="pln"><span class="n"><a href="#t6896">6896</a></span><span class="t"><span class="str">dimension, :math:`\ ^*` is the conjugate operator, and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6897" class="pln"><span class="n"><a href="#t6897">6897</a></span><span class="t"><span class="str">:math:`d` = :attr:`signal_ndim`. :attr:`onesided` flag controls whether to avoid</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6898" class="pln"><span class="n"><a href="#t6898">6898</a></span><span class="t"><span class="str">redundancy in the output results. If set to ``True`` (default), the output will</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6899" class="pln"><span class="n"><a href="#t6899">6899</a></span><span class="t"><span class="str">not be full complex result of shape :math:`(*, 2)`, where :math:`*` is the shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6900" class="pln"><span class="n"><a href="#t6900">6900</a></span><span class="t"><span class="str">of :attr:`input`, but instead the last dimension will be halfed as of size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6901" class="pln"><span class="n"><a href="#t6901">6901</a></span><span class="t"><span class="str">:math:`\lfloor \frac{N_d}{2} \rfloor + 1`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6902" class="pln"><span class="n"><a href="#t6902">6902</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6903" class="pln"><span class="n"><a href="#t6903">6903</a></span><span class="t"><span class="str">The inverse of this function is :func:`~torch.irfft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6904" class="pln"><span class="n"><a href="#t6904">6904</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6905" class="pln"><span class="n"><a href="#t6905">6905</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6906" class="pln"><span class="n"><a href="#t6906">6906</a></span><span class="t"><span class="str">    For CUDA tensors, an LRU cache is used for cuFFT plans to speed up</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6907" class="pln"><span class="n"><a href="#t6907">6907</a></span><span class="t"><span class="str">    repeatedly running FFT methods on tensors of same geometry with same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6908" class="pln"><span class="n"><a href="#t6908">6908</a></span><span class="t"><span class="str">    configuration. See :ref:`cufft-plan-cache` for more details on how to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6909" class="pln"><span class="n"><a href="#t6909">6909</a></span><span class="t"><span class="str">    monitor and control the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6910" class="pln"><span class="n"><a href="#t6910">6910</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6911" class="pln"><span class="n"><a href="#t6911">6911</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6912" class="pln"><span class="n"><a href="#t6912">6912</a></span><span class="t"><span class="str">    For CPU tensors, this method is currently only available with MKL. Use</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6913" class="pln"><span class="n"><a href="#t6913">6913</a></span><span class="t"><span class="str">    :func:`torch.backends.mkl.is_available` to check if MKL is installed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6914" class="pln"><span class="n"><a href="#t6914">6914</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6915" class="pln"><span class="n"><a href="#t6915">6915</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6916" class="pln"><span class="n"><a href="#t6916">6916</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of at least :attr:`signal_ndim` dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6917" class="pln"><span class="n"><a href="#t6917">6917</a></span><span class="t"><span class="str">    signal_ndim (int): the number of dimensions in each signal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6918" class="pln"><span class="n"><a href="#t6918">6918</a></span><span class="t"><span class="str">        :attr:`signal_ndim` can only be 1, 2 or 3</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6919" class="pln"><span class="n"><a href="#t6919">6919</a></span><span class="t"><span class="str">    normalized (bool, optional): controls whether to return normalized results.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6920" class="pln"><span class="n"><a href="#t6920">6920</a></span><span class="t"><span class="str">        Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6921" class="pln"><span class="n"><a href="#t6921">6921</a></span><span class="t"><span class="str">    onesided (bool, optional): controls whether to return half of results to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6922" class="pln"><span class="n"><a href="#t6922">6922</a></span><span class="t"><span class="str">        avoid redundancy. Default: ``True``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6923" class="pln"><span class="n"><a href="#t6923">6923</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6924" class="pln"><span class="n"><a href="#t6924">6924</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6925" class="pln"><span class="n"><a href="#t6925">6925</a></span><span class="t"><span class="str">    Tensor: A tensor containing the real-to-complex Fourier transform result</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6926" class="pln"><span class="n"><a href="#t6926">6926</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6927" class="pln"><span class="n"><a href="#t6927">6927</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6928" class="pln"><span class="n"><a href="#t6928">6928</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6929" class="pln"><span class="n"><a href="#t6929">6929</a></span><span class="t"><span class="str">    >>> x = torch.randn(5, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6930" class="pln"><span class="n"><a href="#t6930">6930</a></span><span class="t"><span class="str">    >>> torch.rfft(x, 2).shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6931" class="pln"><span class="n"><a href="#t6931">6931</a></span><span class="t"><span class="str">    torch.Size([5, 3, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6932" class="pln"><span class="n"><a href="#t6932">6932</a></span><span class="t"><span class="str">    >>> torch.rfft(x, 2, onesided=False).shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6933" class="pln"><span class="n"><a href="#t6933">6933</a></span><span class="t"><span class="str">    torch.Size([5, 5, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6934" class="pln"><span class="n"><a href="#t6934">6934</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6935" class="pln"><span class="n"><a href="#t6935">6935</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6936" class="pln"><span class="n"><a href="#t6936">6936</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6937" class="pln"><span class="n"><a href="#t6937">6937</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6938" class="run"><span class="n"><a href="#t6938">6938</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">irfft</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6939" class="pln"><span class="n"><a href="#t6939">6939</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6940" class="pln"><span class="n"><a href="#t6940">6940</a></span><span class="t"><span class="str">irfft(input, signal_ndim, normalized=False, onesided=True, signal_sizes=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6941" class="pln"><span class="n"><a href="#t6941">6941</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6942" class="pln"><span class="n"><a href="#t6942">6942</a></span><span class="t"><span class="str">Complex-to-real Inverse Discrete Fourier Transform</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6943" class="pln"><span class="n"><a href="#t6943">6943</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6944" class="pln"><span class="n"><a href="#t6944">6944</a></span><span class="t"><span class="str">This method computes the complex-to-real inverse discrete Fourier transform.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6945" class="pln"><span class="n"><a href="#t6945">6945</a></span><span class="t"><span class="str">It is mathematically equivalent with :func:`ifft` with differences only in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6946" class="pln"><span class="n"><a href="#t6946">6946</a></span><span class="t"><span class="str">formats of the input and output.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6947" class="pln"><span class="n"><a href="#t6947">6947</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6948" class="pln"><span class="n"><a href="#t6948">6948</a></span><span class="t"><span class="str">The argument specifications are almost identical with :func:`~torch.ifft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6949" class="pln"><span class="n"><a href="#t6949">6949</a></span><span class="t"><span class="str">Similar to :func:`~torch.ifft`, if :attr:`normalized` is set to ``True``,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6950" class="pln"><span class="n"><a href="#t6950">6950</a></span><span class="t"><span class="str">this normalizes the result by multiplying it with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6951" class="pln"><span class="n"><a href="#t6951">6951</a></span><span class="t"><span class="str">:math:`\sqrt{\prod_{i=1}^K N_i}` so that the operator is unitary, where</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6952" class="pln"><span class="n"><a href="#t6952">6952</a></span><span class="t"><span class="str">:math:`N_i` is the size of signal dimension :math:`i`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6953" class="pln"><span class="n"><a href="#t6953">6953</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6954" class="pln"><span class="n"><a href="#t6954">6954</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6955" class="pln"><span class="n"><a href="#t6955">6955</a></span><span class="t"><span class="str">    Due to the conjugate symmetry, :attr:`input` do not need to contain the full</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6956" class="pln"><span class="n"><a href="#t6956">6956</a></span><span class="t"><span class="str">    complex frequency values. Roughly half of the values will be sufficient, as</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6957" class="pln"><span class="n"><a href="#t6957">6957</a></span><span class="t"><span class="str">    is the case when :attr:`input` is given by :func:`~torch.rfft` with</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6958" class="pln"><span class="n"><a href="#t6958">6958</a></span><span class="t"><span class="str">    ``rfft(signal, onesided=True)``. In such case, set the :attr:`onesided`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6959" class="pln"><span class="n"><a href="#t6959">6959</a></span><span class="t"><span class="str">    argument of this method to ``True``. Moreover, the original signal shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6960" class="pln"><span class="n"><a href="#t6960">6960</a></span><span class="t"><span class="str">    information can sometimes be lost, optionally set :attr:`signal_sizes` to be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6961" class="pln"><span class="n"><a href="#t6961">6961</a></span><span class="t"><span class="str">    the size of the original signal (without the batch dimensions if in batched</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6962" class="pln"><span class="n"><a href="#t6962">6962</a></span><span class="t"><span class="str">    mode) to recover it with correct shape.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6963" class="pln"><span class="n"><a href="#t6963">6963</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6964" class="pln"><span class="n"><a href="#t6964">6964</a></span><span class="t"><span class="str">    Therefore, to invert an :func:`~torch.rfft`, the :attr:`normalized` and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6965" class="pln"><span class="n"><a href="#t6965">6965</a></span><span class="t"><span class="str">    :attr:`onesided` arguments should be set identically for :func:`~torch.irfft`,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6966" class="pln"><span class="n"><a href="#t6966">6966</a></span><span class="t"><span class="str">    and preferably a :attr:`signal_sizes` is given to avoid size mismatch. See the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6967" class="pln"><span class="n"><a href="#t6967">6967</a></span><span class="t"><span class="str">    example below for a case of size mismatch.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6968" class="pln"><span class="n"><a href="#t6968">6968</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6969" class="pln"><span class="n"><a href="#t6969">6969</a></span><span class="t"><span class="str">    See :func:`~torch.rfft` for details on conjugate symmetry.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6970" class="pln"><span class="n"><a href="#t6970">6970</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6971" class="pln"><span class="n"><a href="#t6971">6971</a></span><span class="t"><span class="str">The inverse of this function is :func:`~torch.rfft`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6972" class="pln"><span class="n"><a href="#t6972">6972</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6973" class="pln"><span class="n"><a href="#t6973">6973</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6974" class="pln"><span class="n"><a href="#t6974">6974</a></span><span class="t"><span class="str">    Generally speaking, input to this function should contain values</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6975" class="pln"><span class="n"><a href="#t6975">6975</a></span><span class="t"><span class="str">    following conjugate symmetry. Note that even if :attr:`onesided` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6976" class="pln"><span class="n"><a href="#t6976">6976</a></span><span class="t"><span class="str">    ``True``, often symmetry on some part is still needed. When this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6977" class="pln"><span class="n"><a href="#t6977">6977</a></span><span class="t"><span class="str">    requirement is not satisfied, the behavior of :func:`~torch.irfft` is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6978" class="pln"><span class="n"><a href="#t6978">6978</a></span><span class="t"><span class="str">    undefined. Since :func:`torch.autograd.gradcheck` estimates numerical</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6979" class="pln"><span class="n"><a href="#t6979">6979</a></span><span class="t"><span class="str">    Jacobian with point perturbations, :func:`~torch.irfft` will almost</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6980" class="pln"><span class="n"><a href="#t6980">6980</a></span><span class="t"><span class="str">    certainly fail the check.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6981" class="pln"><span class="n"><a href="#t6981">6981</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6982" class="pln"><span class="n"><a href="#t6982">6982</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6983" class="pln"><span class="n"><a href="#t6983">6983</a></span><span class="t"><span class="str">    For CUDA tensors, an LRU cache is used for cuFFT plans to speed up</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6984" class="pln"><span class="n"><a href="#t6984">6984</a></span><span class="t"><span class="str">    repeatedly running FFT methods on tensors of same geometry with same</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6985" class="pln"><span class="n"><a href="#t6985">6985</a></span><span class="t"><span class="str">    configuration. See :ref:`cufft-plan-cache` for more details on how to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6986" class="pln"><span class="n"><a href="#t6986">6986</a></span><span class="t"><span class="str">    monitor and control the cache.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6987" class="pln"><span class="n"><a href="#t6987">6987</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6988" class="pln"><span class="n"><a href="#t6988">6988</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6989" class="pln"><span class="n"><a href="#t6989">6989</a></span><span class="t"><span class="str">    For CPU tensors, this method is currently only available with MKL. Use</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6990" class="pln"><span class="n"><a href="#t6990">6990</a></span><span class="t"><span class="str">    :func:`torch.backends.mkl.is_available` to check if MKL is installed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6991" class="pln"><span class="n"><a href="#t6991">6991</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t6992" class="pln"><span class="n"><a href="#t6992">6992</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6993" class="pln"><span class="n"><a href="#t6993">6993</a></span><span class="t"><span class="str">    input (Tensor): the input tensor of at least :attr:`signal_ndim` ``+ 1``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6994" class="pln"><span class="n"><a href="#t6994">6994</a></span><span class="t"><span class="str">        dimensions</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6995" class="pln"><span class="n"><a href="#t6995">6995</a></span><span class="t"><span class="str">    signal_ndim (int): the number of dimensions in each signal.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6996" class="pln"><span class="n"><a href="#t6996">6996</a></span><span class="t"><span class="str">        :attr:`signal_ndim` can only be 1, 2 or 3</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6997" class="pln"><span class="n"><a href="#t6997">6997</a></span><span class="t"><span class="str">    normalized (bool, optional): controls whether to return normalized results.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6998" class="pln"><span class="n"><a href="#t6998">6998</a></span><span class="t"><span class="str">        Default: ``False``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6999" class="pln"><span class="n"><a href="#t6999">6999</a></span><span class="t"><span class="str">    onesided (bool, optional): controls whether :attr:`input` was halfed to avoid</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7000" class="pln"><span class="n"><a href="#t7000">7000</a></span><span class="t"><span class="str">        redundancy, e.g., by :func:`rfft`. Default: ``True``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7001" class="pln"><span class="n"><a href="#t7001">7001</a></span><span class="t"><span class="str">    signal_sizes (list or :class:`torch.Size`, optional): the size of the original</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7002" class="pln"><span class="n"><a href="#t7002">7002</a></span><span class="t"><span class="str">        signal (without batch dimension). Default: ``None``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7003" class="pln"><span class="n"><a href="#t7003">7003</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7004" class="pln"><span class="n"><a href="#t7004">7004</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7005" class="pln"><span class="n"><a href="#t7005">7005</a></span><span class="t"><span class="str">    Tensor: A tensor containing the complex-to-real inverse Fourier transform result</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7006" class="pln"><span class="n"><a href="#t7006">7006</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7007" class="pln"><span class="n"><a href="#t7007">7007</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7008" class="pln"><span class="n"><a href="#t7008">7008</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7009" class="pln"><span class="n"><a href="#t7009">7009</a></span><span class="t"><span class="str">    >>> x = torch.randn(4, 4)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7010" class="pln"><span class="n"><a href="#t7010">7010</a></span><span class="t"><span class="str">    >>> torch.rfft(x, 2, onesided=True).shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7011" class="pln"><span class="n"><a href="#t7011">7011</a></span><span class="t"><span class="str">    torch.Size([4, 3, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7012" class="pln"><span class="n"><a href="#t7012">7012</a></span><span class="t"><span class="str">    >>></span>&nbsp;</span><span class="r"></span></p>
    <p id="t7013" class="pln"><span class="n"><a href="#t7013">7013</a></span><span class="t"><span class="str">    >>> # notice that with onesided=True, output size does not determine the original signal size</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7014" class="pln"><span class="n"><a href="#t7014">7014</a></span><span class="t"><span class="str">    >>> x = torch.randn(4, 5)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7015" class="pln"><span class="n"><a href="#t7015">7015</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7016" class="pln"><span class="n"><a href="#t7016">7016</a></span><span class="t"><span class="str">    >>> torch.rfft(x, 2, onesided=True).shape</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7017" class="pln"><span class="n"><a href="#t7017">7017</a></span><span class="t"><span class="str">    torch.Size([4, 3, 2])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7018" class="pln"><span class="n"><a href="#t7018">7018</a></span><span class="t"><span class="str">    >>></span>&nbsp;</span><span class="r"></span></p>
    <p id="t7019" class="pln"><span class="n"><a href="#t7019">7019</a></span><span class="t"><span class="str">    >>> # now we use the original shape to recover x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7020" class="pln"><span class="n"><a href="#t7020">7020</a></span><span class="t"><span class="str">    >>> x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7021" class="pln"><span class="n"><a href="#t7021">7021</a></span><span class="t"><span class="str">    tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7022" class="pln"><span class="n"><a href="#t7022">7022</a></span><span class="t"><span class="str">            [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7023" class="pln"><span class="n"><a href="#t7023">7023</a></span><span class="t"><span class="str">            [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7024" class="pln"><span class="n"><a href="#t7024">7024</a></span><span class="t"><span class="str">            [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7025" class="pln"><span class="n"><a href="#t7025">7025</a></span><span class="t"><span class="str">    >>> y = torch.rfft(x, 2, onesided=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7026" class="pln"><span class="n"><a href="#t7026">7026</a></span><span class="t"><span class="str">    >>> torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7027" class="pln"><span class="n"><a href="#t7027">7027</a></span><span class="t"><span class="str">    tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7028" class="pln"><span class="n"><a href="#t7028">7028</a></span><span class="t"><span class="str">            [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7029" class="pln"><span class="n"><a href="#t7029">7029</a></span><span class="t"><span class="str">            [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7030" class="pln"><span class="n"><a href="#t7030">7030</a></span><span class="t"><span class="str">            [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7031" class="pln"><span class="n"><a href="#t7031">7031</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7032" class="pln"><span class="n"><a href="#t7032">7032</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7033" class="pln"><span class="n"><a href="#t7033">7033</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7034" class="pln"><span class="n"><a href="#t7034">7034</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7035" class="run"><span class="n"><a href="#t7035">7035</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">hann_window</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7036" class="pln"><span class="n"><a href="#t7036">7036</a></span><span class="t">           <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7037" class="pln"><span class="n"><a href="#t7037">7037</a></span><span class="t"><span class="str">hann_window(window_length, periodic=True, dtype=None, \</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7038" class="pln"><span class="n"><a href="#t7038">7038</a></span><span class="t"><span class="str">layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7039" class="pln"><span class="n"><a href="#t7039">7039</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7040" class="pln"><span class="n"><a href="#t7040">7040</a></span><span class="t"><span class="str">Hann window function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7041" class="pln"><span class="n"><a href="#t7041">7041</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7042" class="pln"><span class="n"><a href="#t7042">7042</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7043" class="pln"><span class="n"><a href="#t7043">7043</a></span><span class="t"><span class="str">    w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] =</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7044" class="pln"><span class="n"><a href="#t7044">7044</a></span><span class="t"><span class="str">            \sin^2 \left( \frac{\pi n}{N - 1} \right),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7045" class="pln"><span class="n"><a href="#t7045">7045</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7046" class="pln"><span class="n"><a href="#t7046">7046</a></span><span class="t"><span class="str">where :math:`N` is the full window size.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7047" class="pln"><span class="n"><a href="#t7047">7047</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7048" class="pln"><span class="n"><a href="#t7048">7048</a></span><span class="t"><span class="str">The input :attr:`window_length` is a positive integer controlling the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7049" class="pln"><span class="n"><a href="#t7049">7049</a></span><span class="t"><span class="str">returned window size. :attr:`periodic` flag determines whether the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7050" class="pln"><span class="n"><a href="#t7050">7050</a></span><span class="t"><span class="str">window trims off the last duplicate value from the symmetric window and is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7051" class="pln"><span class="n"><a href="#t7051">7051</a></span><span class="t"><span class="str">ready to be used as a periodic window with functions like</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7052" class="pln"><span class="n"><a href="#t7052">7052</a></span><span class="t"><span class="str">:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7053" class="pln"><span class="n"><a href="#t7053">7053</a></span><span class="t"><span class="str">above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7054" class="pln"><span class="n"><a href="#t7054">7054</a></span><span class="t"><span class="str">``torch.hann_window(L, periodic=True)`` equal to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7055" class="pln"><span class="n"><a href="#t7055">7055</a></span><span class="t"><span class="str">``torch.hann_window(L + 1, periodic=False)[:-1])``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7056" class="pln"><span class="n"><a href="#t7056">7056</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7057" class="pln"><span class="n"><a href="#t7057">7057</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7058" class="pln"><span class="n"><a href="#t7058">7058</a></span><span class="t"><span class="str">    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7059" class="pln"><span class="n"><a href="#t7059">7059</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7060" class="pln"><span class="n"><a href="#t7060">7060</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7061" class="pln"><span class="n"><a href="#t7061">7061</a></span><span class="t"><span class="str">    window_length (int): the size of returned window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7062" class="pln"><span class="n"><a href="#t7062">7062</a></span><span class="t"><span class="str">    periodic (bool, optional): If True, returns a window to be used as periodic</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7063" class="pln"><span class="n"><a href="#t7063">7063</a></span><span class="t"><span class="str">        function. If False, return a symmetric window.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7064" class="pln"><span class="n"><a href="#t7064">7064</a></span><span class="t"><span class="str">    {dtype} Only floating point types are supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7065" class="pln"><span class="n"><a href="#t7065">7065</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7066" class="pln"><span class="n"><a href="#t7066">7066</a></span><span class="t"><span class="str">          ``torch.strided`` (dense layout) is supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7067" class="pln"><span class="n"><a href="#t7067">7067</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7068" class="pln"><span class="n"><a href="#t7068">7068</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7069" class="pln"><span class="n"><a href="#t7069">7069</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7070" class="pln"><span class="n"><a href="#t7070">7070</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7071" class="pln"><span class="n"><a href="#t7071">7071</a></span><span class="t"><span class="str">    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7072" class="pln"><span class="n"><a href="#t7072">7072</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7073" class="pln"><span class="n"><a href="#t7073">7073</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7074" class="pln"><span class="n"><a href="#t7074">7074</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7075" class="pln"><span class="n"><a href="#t7075">7075</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7076" class="run"><span class="n"><a href="#t7076">7076</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">hamming_window</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7077" class="pln"><span class="n"><a href="#t7077">7077</a></span><span class="t">           <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7078" class="pln"><span class="n"><a href="#t7078">7078</a></span><span class="t"><span class="str">hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, dtype=None, \</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7079" class="pln"><span class="n"><a href="#t7079">7079</a></span><span class="t"><span class="str">layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7080" class="pln"><span class="n"><a href="#t7080">7080</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7081" class="pln"><span class="n"><a href="#t7081">7081</a></span><span class="t"><span class="str">Hamming window function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7082" class="pln"><span class="n"><a href="#t7082">7082</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7083" class="pln"><span class="n"><a href="#t7083">7083</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7084" class="pln"><span class="n"><a href="#t7084">7084</a></span><span class="t"><span class="str">    w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7085" class="pln"><span class="n"><a href="#t7085">7085</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7086" class="pln"><span class="n"><a href="#t7086">7086</a></span><span class="t"><span class="str">where :math:`N` is the full window size.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7087" class="pln"><span class="n"><a href="#t7087">7087</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7088" class="pln"><span class="n"><a href="#t7088">7088</a></span><span class="t"><span class="str">The input :attr:`window_length` is a positive integer controlling the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7089" class="pln"><span class="n"><a href="#t7089">7089</a></span><span class="t"><span class="str">returned window size. :attr:`periodic` flag determines whether the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7090" class="pln"><span class="n"><a href="#t7090">7090</a></span><span class="t"><span class="str">window trims off the last duplicate value from the symmetric window and is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7091" class="pln"><span class="n"><a href="#t7091">7091</a></span><span class="t"><span class="str">ready to be used as a periodic window with functions like</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7092" class="pln"><span class="n"><a href="#t7092">7092</a></span><span class="t"><span class="str">:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7093" class="pln"><span class="n"><a href="#t7093">7093</a></span><span class="t"><span class="str">above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7094" class="pln"><span class="n"><a href="#t7094">7094</a></span><span class="t"><span class="str">``torch.hamming_window(L, periodic=True)`` equal to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7095" class="pln"><span class="n"><a href="#t7095">7095</a></span><span class="t"><span class="str">``torch.hamming_window(L + 1, periodic=False)[:-1])``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7096" class="pln"><span class="n"><a href="#t7096">7096</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7097" class="pln"><span class="n"><a href="#t7097">7097</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7098" class="pln"><span class="n"><a href="#t7098">7098</a></span><span class="t"><span class="str">    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7099" class="pln"><span class="n"><a href="#t7099">7099</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7100" class="pln"><span class="n"><a href="#t7100">7100</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7101" class="pln"><span class="n"><a href="#t7101">7101</a></span><span class="t"><span class="str">    This is a generalized version of :meth:`torch.hann_window`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7102" class="pln"><span class="n"><a href="#t7102">7102</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7103" class="pln"><span class="n"><a href="#t7103">7103</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7104" class="pln"><span class="n"><a href="#t7104">7104</a></span><span class="t"><span class="str">    window_length (int): the size of returned window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7105" class="pln"><span class="n"><a href="#t7105">7105</a></span><span class="t"><span class="str">    periodic (bool, optional): If True, returns a window to be used as periodic</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7106" class="pln"><span class="n"><a href="#t7106">7106</a></span><span class="t"><span class="str">        function. If False, return a symmetric window.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7107" class="pln"><span class="n"><a href="#t7107">7107</a></span><span class="t"><span class="str">    alpha (float, optional): The coefficient :math:`\alpha` in the equation above</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7108" class="pln"><span class="n"><a href="#t7108">7108</a></span><span class="t"><span class="str">    beta (float, optional): The coefficient :math:`\beta` in the equation above</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7109" class="pln"><span class="n"><a href="#t7109">7109</a></span><span class="t"><span class="str">    {dtype} Only floating point types are supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7110" class="pln"><span class="n"><a href="#t7110">7110</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7111" class="pln"><span class="n"><a href="#t7111">7111</a></span><span class="t"><span class="str">          ``torch.strided`` (dense layout) is supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7112" class="pln"><span class="n"><a href="#t7112">7112</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7113" class="pln"><span class="n"><a href="#t7113">7113</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7114" class="pln"><span class="n"><a href="#t7114">7114</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7115" class="pln"><span class="n"><a href="#t7115">7115</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7116" class="pln"><span class="n"><a href="#t7116">7116</a></span><span class="t"><span class="str">    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7117" class="pln"><span class="n"><a href="#t7117">7117</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7118" class="pln"><span class="n"><a href="#t7118">7118</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7119" class="pln"><span class="n"><a href="#t7119">7119</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7120" class="pln"><span class="n"><a href="#t7120">7120</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7121" class="run"><span class="n"><a href="#t7121">7121</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">bartlett_window</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7122" class="pln"><span class="n"><a href="#t7122">7122</a></span><span class="t">           <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7123" class="pln"><span class="n"><a href="#t7123">7123</a></span><span class="t"><span class="str">bartlett_window(window_length, periodic=True, dtype=None, \</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7124" class="pln"><span class="n"><a href="#t7124">7124</a></span><span class="t"><span class="str">layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7125" class="pln"><span class="n"><a href="#t7125">7125</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7126" class="pln"><span class="n"><a href="#t7126">7126</a></span><span class="t"><span class="str">Bartlett window function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7127" class="pln"><span class="n"><a href="#t7127">7127</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7128" class="pln"><span class="n"><a href="#t7128">7128</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7129" class="pln"><span class="n"><a href="#t7129">7129</a></span><span class="t"><span class="str">    w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7130" class="pln"><span class="n"><a href="#t7130">7130</a></span><span class="t"><span class="str">        \frac{2n}{N - 1} &amp; \text{if } 0 \leq n \leq \frac{N - 1}{2} \\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7131" class="pln"><span class="n"><a href="#t7131">7131</a></span><span class="t"><span class="str">        2 - \frac{2n}{N - 1} &amp; \text{if } \frac{N - 1}{2} &lt; n &lt; N \\</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7132" class="pln"><span class="n"><a href="#t7132">7132</a></span><span class="t"><span class="str">    \end{cases},</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7133" class="pln"><span class="n"><a href="#t7133">7133</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7134" class="pln"><span class="n"><a href="#t7134">7134</a></span><span class="t"><span class="str">where :math:`N` is the full window size.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7135" class="pln"><span class="n"><a href="#t7135">7135</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7136" class="pln"><span class="n"><a href="#t7136">7136</a></span><span class="t"><span class="str">The input :attr:`window_length` is a positive integer controlling the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7137" class="pln"><span class="n"><a href="#t7137">7137</a></span><span class="t"><span class="str">returned window size. :attr:`periodic` flag determines whether the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7138" class="pln"><span class="n"><a href="#t7138">7138</a></span><span class="t"><span class="str">window trims off the last duplicate value from the symmetric window and is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7139" class="pln"><span class="n"><a href="#t7139">7139</a></span><span class="t"><span class="str">ready to be used as a periodic window with functions like</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7140" class="pln"><span class="n"><a href="#t7140">7140</a></span><span class="t"><span class="str">:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7141" class="pln"><span class="n"><a href="#t7141">7141</a></span><span class="t"><span class="str">above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7142" class="pln"><span class="n"><a href="#t7142">7142</a></span><span class="t"><span class="str">``torch.bartlett_window(L, periodic=True)`` equal to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7143" class="pln"><span class="n"><a href="#t7143">7143</a></span><span class="t"><span class="str">``torch.bartlett_window(L + 1, periodic=False)[:-1])``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7144" class="pln"><span class="n"><a href="#t7144">7144</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7145" class="pln"><span class="n"><a href="#t7145">7145</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7146" class="pln"><span class="n"><a href="#t7146">7146</a></span><span class="t"><span class="str">    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7147" class="pln"><span class="n"><a href="#t7147">7147</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7148" class="pln"><span class="n"><a href="#t7148">7148</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7149" class="pln"><span class="n"><a href="#t7149">7149</a></span><span class="t"><span class="str">    window_length (int): the size of returned window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7150" class="pln"><span class="n"><a href="#t7150">7150</a></span><span class="t"><span class="str">    periodic (bool, optional): If True, returns a window to be used as periodic</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7151" class="pln"><span class="n"><a href="#t7151">7151</a></span><span class="t"><span class="str">        function. If False, return a symmetric window.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7152" class="pln"><span class="n"><a href="#t7152">7152</a></span><span class="t"><span class="str">    {dtype} Only floating point types are supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7153" class="pln"><span class="n"><a href="#t7153">7153</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7154" class="pln"><span class="n"><a href="#t7154">7154</a></span><span class="t"><span class="str">          ``torch.strided`` (dense layout) is supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7155" class="pln"><span class="n"><a href="#t7155">7155</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7156" class="pln"><span class="n"><a href="#t7156">7156</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7157" class="pln"><span class="n"><a href="#t7157">7157</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7158" class="pln"><span class="n"><a href="#t7158">7158</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7159" class="pln"><span class="n"><a href="#t7159">7159</a></span><span class="t"><span class="str">    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7160" class="pln"><span class="n"><a href="#t7160">7160</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7161" class="pln"><span class="n"><a href="#t7161">7161</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7162" class="pln"><span class="n"><a href="#t7162">7162</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7163" class="pln"><span class="n"><a href="#t7163">7163</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7164" class="run"><span class="n"><a href="#t7164">7164</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">blackman_window</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7165" class="pln"><span class="n"><a href="#t7165">7165</a></span><span class="t">           <span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7166" class="pln"><span class="n"><a href="#t7166">7166</a></span><span class="t"><span class="str">blackman_window(window_length, periodic=True, dtype=None, \</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7167" class="pln"><span class="n"><a href="#t7167">7167</a></span><span class="t"><span class="str">layout=torch.strided, device=None, requires_grad=False) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7168" class="pln"><span class="n"><a href="#t7168">7168</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7169" class="pln"><span class="n"><a href="#t7169">7169</a></span><span class="t"><span class="str">Blackman window function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7170" class="pln"><span class="n"><a href="#t7170">7170</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7171" class="pln"><span class="n"><a href="#t7171">7171</a></span><span class="t"><span class="str">.. math::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7172" class="pln"><span class="n"><a href="#t7172">7172</a></span><span class="t"><span class="str">    w[n] = 0.42 - 0.5 \cos \left( \frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7173" class="pln"><span class="n"><a href="#t7173">7173</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7174" class="pln"><span class="n"><a href="#t7174">7174</a></span><span class="t"><span class="str">where :math:`N` is the full window size.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7175" class="pln"><span class="n"><a href="#t7175">7175</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7176" class="pln"><span class="n"><a href="#t7176">7176</a></span><span class="t"><span class="str">The input :attr:`window_length` is a positive integer controlling the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7177" class="pln"><span class="n"><a href="#t7177">7177</a></span><span class="t"><span class="str">returned window size. :attr:`periodic` flag determines whether the returned</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7178" class="pln"><span class="n"><a href="#t7178">7178</a></span><span class="t"><span class="str">window trims off the last duplicate value from the symmetric window and is</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7179" class="pln"><span class="n"><a href="#t7179">7179</a></span><span class="t"><span class="str">ready to be used as a periodic window with functions like</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7180" class="pln"><span class="n"><a href="#t7180">7180</a></span><span class="t"><span class="str">:meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7181" class="pln"><span class="n"><a href="#t7181">7181</a></span><span class="t"><span class="str">above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7182" class="pln"><span class="n"><a href="#t7182">7182</a></span><span class="t"><span class="str">``torch.blackman_window(L, periodic=True)`` equal to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7183" class="pln"><span class="n"><a href="#t7183">7183</a></span><span class="t"><span class="str">``torch.blackman_window(L + 1, periodic=False)[:-1])``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7184" class="pln"><span class="n"><a href="#t7184">7184</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7185" class="pln"><span class="n"><a href="#t7185">7185</a></span><span class="t"><span class="str">.. note::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7186" class="pln"><span class="n"><a href="#t7186">7186</a></span><span class="t"><span class="str">    If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7187" class="pln"><span class="n"><a href="#t7187">7187</a></span><span class="t"><span class="str">"""</span> <span class="op">+</span> <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7188" class="pln"><span class="n"><a href="#t7188">7188</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7189" class="pln"><span class="n"><a href="#t7189">7189</a></span><span class="t"><span class="str">    window_length (int): the size of returned window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7190" class="pln"><span class="n"><a href="#t7190">7190</a></span><span class="t"><span class="str">    periodic (bool, optional): If True, returns a window to be used as periodic</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7191" class="pln"><span class="n"><a href="#t7191">7191</a></span><span class="t"><span class="str">        function. If False, return a symmetric window.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7192" class="pln"><span class="n"><a href="#t7192">7192</a></span><span class="t"><span class="str">    {dtype} Only floating point types are supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7193" class="pln"><span class="n"><a href="#t7193">7193</a></span><span class="t"><span class="str">    layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7194" class="pln"><span class="n"><a href="#t7194">7194</a></span><span class="t"><span class="str">          ``torch.strided`` (dense layout) is supported.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7195" class="pln"><span class="n"><a href="#t7195">7195</a></span><span class="t"><span class="str">    {device}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7196" class="pln"><span class="n"><a href="#t7196">7196</a></span><span class="t"><span class="str">    {requires_grad}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7197" class="pln"><span class="n"><a href="#t7197">7197</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7198" class="pln"><span class="n"><a href="#t7198">7198</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7199" class="pln"><span class="n"><a href="#t7199">7199</a></span><span class="t"><span class="str">    Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7200" class="pln"><span class="n"><a href="#t7200">7200</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7201" class="pln"><span class="n"><a href="#t7201">7201</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">factory_common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7202" class="pln"><span class="n"><a href="#t7202">7202</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7203" class="pln"><span class="n"><a href="#t7203">7203</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7204" class="run"><span class="n"><a href="#t7204">7204</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">unbind</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7205" class="pln"><span class="n"><a href="#t7205">7205</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7206" class="pln"><span class="n"><a href="#t7206">7206</a></span><span class="t"><span class="str">unbind(input, dim=0) -> seq</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7207" class="pln"><span class="n"><a href="#t7207">7207</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7208" class="pln"><span class="n"><a href="#t7208">7208</a></span><span class="t"><span class="str">Removes a tensor dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7209" class="pln"><span class="n"><a href="#t7209">7209</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7210" class="pln"><span class="n"><a href="#t7210">7210</a></span><span class="t"><span class="str">Returns a tuple of all slices along a given dimension, already without it.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7211" class="pln"><span class="n"><a href="#t7211">7211</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7212" class="pln"><span class="n"><a href="#t7212">7212</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7213" class="pln"><span class="n"><a href="#t7213">7213</a></span><span class="t"><span class="str">    input (Tensor): the tensor to unbind</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7214" class="pln"><span class="n"><a href="#t7214">7214</a></span><span class="t"><span class="str">    dim (int): dimension to remove</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7215" class="pln"><span class="n"><a href="#t7215">7215</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7216" class="pln"><span class="n"><a href="#t7216">7216</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7217" class="pln"><span class="n"><a href="#t7217">7217</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7218" class="pln"><span class="n"><a href="#t7218">7218</a></span><span class="t"><span class="str">    >>> torch.unbind(torch.tensor([[1, 2, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7219" class="pln"><span class="n"><a href="#t7219">7219</a></span><span class="t"><span class="str">    >>>                            [4, 5, 6],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7220" class="pln"><span class="n"><a href="#t7220">7220</a></span><span class="t"><span class="str">    >>>                            [7, 8, 9]]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7221" class="pln"><span class="n"><a href="#t7221">7221</a></span><span class="t"><span class="str">    (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7222" class="pln"><span class="n"><a href="#t7222">7222</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7223" class="pln"><span class="n"><a href="#t7223">7223</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7224" class="pln"><span class="n"><a href="#t7224">7224</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7225" class="run"><span class="n"><a href="#t7225">7225</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">combinations</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7226" class="pln"><span class="n"><a href="#t7226">7226</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7227" class="pln"><span class="n"><a href="#t7227">7227</a></span><span class="t"><span class="str">combinations(input, r=2, with_replacement=False) -> seq</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7228" class="pln"><span class="n"><a href="#t7228">7228</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7229" class="pln"><span class="n"><a href="#t7229">7229</a></span><span class="t"><span class="str">Compute combinations of length :math:`r` of the given tensor. The behavior is similar to</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7230" class="pln"><span class="n"><a href="#t7230">7230</a></span><span class="t"><span class="str">python's `itertools.combinations` when `with_replacement` is set to `False`, and</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7231" class="pln"><span class="n"><a href="#t7231">7231</a></span><span class="t"><span class="str">`itertools.combinations_with_replacement` when `with_replacement` is set to `True`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7232" class="pln"><span class="n"><a href="#t7232">7232</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7233" class="pln"><span class="n"><a href="#t7233">7233</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7234" class="pln"><span class="n"><a href="#t7234">7234</a></span><span class="t"><span class="str">    input (Tensor): 1D vector.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7235" class="pln"><span class="n"><a href="#t7235">7235</a></span><span class="t"><span class="str">    r (int, optional): number of elements to combine</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7236" class="pln"><span class="n"><a href="#t7236">7236</a></span><span class="t"><span class="str">    with_replacement (boolean, optional): whether to allow duplication in combination</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7237" class="pln"><span class="n"><a href="#t7237">7237</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7238" class="pln"><span class="n"><a href="#t7238">7238</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7239" class="pln"><span class="n"><a href="#t7239">7239</a></span><span class="t"><span class="str">    Tensor: A tensor equivalent to converting all the input tensors into lists, do</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7240" class="pln"><span class="n"><a href="#t7240">7240</a></span><span class="t"><span class="str">    `itertools.combinations` or `itertools.combinations_with_replacement` on these</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7241" class="pln"><span class="n"><a href="#t7241">7241</a></span><span class="t"><span class="str">    lists, and finally convert the resulting list into tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7242" class="pln"><span class="n"><a href="#t7242">7242</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7243" class="pln"><span class="n"><a href="#t7243">7243</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7244" class="pln"><span class="n"><a href="#t7244">7244</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7245" class="pln"><span class="n"><a href="#t7245">7245</a></span><span class="t"><span class="str">    >>> a = [1, 2, 3]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7246" class="pln"><span class="n"><a href="#t7246">7246</a></span><span class="t"><span class="str">    >>> list(itertools.combinations(a, r=2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7247" class="pln"><span class="n"><a href="#t7247">7247</a></span><span class="t"><span class="str">    [(1, 2), (1, 3), (2, 3)]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7248" class="pln"><span class="n"><a href="#t7248">7248</a></span><span class="t"><span class="str">    >>> list(itertools.combinations(a, r=3))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7249" class="pln"><span class="n"><a href="#t7249">7249</a></span><span class="t"><span class="str">    [(1, 2, 3)]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7250" class="pln"><span class="n"><a href="#t7250">7250</a></span><span class="t"><span class="str">    >>> list(itertools.combinations_with_replacement(a, r=2))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7251" class="pln"><span class="n"><a href="#t7251">7251</a></span><span class="t"><span class="str">    [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7252" class="pln"><span class="n"><a href="#t7252">7252</a></span><span class="t"><span class="str">    >>> tensor_a = torch.tensor(a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7253" class="pln"><span class="n"><a href="#t7253">7253</a></span><span class="t"><span class="str">    >>> torch.combinations(tensor_a)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7254" class="pln"><span class="n"><a href="#t7254">7254</a></span><span class="t"><span class="str">    tensor([[1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7255" class="pln"><span class="n"><a href="#t7255">7255</a></span><span class="t"><span class="str">            [1, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7256" class="pln"><span class="n"><a href="#t7256">7256</a></span><span class="t"><span class="str">            [2, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7257" class="pln"><span class="n"><a href="#t7257">7257</a></span><span class="t"><span class="str">    >>> torch.combinations(tensor_a, r=3)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7258" class="pln"><span class="n"><a href="#t7258">7258</a></span><span class="t"><span class="str">    tensor([[1, 2, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7259" class="pln"><span class="n"><a href="#t7259">7259</a></span><span class="t"><span class="str">    >>> torch.combinations(tensor_a, with_replacement=True)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7260" class="pln"><span class="n"><a href="#t7260">7260</a></span><span class="t"><span class="str">    tensor([[1, 1],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7261" class="pln"><span class="n"><a href="#t7261">7261</a></span><span class="t"><span class="str">            [1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7262" class="pln"><span class="n"><a href="#t7262">7262</a></span><span class="t"><span class="str">            [1, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7263" class="pln"><span class="n"><a href="#t7263">7263</a></span><span class="t"><span class="str">            [2, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7264" class="pln"><span class="n"><a href="#t7264">7264</a></span><span class="t"><span class="str">            [2, 3],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7265" class="pln"><span class="n"><a href="#t7265">7265</a></span><span class="t"><span class="str">            [3, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7266" class="pln"><span class="n"><a href="#t7266">7266</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7267" class="pln"><span class="n"><a href="#t7267">7267</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7268" class="run"><span class="n"><a href="#t7268">7268</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">trapz</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7269" class="pln"><span class="n"><a href="#t7269">7269</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7270" class="pln"><span class="n"><a href="#t7270">7270</a></span><span class="t"><span class="str">.. function:: trapz(y, x, *, dim=-1) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7271" class="pln"><span class="n"><a href="#t7271">7271</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7272" class="pln"><span class="n"><a href="#t7272">7272</a></span><span class="t"><span class="str">Estimate :math:`\int y\,dx` along `dim`, using the trapezoid rule.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7273" class="pln"><span class="n"><a href="#t7273">7273</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7274" class="pln"><span class="n"><a href="#t7274">7274</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7275" class="pln"><span class="n"><a href="#t7275">7275</a></span><span class="t"><span class="str">    y (Tensor): The values of the function to integrate</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7276" class="pln"><span class="n"><a href="#t7276">7276</a></span><span class="t"><span class="str">    x (Tensor): The points at which the function `y` is sampled.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7277" class="pln"><span class="n"><a href="#t7277">7277</a></span><span class="t"><span class="str">        If `x` is not in ascending order, intervals on which it is decreasing</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7278" class="pln"><span class="n"><a href="#t7278">7278</a></span><span class="t"><span class="str">        contribute negatively to the estimated integral (i.e., the convention</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7279" class="pln"><span class="n"><a href="#t7279">7279</a></span><span class="t"><span class="str">        :math:`\int_a^b f = -\int_b^a f` is followed).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7280" class="pln"><span class="n"><a href="#t7280">7280</a></span><span class="t"><span class="str">    dim (int): The dimension along which to integrate.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7281" class="pln"><span class="n"><a href="#t7281">7281</a></span><span class="t"><span class="str">        By default, use the last dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7282" class="pln"><span class="n"><a href="#t7282">7282</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7283" class="pln"><span class="n"><a href="#t7283">7283</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7284" class="pln"><span class="n"><a href="#t7284">7284</a></span><span class="t"><span class="str">    A Tensor with the same shape as the input, except with `dim` removed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7285" class="pln"><span class="n"><a href="#t7285">7285</a></span><span class="t"><span class="str">    Each element of the returned tensor represents the estimated integral</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7286" class="pln"><span class="n"><a href="#t7286">7286</a></span><span class="t"><span class="str">    :math:`\int y\,dx` along `dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7287" class="pln"><span class="n"><a href="#t7287">7287</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7288" class="pln"><span class="n"><a href="#t7288">7288</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7289" class="pln"><span class="n"><a href="#t7289">7289</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7290" class="pln"><span class="n"><a href="#t7290">7290</a></span><span class="t"><span class="str">    >>> y = torch.randn((2, 3))</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7291" class="pln"><span class="n"><a href="#t7291">7291</a></span><span class="t"><span class="str">    >>> y</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7292" class="pln"><span class="n"><a href="#t7292">7292</a></span><span class="t"><span class="str">    tensor([[-2.1156,  0.6857, -0.2700],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7293" class="pln"><span class="n"><a href="#t7293">7293</a></span><span class="t"><span class="str">            [-1.2145,  0.5540,  2.0431]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7294" class="pln"><span class="n"><a href="#t7294">7294</a></span><span class="t"><span class="str">    >>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7295" class="pln"><span class="n"><a href="#t7295">7295</a></span><span class="t"><span class="str">    >>> torch.trapz(y, x)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7296" class="pln"><span class="n"><a href="#t7296">7296</a></span><span class="t"><span class="str">    tensor([-1.2220,  0.9683])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7297" class="pln"><span class="n"><a href="#t7297">7297</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7298" class="pln"><span class="n"><a href="#t7298">7298</a></span><span class="t"><span class="str">.. function:: trapz(y, *, dx=1, dim=-1) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7299" class="pln"><span class="n"><a href="#t7299">7299</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7300" class="pln"><span class="n"><a href="#t7300">7300</a></span><span class="t"><span class="str">As above, but the sample points are spaced uniformly at a distance of `dx`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7301" class="pln"><span class="n"><a href="#t7301">7301</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7302" class="pln"><span class="n"><a href="#t7302">7302</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7303" class="pln"><span class="n"><a href="#t7303">7303</a></span><span class="t"><span class="str">    y (Tensor): The values of the function to integrate</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7304" class="pln"><span class="n"><a href="#t7304">7304</a></span><span class="t"><span class="str">    dx (float): The distance between points at which `y` is sampled.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7305" class="pln"><span class="n"><a href="#t7305">7305</a></span><span class="t"><span class="str">    dim (int): The dimension along which to integrate.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7306" class="pln"><span class="n"><a href="#t7306">7306</a></span><span class="t"><span class="str">        By default, use the last dimension.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7307" class="pln"><span class="n"><a href="#t7307">7307</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7308" class="pln"><span class="n"><a href="#t7308">7308</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7309" class="pln"><span class="n"><a href="#t7309">7309</a></span><span class="t"><span class="str">    A Tensor with the same shape as the input, except with `dim` removed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7310" class="pln"><span class="n"><a href="#t7310">7310</a></span><span class="t"><span class="str">    Each element of the returned tensor represents the estimated integral</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7311" class="pln"><span class="n"><a href="#t7311">7311</a></span><span class="t"><span class="str">    :math:`\int y\,dx` along `dim`.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7312" class="pln"><span class="n"><a href="#t7312">7312</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7313" class="pln"><span class="n"><a href="#t7313">7313</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7314" class="run"><span class="n"><a href="#t7314">7314</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">repeat_interleave</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7315" class="pln"><span class="n"><a href="#t7315">7315</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7316" class="pln"><span class="n"><a href="#t7316">7316</a></span><span class="t"><span class="str">.. function:: repeat_interleave(input, repeats, dim=None) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7317" class="pln"><span class="n"><a href="#t7317">7317</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7318" class="pln"><span class="n"><a href="#t7318">7318</a></span><span class="t"><span class="str">Repeat elements of a tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7319" class="pln"><span class="n"><a href="#t7319">7319</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7320" class="pln"><span class="n"><a href="#t7320">7320</a></span><span class="t"><span class="str">.. warning::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7321" class="pln"><span class="n"><a href="#t7321">7321</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7322" class="pln"><span class="n"><a href="#t7322">7322</a></span><span class="t"><span class="str">    This is different from :meth:`torch.Tensor.repeat` but similar to ``numpy.repeat``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7323" class="pln"><span class="n"><a href="#t7323">7323</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7324" class="pln"><span class="n"><a href="#t7324">7324</a></span><span class="t"><span class="str">Args:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7325" class="pln"><span class="n"><a href="#t7325">7325</a></span><span class="t"><span class="str">    {input}</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7326" class="pln"><span class="n"><a href="#t7326">7326</a></span><span class="t"><span class="str">    repeats (Tensor or int): The number of repetitions for each element.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7327" class="pln"><span class="n"><a href="#t7327">7327</a></span><span class="t"><span class="str">        repeats is broadcasted to fit the shape of the given axis.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7328" class="pln"><span class="n"><a href="#t7328">7328</a></span><span class="t"><span class="str">    dim (int, optional): The dimension along which to repeat values.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7329" class="pln"><span class="n"><a href="#t7329">7329</a></span><span class="t"><span class="str">        By default, use the flattened input array, and return a flat output</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7330" class="pln"><span class="n"><a href="#t7330">7330</a></span><span class="t"><span class="str">        array.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7331" class="pln"><span class="n"><a href="#t7331">7331</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7332" class="pln"><span class="n"><a href="#t7332">7332</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7333" class="pln"><span class="n"><a href="#t7333">7333</a></span><span class="t"><span class="str">    Tensor: Repeated tensor which has the same shape as input, except along the</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7334" class="pln"><span class="n"><a href="#t7334">7334</a></span><span class="t"><span class="str">     given axis.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7335" class="pln"><span class="n"><a href="#t7335">7335</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7336" class="pln"><span class="n"><a href="#t7336">7336</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7337" class="pln"><span class="n"><a href="#t7337">7337</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7338" class="pln"><span class="n"><a href="#t7338">7338</a></span><span class="t"><span class="str">    >>> x = torch.tensor([1, 2, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7339" class="pln"><span class="n"><a href="#t7339">7339</a></span><span class="t"><span class="str">    >>> x.repeat_interleave(2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7340" class="pln"><span class="n"><a href="#t7340">7340</a></span><span class="t"><span class="str">    tensor([1, 1, 2, 2, 3, 3])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7341" class="pln"><span class="n"><a href="#t7341">7341</a></span><span class="t"><span class="str">    >>> y = torch.tensor([[1, 2], [3, 4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7342" class="pln"><span class="n"><a href="#t7342">7342</a></span><span class="t"><span class="str">    >>> torch.repeat_interleave(y, 2)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7343" class="pln"><span class="n"><a href="#t7343">7343</a></span><span class="t"><span class="str">    tensor([1, 1, 2, 2, 3, 3, 4, 4])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7344" class="pln"><span class="n"><a href="#t7344">7344</a></span><span class="t"><span class="str">    >>> torch.repeat_interleave(y, 3, dim=1)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7345" class="pln"><span class="n"><a href="#t7345">7345</a></span><span class="t"><span class="str">    tensor([[1, 1, 1, 2, 2, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7346" class="pln"><span class="n"><a href="#t7346">7346</a></span><span class="t"><span class="str">            [3, 3, 3, 4, 4, 4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7347" class="pln"><span class="n"><a href="#t7347">7347</a></span><span class="t"><span class="str">    >>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7348" class="pln"><span class="n"><a href="#t7348">7348</a></span><span class="t"><span class="str">    tensor([[1, 2],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7349" class="pln"><span class="n"><a href="#t7349">7349</a></span><span class="t"><span class="str">            [3, 4],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7350" class="pln"><span class="n"><a href="#t7350">7350</a></span><span class="t"><span class="str">            [3, 4]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7351" class="pln"><span class="n"><a href="#t7351">7351</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7352" class="pln"><span class="n"><a href="#t7352">7352</a></span><span class="t"><span class="str">.. function:: repeat_interleave(repeats) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7353" class="pln"><span class="n"><a href="#t7353">7353</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7354" class="pln"><span class="n"><a href="#t7354">7354</a></span><span class="t"><span class="str">If the `repeats` is `tensor([n1, n2, n3, ...])`, then the output will be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7355" class="pln"><span class="n"><a href="#t7355">7355</a></span><span class="t"><span class="str">`tensor([0, 0, ..., 1, 1, ..., 2, 2, ..., ...])` where `0` appears `n1` times,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7356" class="pln"><span class="n"><a href="#t7356">7356</a></span><span class="t"><span class="str">`1` appears `n2` times, `2` appears `n3` times, etc.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7357" class="pln"><span class="n"><a href="#t7357">7357</a></span><span class="t"><span class="str">"""</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="op">**</span><span class="nam">common_args</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7358" class="pln"><span class="n"><a href="#t7358">7358</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7359" class="pln"><span class="n"><a href="#t7359">7359</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7360" class="run"><span class="n"><a href="#t7360">7360</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">quantize_per_tensor</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7361" class="pln"><span class="n"><a href="#t7361">7361</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7362" class="pln"><span class="n"><a href="#t7362">7362</a></span><span class="t"><span class="str">quantize_per_tensor(input, scale, zero_point, dtype) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7363" class="pln"><span class="n"><a href="#t7363">7363</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7364" class="pln"><span class="n"><a href="#t7364">7364</a></span><span class="t"><span class="str">Converts a float tensor to quantized tensor with given scale and zero point.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7365" class="pln"><span class="n"><a href="#t7365">7365</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7366" class="pln"><span class="n"><a href="#t7366">7366</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7367" class="pln"><span class="n"><a href="#t7367">7367</a></span><span class="t"><span class="str">    input (Tensor): float tensor to quantize</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7368" class="pln"><span class="n"><a href="#t7368">7368</a></span><span class="t"><span class="str">    scale (float): scale to apply in quantization formula</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7369" class="pln"><span class="n"><a href="#t7369">7369</a></span><span class="t"><span class="str">    zero_point (int): offset in integer value that maps to float zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7370" class="pln"><span class="n"><a href="#t7370">7370</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7371" class="pln"><span class="n"><a href="#t7371">7371</a></span><span class="t"><span class="str">        Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7372" class="pln"><span class="n"><a href="#t7372">7372</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7373" class="pln"><span class="n"><a href="#t7373">7373</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7374" class="pln"><span class="n"><a href="#t7374">7374</a></span><span class="t"><span class="str">    Tensor: A newly quantized tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7375" class="pln"><span class="n"><a href="#t7375">7375</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7376" class="pln"><span class="n"><a href="#t7376">7376</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7377" class="pln"><span class="n"><a href="#t7377">7377</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7378" class="pln"><span class="n"><a href="#t7378">7378</a></span><span class="t"><span class="str">    >>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7379" class="pln"><span class="n"><a href="#t7379">7379</a></span><span class="t"><span class="str">    tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7380" class="pln"><span class="n"><a href="#t7380">7380</a></span><span class="t"><span class="str">           quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7381" class="pln"><span class="n"><a href="#t7381">7381</a></span><span class="t"><span class="str">    >>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7382" class="pln"><span class="n"><a href="#t7382">7382</a></span><span class="t"><span class="str">    tensor([ 0, 10, 20, 30], dtype=torch.uint8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7383" class="pln"><span class="n"><a href="#t7383">7383</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7384" class="pln"><span class="n"><a href="#t7384">7384</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7385" class="run"><span class="n"><a href="#t7385">7385</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">quantize_per_channel</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7386" class="pln"><span class="n"><a href="#t7386">7386</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7387" class="pln"><span class="n"><a href="#t7387">7387</a></span><span class="t"><span class="str">quantize_per_channel(input, scales, zero_points, axis, dtype) -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7388" class="pln"><span class="n"><a href="#t7388">7388</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7389" class="pln"><span class="n"><a href="#t7389">7389</a></span><span class="t"><span class="str">Converts a float tensor to per-channel quantized tensor with given scales and zero points.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7390" class="pln"><span class="n"><a href="#t7390">7390</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7391" class="pln"><span class="n"><a href="#t7391">7391</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7392" class="pln"><span class="n"><a href="#t7392">7392</a></span><span class="t"><span class="str">    input (Tensor): float tensor to quantize</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7393" class="pln"><span class="n"><a href="#t7393">7393</a></span><span class="t"><span class="str">    scales (Tensor): float 1D tensor of scales to use, size should match ``input.size(axis)``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7394" class="pln"><span class="n"><a href="#t7394">7394</a></span><span class="t"><span class="str">    zero_points (int): integer 1D tensor of offset to use, size should match ``input.size(axis)``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7395" class="pln"><span class="n"><a href="#t7395">7395</a></span><span class="t"><span class="str">    axis (int): dimension on which apply per-channel quantization</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7396" class="pln"><span class="n"><a href="#t7396">7396</a></span><span class="t"><span class="str">    dtype (:class:`torch.dtype`): the desired data type of returned tensor.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7397" class="pln"><span class="n"><a href="#t7397">7397</a></span><span class="t"><span class="str">        Has to be one of the quantized dtypes: ``torch.quint8``, ``torch.qint8``, ``torch.qint32``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7398" class="pln"><span class="n"><a href="#t7398">7398</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7399" class="pln"><span class="n"><a href="#t7399">7399</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7400" class="pln"><span class="n"><a href="#t7400">7400</a></span><span class="t"><span class="str">    Tensor: A newly quantized tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7401" class="pln"><span class="n"><a href="#t7401">7401</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7402" class="pln"><span class="n"><a href="#t7402">7402</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7403" class="pln"><span class="n"><a href="#t7403">7403</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7404" class="pln"><span class="n"><a href="#t7404">7404</a></span><span class="t"><span class="str">    >>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7405" class="pln"><span class="n"><a href="#t7405">7405</a></span><span class="t"><span class="str">    >>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7406" class="pln"><span class="n"><a href="#t7406">7406</a></span><span class="t"><span class="str">    tensor([[-1.,  0.],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7407" class="pln"><span class="n"><a href="#t7407">7407</a></span><span class="t"><span class="str">            [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7408" class="pln"><span class="n"><a href="#t7408">7408</a></span><span class="t"><span class="str">           quantization_scheme=torch.per_channel_affine,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7409" class="pln"><span class="n"><a href="#t7409">7409</a></span><span class="t"><span class="str">           scale=tensor([0.1000, 0.0100], dtype=torch.float64),</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7410" class="pln"><span class="n"><a href="#t7410">7410</a></span><span class="t"><span class="str">           zero_point=tensor([10,  0]), axis=0)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7411" class="pln"><span class="n"><a href="#t7411">7411</a></span><span class="t"><span class="str">    >>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7412" class="pln"><span class="n"><a href="#t7412">7412</a></span><span class="t"><span class="str">    tensor([[  0,  10],</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7413" class="pln"><span class="n"><a href="#t7413">7413</a></span><span class="t"><span class="str">            [100, 200]], dtype=torch.uint8)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7414" class="pln"><span class="n"><a href="#t7414">7414</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7415" class="pln"><span class="n"><a href="#t7415">7415</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7416" class="run"><span class="n"><a href="#t7416">7416</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7417" class="pln"><span class="n"><a href="#t7417">7417</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7418" class="pln"><span class="n"><a href="#t7418">7418</a></span><span class="t"><span class="str">Generator(device='cpu') -> Generator</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7419" class="pln"><span class="n"><a href="#t7419">7419</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7420" class="pln"><span class="n"><a href="#t7420">7420</a></span><span class="t"><span class="str">Creates and returns a generator object which manages the state of the algorithm that</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7421" class="pln"><span class="n"><a href="#t7421">7421</a></span><span class="t"><span class="str">produces pseudo random numbers. Used as a keyword argument in many :ref:`inplace-random-sampling`</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7422" class="pln"><span class="n"><a href="#t7422">7422</a></span><span class="t"><span class="str">functions.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7423" class="pln"><span class="n"><a href="#t7423">7423</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7424" class="pln"><span class="n"><a href="#t7424">7424</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7425" class="pln"><span class="n"><a href="#t7425">7425</a></span><span class="t"><span class="str">    device (:class:`torch.device`, optional): the desired device for the generator.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7426" class="pln"><span class="n"><a href="#t7426">7426</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7427" class="pln"><span class="n"><a href="#t7427">7427</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7428" class="pln"><span class="n"><a href="#t7428">7428</a></span><span class="t"><span class="str">    Generator: An torch.Generator object.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7429" class="pln"><span class="n"><a href="#t7429">7429</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7430" class="pln"><span class="n"><a href="#t7430">7430</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7431" class="pln"><span class="n"><a href="#t7431">7431</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7432" class="pln"><span class="n"><a href="#t7432">7432</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7433" class="pln"><span class="n"><a href="#t7433">7433</a></span><span class="t"><span class="str">    >>> g_cuda = torch.Generator(device='cuda')</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7434" class="pln"><span class="n"><a href="#t7434">7434</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7435" class="pln"><span class="n"><a href="#t7435">7435</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7436" class="pln"><span class="n"><a href="#t7436">7436</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7437" class="run"><span class="n"><a href="#t7437">7437</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">.</span><span class="nam">set_state</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7438" class="pln"><span class="n"><a href="#t7438">7438</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7439" class="pln"><span class="n"><a href="#t7439">7439</a></span><span class="t"><span class="str">Generator.set_state(new_state) -> void</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7440" class="pln"><span class="n"><a href="#t7440">7440</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7441" class="pln"><span class="n"><a href="#t7441">7441</a></span><span class="t"><span class="str">Sets the Generator state.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7442" class="pln"><span class="n"><a href="#t7442">7442</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7443" class="pln"><span class="n"><a href="#t7443">7443</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7444" class="pln"><span class="n"><a href="#t7444">7444</a></span><span class="t"><span class="str">    new_state (torch.ByteTensor): The desired state.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7445" class="pln"><span class="n"><a href="#t7445">7445</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7446" class="pln"><span class="n"><a href="#t7446">7446</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7447" class="pln"><span class="n"><a href="#t7447">7447</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7448" class="pln"><span class="n"><a href="#t7448">7448</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7449" class="pln"><span class="n"><a href="#t7449">7449</a></span><span class="t"><span class="str">    >>> g_cpu_other = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7450" class="pln"><span class="n"><a href="#t7450">7450</a></span><span class="t"><span class="str">    >>> g_cpu.set_state(g_cpu_other.get_state())</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7451" class="pln"><span class="n"><a href="#t7451">7451</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7452" class="pln"><span class="n"><a href="#t7452">7452</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7453" class="pln"><span class="n"><a href="#t7453">7453</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7454" class="run"><span class="n"><a href="#t7454">7454</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">.</span><span class="nam">get_state</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7455" class="pln"><span class="n"><a href="#t7455">7455</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7456" class="pln"><span class="n"><a href="#t7456">7456</a></span><span class="t"><span class="str">Generator.get_state() -> Tensor</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7457" class="pln"><span class="n"><a href="#t7457">7457</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7458" class="pln"><span class="n"><a href="#t7458">7458</a></span><span class="t"><span class="str">Returns the Generator state as a ``torch.ByteTensor``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7459" class="pln"><span class="n"><a href="#t7459">7459</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7460" class="pln"><span class="n"><a href="#t7460">7460</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7461" class="pln"><span class="n"><a href="#t7461">7461</a></span><span class="t"><span class="str">    Tensor: A ``torch.ByteTensor`` which contains all the necessary bits</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7462" class="pln"><span class="n"><a href="#t7462">7462</a></span><span class="t"><span class="str">    to restore a Generator to a specific point in time.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7463" class="pln"><span class="n"><a href="#t7463">7463</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7464" class="pln"><span class="n"><a href="#t7464">7464</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7465" class="pln"><span class="n"><a href="#t7465">7465</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7466" class="pln"><span class="n"><a href="#t7466">7466</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7467" class="pln"><span class="n"><a href="#t7467">7467</a></span><span class="t"><span class="str">    >>> g_cpu.get_state()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7468" class="pln"><span class="n"><a href="#t7468">7468</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7469" class="pln"><span class="n"><a href="#t7469">7469</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7470" class="pln"><span class="n"><a href="#t7470">7470</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7471" class="run"><span class="n"><a href="#t7471">7471</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">.</span><span class="nam">manual_seed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7472" class="pln"><span class="n"><a href="#t7472">7472</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7473" class="pln"><span class="n"><a href="#t7473">7473</a></span><span class="t"><span class="str">Generator.manual_seed(seed) -> Generator</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7474" class="pln"><span class="n"><a href="#t7474">7474</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7475" class="pln"><span class="n"><a href="#t7475">7475</a></span><span class="t"><span class="str">Sets the seed for generating random numbers. Returns a `torch.Generator` object.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7476" class="pln"><span class="n"><a href="#t7476">7476</a></span><span class="t"><span class="str">It is recommended to set a large seed, i.e. a number that has a good balance of 0</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7477" class="pln"><span class="n"><a href="#t7477">7477</a></span><span class="t"><span class="str">and 1 bits. Avoid having many 0 bits in the seed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7478" class="pln"><span class="n"><a href="#t7478">7478</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7479" class="pln"><span class="n"><a href="#t7479">7479</a></span><span class="t"><span class="str">Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7480" class="pln"><span class="n"><a href="#t7480">7480</a></span><span class="t"><span class="str">    seed (int): The desired seed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7481" class="pln"><span class="n"><a href="#t7481">7481</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7482" class="pln"><span class="n"><a href="#t7482">7482</a></span><span class="t"><span class="str">Returns:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7483" class="pln"><span class="n"><a href="#t7483">7483</a></span><span class="t"><span class="str">    Generator: An torch.Generator object.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7484" class="pln"><span class="n"><a href="#t7484">7484</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7485" class="pln"><span class="n"><a href="#t7485">7485</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7486" class="pln"><span class="n"><a href="#t7486">7486</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7487" class="pln"><span class="n"><a href="#t7487">7487</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7488" class="pln"><span class="n"><a href="#t7488">7488</a></span><span class="t"><span class="str">    >>> g_cpu.manual_seed(2147483647)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7489" class="pln"><span class="n"><a href="#t7489">7489</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7490" class="pln"><span class="n"><a href="#t7490">7490</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7491" class="pln"><span class="n"><a href="#t7491">7491</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7492" class="run"><span class="n"><a href="#t7492">7492</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">.</span><span class="nam">initial_seed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7493" class="pln"><span class="n"><a href="#t7493">7493</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7494" class="pln"><span class="n"><a href="#t7494">7494</a></span><span class="t"><span class="str">Generator.initial_seed() -> int</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7495" class="pln"><span class="n"><a href="#t7495">7495</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7496" class="pln"><span class="n"><a href="#t7496">7496</a></span><span class="t"><span class="str">Returns the initial seed for generating random numbers.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7497" class="pln"><span class="n"><a href="#t7497">7497</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7498" class="pln"><span class="n"><a href="#t7498">7498</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7499" class="pln"><span class="n"><a href="#t7499">7499</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7500" class="pln"><span class="n"><a href="#t7500">7500</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7501" class="pln"><span class="n"><a href="#t7501">7501</a></span><span class="t"><span class="str">    >>> g_cpu.initial_seed()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7502" class="pln"><span class="n"><a href="#t7502">7502</a></span><span class="t"><span class="str">    2147483647</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7503" class="pln"><span class="n"><a href="#t7503">7503</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7504" class="pln"><span class="n"><a href="#t7504">7504</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7505" class="pln"><span class="n"><a href="#t7505">7505</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7506" class="run"><span class="n"><a href="#t7506">7506</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">.</span><span class="nam">seed</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7507" class="pln"><span class="n"><a href="#t7507">7507</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7508" class="pln"><span class="n"><a href="#t7508">7508</a></span><span class="t"><span class="str">Generator.seed() -> int</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7509" class="pln"><span class="n"><a href="#t7509">7509</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7510" class="pln"><span class="n"><a href="#t7510">7510</a></span><span class="t"><span class="str">Gets a non-deterministic random number from std::random_device or the current</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7511" class="pln"><span class="n"><a href="#t7511">7511</a></span><span class="t"><span class="str">time and uses it to seed a Generator.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7512" class="pln"><span class="n"><a href="#t7512">7512</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7513" class="pln"><span class="n"><a href="#t7513">7513</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7514" class="pln"><span class="n"><a href="#t7514">7514</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7515" class="pln"><span class="n"><a href="#t7515">7515</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7516" class="pln"><span class="n"><a href="#t7516">7516</a></span><span class="t"><span class="str">    >>> g_cpu.seed()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7517" class="pln"><span class="n"><a href="#t7517">7517</a></span><span class="t"><span class="str">    1516516984916</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7518" class="pln"><span class="n"><a href="#t7518">7518</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7519" class="pln"><span class="n"><a href="#t7519">7519</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7520" class="pln"><span class="n"><a href="#t7520">7520</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7521" class="run"><span class="n"><a href="#t7521">7521</a></span><span class="t"><span class="nam">add_docstr</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">Generator</span><span class="op">.</span><span class="nam">device</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7522" class="pln"><span class="n"><a href="#t7522">7522</a></span><span class="t">           <span class="str">r"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7523" class="pln"><span class="n"><a href="#t7523">7523</a></span><span class="t"><span class="str">Generator.device -> device</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7524" class="pln"><span class="n"><a href="#t7524">7524</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7525" class="pln"><span class="n"><a href="#t7525">7525</a></span><span class="t"><span class="str">Gets the current device of the generator.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7526" class="pln"><span class="n"><a href="#t7526">7526</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7527" class="pln"><span class="n"><a href="#t7527">7527</a></span><span class="t"><span class="str">Example::</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7528" class="pln"><span class="n"><a href="#t7528">7528</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t7529" class="pln"><span class="n"><a href="#t7529">7529</a></span><span class="t"><span class="str">    >>> g_cpu = torch.Generator()</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7530" class="pln"><span class="n"><a href="#t7530">7530</a></span><span class="t"><span class="str">    >>> g_cpu.device</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7531" class="pln"><span class="n"><a href="#t7531">7531</a></span><span class="t"><span class="str">    device(type='cpu')</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7532" class="pln"><span class="n"><a href="#t7532">7532</a></span><span class="t"><span class="str">"""</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
</div>
<div id="footer">
    <div class="content">
        <p>
            <a class="nav" href="index.html">&#xab; index</a> &nbsp; &nbsp; <a class="nav" href="https://coverage.readthedocs.io">coverage.py v5.0.3</a>,
            created at 2020-03-12 22:47
        </p>
    </div>
</div>
</body>
</html>
