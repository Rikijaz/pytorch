<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=emulateIE7" />
    <title>Coverage for C:\Users\paper\Anaconda3\Lib\site-packages\torch\autograd\__init__.py: 49%</title>
    <link rel="stylesheet" href="style.css" type="text/css">
    <script type="text/javascript" src="jquery.min.js"></script>
    <script type="text/javascript" src="jquery.hotkeys.js"></script>
    <script type="text/javascript" src="jquery.isonscreen.js"></script>
    <script type="text/javascript" src="coverage_html.js"></script>
    <script type="text/javascript">
        jQuery(document).ready(coverage.pyfile_ready);
    </script>
</head>
<body class="pyfile">
<div id="header">
    <div class="content">
        <h1>Coverage for <b>C:\Users\paper\Anaconda3\Lib\site-packages\torch\autograd\__init__.py</b> :
            <span class="pc_cov">49%</span>
        </h1>
        <img id="keyboard_icon" src="keybd_closed.png" alt="Show keyboard shortcuts" />
        <h2 class="stats">
            61 statements &nbsp;
            <span class="run shortkey_r button_toggle_run">30 run</span>
            <span class="mis show_mis shortkey_m button_toggle_mis">31 missing</span>
            <span class="exc show_exc shortkey_x button_toggle_exc">0 excluded</span>
        </h2>
    </div>
</div>
<div class="help_panel">
    <img id="panel_icon" src="keybd_open.png" alt="Hide keyboard shortcuts" />
    <p class="legend">Hot-keys on this page</p>
    <div>
    <p class="keyhelp">
        <span class="key">r</span>
        <span class="key">m</span>
        <span class="key">x</span>
        <span class="key">p</span> &nbsp; toggle line displays
    </p>
    <p class="keyhelp">
        <span class="key">j</span>
        <span class="key">k</span> &nbsp; next/prev highlighted chunk
    </p>
    <p class="keyhelp">
        <span class="key">0</span> &nbsp; (zero) top of page
    </p>
    <p class="keyhelp">
        <span class="key">1</span> &nbsp; (one) first highlighted chunk
    </p>
    </div>
</div>
<div id="source">
    <p id="t1" class="pln"><span class="n"><a href="#t1">1</a></span><span class="t"><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t2" class="pln"><span class="n"><a href="#t2">2</a></span><span class="t"><span class="str">``torch.autograd`` provides classes and functions implementing automatic</span>&nbsp;</span><span class="r"></span></p>
    <p id="t3" class="pln"><span class="n"><a href="#t3">3</a></span><span class="t"><span class="str">differentiation of arbitrary scalar valued functions. It requires minimal</span>&nbsp;</span><span class="r"></span></p>
    <p id="t4" class="pln"><span class="n"><a href="#t4">4</a></span><span class="t"><span class="str">changes to the existing code - you only need to declare :class:`Tensor` s</span>&nbsp;</span><span class="r"></span></p>
    <p id="t5" class="pln"><span class="n"><a href="#t5">5</a></span><span class="t"><span class="str">for which gradients should be computed with the ``requires_grad=True`` keyword.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t6" class="pln"><span class="n"><a href="#t6">6</a></span><span class="t"><span class="str">"""</span>&nbsp;</span><span class="r"></span></p>
    <p id="t7" class="run"><span class="n"><a href="#t7">7</a></span><span class="t"><span class="key">import</span> <span class="nam">torch</span>&nbsp;</span><span class="r"></span></p>
    <p id="t8" class="run"><span class="n"><a href="#t8">8</a></span><span class="t"><span class="key">import</span> <span class="nam">warnings</span>&nbsp;</span><span class="r"></span></p>
    <p id="t9" class="pln"><span class="n"><a href="#t9">9</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t10" class="run"><span class="n"><a href="#t10">10</a></span><span class="t"><span class="key">from</span> <span class="op">.</span><span class="nam">variable</span> <span class="key">import</span> <span class="nam">Variable</span>&nbsp;</span><span class="r"></span></p>
    <p id="t11" class="run"><span class="n"><a href="#t11">11</a></span><span class="t"><span class="key">from</span> <span class="op">.</span><span class="nam">function</span> <span class="key">import</span> <span class="nam">Function</span><span class="op">,</span> <span class="nam">NestedIOFunction</span>&nbsp;</span><span class="r"></span></p>
    <p id="t12" class="run"><span class="n"><a href="#t12">12</a></span><span class="t"><span class="key">from</span> <span class="op">.</span><span class="nam">gradcheck</span> <span class="key">import</span> <span class="nam">gradcheck</span><span class="op">,</span> <span class="nam">gradgradcheck</span>&nbsp;</span><span class="r"></span></p>
    <p id="t13" class="run"><span class="n"><a href="#t13">13</a></span><span class="t"><span class="key">from</span> <span class="op">.</span><span class="nam">grad_mode</span> <span class="key">import</span> <span class="nam">no_grad</span><span class="op">,</span> <span class="nam">enable_grad</span><span class="op">,</span> <span class="nam">set_grad_enabled</span>&nbsp;</span><span class="r"></span></p>
    <p id="t14" class="run"><span class="n"><a href="#t14">14</a></span><span class="t"><span class="key">from</span> <span class="op">.</span><span class="nam">anomaly_mode</span> <span class="key">import</span> <span class="nam">detect_anomaly</span><span class="op">,</span> <span class="nam">set_detect_anomaly</span>&nbsp;</span><span class="r"></span></p>
    <p id="t15" class="run"><span class="n"><a href="#t15">15</a></span><span class="t"><span class="key">from</span> <span class="op">.</span> <span class="key">import</span> <span class="nam">profiler</span>&nbsp;</span><span class="r"></span></p>
    <p id="t16" class="pln"><span class="n"><a href="#t16">16</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t17" class="run"><span class="n"><a href="#t17">17</a></span><span class="t"><span class="nam">__all__</span> <span class="op">=</span> <span class="op">[</span><span class="str">'Variable'</span><span class="op">,</span> <span class="str">'Function'</span><span class="op">,</span> <span class="str">'backward'</span><span class="op">,</span> <span class="str">'grad_mode'</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t18" class="pln"><span class="n"><a href="#t18">18</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t19" class="pln"><span class="n"><a href="#t19">19</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t20" class="run"><span class="n"><a href="#t20">20</a></span><span class="t"><span class="key">def</span> <span class="nam">_make_grads</span><span class="op">(</span><span class="nam">outputs</span><span class="op">,</span> <span class="nam">grads</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t21" class="run"><span class="n"><a href="#t21">21</a></span><span class="t">    <span class="nam">new_grads</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t22" class="run"><span class="n"><a href="#t22">22</a></span><span class="t">    <span class="key">for</span> <span class="nam">out</span><span class="op">,</span> <span class="nam">grad</span> <span class="key">in</span> <span class="nam">zip</span><span class="op">(</span><span class="nam">outputs</span><span class="op">,</span> <span class="nam">grads</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t23" class="run"><span class="n"><a href="#t23">23</a></span><span class="t">        <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">grad</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t24" class="run"><span class="n"><a href="#t24">24</a></span><span class="t">            <span class="key">if</span> <span class="key">not</span> <span class="nam">out</span><span class="op">.</span><span class="nam">shape</span> <span class="op">==</span> <span class="nam">grad</span><span class="op">.</span><span class="nam">shape</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t25" class="mis show_mis"><span class="n"><a href="#t25">25</a></span><span class="t">                <span class="key">raise</span> <span class="nam">RuntimeError</span><span class="op">(</span><span class="str">"Mismatch in shape: grad_output["</span>&nbsp;</span><span class="r"></span></p>
    <p id="t26" class="pln"><span class="n"><a href="#t26">26</a></span><span class="t">                                   <span class="op">+</span> <span class="nam">str</span><span class="op">(</span><span class="nam">grads</span><span class="op">.</span><span class="nam">index</span><span class="op">(</span><span class="nam">grad</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="str">"] has a shape of "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t27" class="pln"><span class="n"><a href="#t27">27</a></span><span class="t">                                   <span class="op">+</span> <span class="nam">str</span><span class="op">(</span><span class="nam">grad</span><span class="op">.</span><span class="nam">shape</span><span class="op">)</span> <span class="op">+</span> <span class="str">" and output["</span>&nbsp;</span><span class="r"></span></p>
    <p id="t28" class="pln"><span class="n"><a href="#t28">28</a></span><span class="t">                                   <span class="op">+</span> <span class="nam">str</span><span class="op">(</span><span class="nam">outputs</span><span class="op">.</span><span class="nam">index</span><span class="op">(</span><span class="nam">out</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="str">"] has a shape of "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t29" class="pln"><span class="n"><a href="#t29">29</a></span><span class="t">                                   <span class="op">+</span> <span class="nam">str</span><span class="op">(</span><span class="nam">out</span><span class="op">.</span><span class="nam">shape</span><span class="op">)</span> <span class="op">+</span> <span class="str">"."</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t30" class="run"><span class="n"><a href="#t30">30</a></span><span class="t">            <span class="nam">new_grads</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">grad</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t31" class="mis show_mis"><span class="n"><a href="#t31">31</a></span><span class="t">        <span class="key">elif</span> <span class="nam">grad</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t32" class="mis show_mis"><span class="n"><a href="#t32">32</a></span><span class="t">            <span class="key">if</span> <span class="nam">out</span><span class="op">.</span><span class="nam">requires_grad</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t33" class="mis show_mis"><span class="n"><a href="#t33">33</a></span><span class="t">                <span class="key">if</span> <span class="nam">out</span><span class="op">.</span><span class="nam">numel</span><span class="op">(</span><span class="op">)</span> <span class="op">!=</span> <span class="num">1</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t34" class="mis show_mis"><span class="n"><a href="#t34">34</a></span><span class="t">                    <span class="key">raise</span> <span class="nam">RuntimeError</span><span class="op">(</span><span class="str">"grad can be implicitly created only for scalar outputs"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t35" class="mis show_mis"><span class="n"><a href="#t35">35</a></span><span class="t">                <span class="nam">new_grads</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">torch</span><span class="op">.</span><span class="nam">ones_like</span><span class="op">(</span><span class="nam">out</span><span class="op">,</span> <span class="nam">memory_format</span><span class="op">=</span><span class="nam">torch</span><span class="op">.</span><span class="nam">preserve_format</span><span class="op">)</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t36" class="pln"><span class="n"><a href="#t36">36</a></span><span class="t">            <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t37" class="mis show_mis"><span class="n"><a href="#t37">37</a></span><span class="t">                <span class="nam">new_grads</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="key">None</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t38" class="pln"><span class="n"><a href="#t38">38</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t39" class="mis show_mis"><span class="n"><a href="#t39">39</a></span><span class="t">            <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"gradients can be either Tensors or None, but got "</span> <span class="op">+</span>&nbsp;</span><span class="r"></span></p>
    <p id="t40" class="pln"><span class="n"><a href="#t40">40</a></span><span class="t">                            <span class="nam">type</span><span class="op">(</span><span class="nam">grad</span><span class="op">)</span><span class="op">.</span><span class="nam">__name__</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t41" class="run"><span class="n"><a href="#t41">41</a></span><span class="t">    <span class="key">return</span> <span class="nam">tuple</span><span class="op">(</span><span class="nam">new_grads</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t42" class="pln"><span class="n"><a href="#t42">42</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t43" class="pln"><span class="n"><a href="#t43">43</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t44" class="run"><span class="n"><a href="#t44">44</a></span><span class="t"><span class="key">def</span> <span class="nam">backward</span><span class="op">(</span><span class="nam">tensors</span><span class="op">,</span> <span class="nam">grad_tensors</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">retain_graph</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">create_graph</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">grad_variables</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t45" class="pln"><span class="n"><a href="#t45">45</a></span><span class="t">    <span class="str">r"""Computes the sum of gradients of given tensors w.r.t. graph leaves.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t46" class="pln"><span class="n"><a href="#t46">46</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t47" class="pln"><span class="n"><a href="#t47">47</a></span><span class="t"><span class="str">    The graph is differentiated using the chain rule. If any of ``tensors``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t48" class="pln"><span class="n"><a href="#t48">48</a></span><span class="t"><span class="str">    are non-scalar (i.e. their data has more than one element) and require</span>&nbsp;</span><span class="r"></span></p>
    <p id="t49" class="pln"><span class="n"><a href="#t49">49</a></span><span class="t"><span class="str">    gradient, then the Jacobian-vector product would be computed, in this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t50" class="pln"><span class="n"><a href="#t50">50</a></span><span class="t"><span class="str">    case the function additionally requires specifying ``grad_tensors``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t51" class="pln"><span class="n"><a href="#t51">51</a></span><span class="t"><span class="str">    It should be a sequence of matching length, that contains the "vector"</span>&nbsp;</span><span class="r"></span></p>
    <p id="t52" class="pln"><span class="n"><a href="#t52">52</a></span><span class="t"><span class="str">    in the Jacobian-vector product, usually the gradient of the differentiated</span>&nbsp;</span><span class="r"></span></p>
    <p id="t53" class="pln"><span class="n"><a href="#t53">53</a></span><span class="t"><span class="str">    function w.r.t. corresponding tensors (``None`` is an acceptable value for</span>&nbsp;</span><span class="r"></span></p>
    <p id="t54" class="pln"><span class="n"><a href="#t54">54</a></span><span class="t"><span class="str">    all tensors that don't need gradient tensors).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t55" class="pln"><span class="n"><a href="#t55">55</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t56" class="pln"><span class="n"><a href="#t56">56</a></span><span class="t"><span class="str">    This function accumulates gradients in the leaves - you might need to zero</span>&nbsp;</span><span class="r"></span></p>
    <p id="t57" class="pln"><span class="n"><a href="#t57">57</a></span><span class="t"><span class="str">    them before calling it.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t58" class="pln"><span class="n"><a href="#t58">58</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t59" class="pln"><span class="n"><a href="#t59">59</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t60" class="pln"><span class="n"><a href="#t60">60</a></span><span class="t"><span class="str">        tensors (sequence of Tensor): Tensors of which the derivative will be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t61" class="pln"><span class="n"><a href="#t61">61</a></span><span class="t"><span class="str">            computed.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t62" class="pln"><span class="n"><a href="#t62">62</a></span><span class="t"><span class="str">        grad_tensors (sequence of (Tensor or None)): The "vector" in the Jacobian-vector</span>&nbsp;</span><span class="r"></span></p>
    <p id="t63" class="pln"><span class="n"><a href="#t63">63</a></span><span class="t"><span class="str">            product, usually gradients w.r.t. each element of corresponding tensors.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t64" class="pln"><span class="n"><a href="#t64">64</a></span><span class="t"><span class="str">            None values can be specified for scalar Tensors or ones that don't require</span>&nbsp;</span><span class="r"></span></p>
    <p id="t65" class="pln"><span class="n"><a href="#t65">65</a></span><span class="t"><span class="str">            grad. If a None value would be acceptable for all grad_tensors, then this</span>&nbsp;</span><span class="r"></span></p>
    <p id="t66" class="pln"><span class="n"><a href="#t66">66</a></span><span class="t"><span class="str">            argument is optional.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t67" class="pln"><span class="n"><a href="#t67">67</a></span><span class="t"><span class="str">        retain_graph (bool, optional): If ``False``, the graph used to compute the grad</span>&nbsp;</span><span class="r"></span></p>
    <p id="t68" class="pln"><span class="n"><a href="#t68">68</a></span><span class="t"><span class="str">            will be freed. Note that in nearly all cases setting this option to ``True``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t69" class="pln"><span class="n"><a href="#t69">69</a></span><span class="t"><span class="str">            is not needed and often can be worked around in a much more efficient</span>&nbsp;</span><span class="r"></span></p>
    <p id="t70" class="pln"><span class="n"><a href="#t70">70</a></span><span class="t"><span class="str">            way. Defaults to the value of ``create_graph``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t71" class="pln"><span class="n"><a href="#t71">71</a></span><span class="t"><span class="str">        create_graph (bool, optional): If ``True``, graph of the derivative will</span>&nbsp;</span><span class="r"></span></p>
    <p id="t72" class="pln"><span class="n"><a href="#t72">72</a></span><span class="t"><span class="str">            be constructed, allowing to compute higher order derivative products.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t73" class="pln"><span class="n"><a href="#t73">73</a></span><span class="t"><span class="str">            Defaults to ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t74" class="pln"><span class="n"><a href="#t74">74</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t75" class="run"><span class="n"><a href="#t75">75</a></span><span class="t">    <span class="key">if</span> <span class="nam">grad_variables</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t76" class="mis show_mis"><span class="n"><a href="#t76">76</a></span><span class="t">        <span class="nam">warnings</span><span class="op">.</span><span class="nam">warn</span><span class="op">(</span><span class="str">"'grad_variables' is deprecated. Use 'grad_tensors' instead."</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t77" class="mis show_mis"><span class="n"><a href="#t77">77</a></span><span class="t">        <span class="key">if</span> <span class="nam">grad_tensors</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t78" class="mis show_mis"><span class="n"><a href="#t78">78</a></span><span class="t">            <span class="nam">grad_tensors</span> <span class="op">=</span> <span class="nam">grad_variables</span>&nbsp;</span><span class="r"></span></p>
    <p id="t79" class="pln"><span class="n"><a href="#t79">79</a></span><span class="t">        <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t80" class="mis show_mis"><span class="n"><a href="#t80">80</a></span><span class="t">            <span class="key">raise</span> <span class="nam">RuntimeError</span><span class="op">(</span><span class="str">"'grad_tensors' and 'grad_variables' (deprecated) "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t81" class="pln"><span class="n"><a href="#t81">81</a></span><span class="t">                               <span class="str">"arguments both passed to backward(). Please only "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t82" class="pln"><span class="n"><a href="#t82">82</a></span><span class="t">                               <span class="str">"use 'grad_tensors'."</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t83" class="pln"><span class="n"><a href="#t83">83</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t84" class="run"><span class="n"><a href="#t84">84</a></span><span class="t">    <span class="nam">tensors</span> <span class="op">=</span> <span class="op">(</span><span class="nam">tensors</span><span class="op">,</span><span class="op">)</span> <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">tensors</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">else</span> <span class="nam">tuple</span><span class="op">(</span><span class="nam">tensors</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t85" class="pln"><span class="n"><a href="#t85">85</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t86" class="run"><span class="n"><a href="#t86">86</a></span><span class="t">    <span class="key">if</span> <span class="nam">grad_tensors</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t87" class="mis show_mis"><span class="n"><a href="#t87">87</a></span><span class="t">        <span class="nam">grad_tensors</span> <span class="op">=</span> <span class="op">[</span><span class="key">None</span><span class="op">]</span> <span class="op">*</span> <span class="nam">len</span><span class="op">(</span><span class="nam">tensors</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t88" class="run"><span class="n"><a href="#t88">88</a></span><span class="t">    <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">grad_tensors</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t89" class="run"><span class="n"><a href="#t89">89</a></span><span class="t">        <span class="nam">grad_tensors</span> <span class="op">=</span> <span class="op">[</span><span class="nam">grad_tensors</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t90" class="pln"><span class="n"><a href="#t90">90</a></span><span class="t">    <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t91" class="mis show_mis"><span class="n"><a href="#t91">91</a></span><span class="t">        <span class="nam">grad_tensors</span> <span class="op">=</span> <span class="nam">list</span><span class="op">(</span><span class="nam">grad_tensors</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t92" class="pln"><span class="n"><a href="#t92">92</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t93" class="run"><span class="n"><a href="#t93">93</a></span><span class="t">    <span class="nam">grad_tensors</span> <span class="op">=</span> <span class="nam">_make_grads</span><span class="op">(</span><span class="nam">tensors</span><span class="op">,</span> <span class="nam">grad_tensors</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t94" class="run"><span class="n"><a href="#t94">94</a></span><span class="t">    <span class="key">if</span> <span class="nam">retain_graph</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t95" class="run"><span class="n"><a href="#t95">95</a></span><span class="t">        <span class="nam">retain_graph</span> <span class="op">=</span> <span class="nam">create_graph</span>&nbsp;</span><span class="r"></span></p>
    <p id="t96" class="pln"><span class="n"><a href="#t96">96</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t97" class="run"><span class="n"><a href="#t97">97</a></span><span class="t">    <span class="nam">Variable</span><span class="op">.</span><span class="nam">_execution_engine</span><span class="op">.</span><span class="nam">run_backward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t98" class="pln"><span class="n"><a href="#t98">98</a></span><span class="t">        <span class="nam">tensors</span><span class="op">,</span> <span class="nam">grad_tensors</span><span class="op">,</span> <span class="nam">retain_graph</span><span class="op">,</span> <span class="nam">create_graph</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t99" class="pln"><span class="n"><a href="#t99">99</a></span><span class="t">        <span class="nam">allow_unreachable</span><span class="op">=</span><span class="key">True</span><span class="op">)</span>  <span class="com"># allow_unreachable flag</span>&nbsp;</span><span class="r"></span></p>
    <p id="t100" class="pln"><span class="n"><a href="#t100">100</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t101" class="pln"><span class="n"><a href="#t101">101</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t102" class="run"><span class="n"><a href="#t102">102</a></span><span class="t"><span class="key">def</span> <span class="nam">grad</span><span class="op">(</span><span class="nam">outputs</span><span class="op">,</span> <span class="nam">inputs</span><span class="op">,</span> <span class="nam">grad_outputs</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">retain_graph</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">create_graph</span><span class="op">=</span><span class="key">False</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t103" class="pln"><span class="n"><a href="#t103">103</a></span><span class="t">         <span class="nam">only_inputs</span><span class="op">=</span><span class="key">True</span><span class="op">,</span> <span class="nam">allow_unused</span><span class="op">=</span><span class="key">False</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t104" class="pln"><span class="n"><a href="#t104">104</a></span><span class="t">    <span class="str">r"""Computes and returns the sum of gradients of outputs w.r.t. the inputs.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t105" class="pln"><span class="n"><a href="#t105">105</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t106" class="pln"><span class="n"><a href="#t106">106</a></span><span class="t"><span class="str">    ``grad_outputs`` should be a sequence of length matching ``output``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t107" class="pln"><span class="n"><a href="#t107">107</a></span><span class="t"><span class="str">    containing the "vector" in Jacobian-vector product, usually the pre-computed</span>&nbsp;</span><span class="r"></span></p>
    <p id="t108" class="pln"><span class="n"><a href="#t108">108</a></span><span class="t"><span class="str">    gradients w.r.t. each of the outputs. If an output doesn't require_grad,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t109" class="pln"><span class="n"><a href="#t109">109</a></span><span class="t"><span class="str">    then the gradient can be ``None``).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t110" class="pln"><span class="n"><a href="#t110">110</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t111" class="pln"><span class="n"><a href="#t111">111</a></span><span class="t"><span class="str">    If ``only_inputs`` is ``True``, the function will only return a list of gradients</span>&nbsp;</span><span class="r"></span></p>
    <p id="t112" class="pln"><span class="n"><a href="#t112">112</a></span><span class="t"><span class="str">    w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining</span>&nbsp;</span><span class="r"></span></p>
    <p id="t113" class="pln"><span class="n"><a href="#t113">113</a></span><span class="t"><span class="str">    leaves will still be computed, and will be accumulated into their ``.grad``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t114" class="pln"><span class="n"><a href="#t114">114</a></span><span class="t"><span class="str">    attribute.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t115" class="pln"><span class="n"><a href="#t115">115</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t116" class="pln"><span class="n"><a href="#t116">116</a></span><span class="t"><span class="str">    Arguments:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t117" class="pln"><span class="n"><a href="#t117">117</a></span><span class="t"><span class="str">        outputs (sequence of Tensor): outputs of the differentiated function.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t118" class="pln"><span class="n"><a href="#t118">118</a></span><span class="t"><span class="str">        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be</span>&nbsp;</span><span class="r"></span></p>
    <p id="t119" class="pln"><span class="n"><a href="#t119">119</a></span><span class="t"><span class="str">            returned (and not accumulated into ``.grad``).</span>&nbsp;</span><span class="r"></span></p>
    <p id="t120" class="pln"><span class="n"><a href="#t120">120</a></span><span class="t"><span class="str">        grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t121" class="pln"><span class="n"><a href="#t121">121</a></span><span class="t"><span class="str">            Usually gradients w.r.t. each output. None values can be specified for scalar</span>&nbsp;</span><span class="r"></span></p>
    <p id="t122" class="pln"><span class="n"><a href="#t122">122</a></span><span class="t"><span class="str">            Tensors or ones that don't require grad. If a None value would be acceptable</span>&nbsp;</span><span class="r"></span></p>
    <p id="t123" class="pln"><span class="n"><a href="#t123">123</a></span><span class="t"><span class="str">            for all grad_tensors, then this argument is optional. Default: None.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t124" class="pln"><span class="n"><a href="#t124">124</a></span><span class="t"><span class="str">        retain_graph (bool, optional): If ``False``, the graph used to compute the grad</span>&nbsp;</span><span class="r"></span></p>
    <p id="t125" class="pln"><span class="n"><a href="#t125">125</a></span><span class="t"><span class="str">            will be freed. Note that in nearly all cases setting this option to ``True``</span>&nbsp;</span><span class="r"></span></p>
    <p id="t126" class="pln"><span class="n"><a href="#t126">126</a></span><span class="t"><span class="str">            is not needed and often can be worked around in a much more efficient</span>&nbsp;</span><span class="r"></span></p>
    <p id="t127" class="pln"><span class="n"><a href="#t127">127</a></span><span class="t"><span class="str">            way. Defaults to the value of ``create_graph``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t128" class="pln"><span class="n"><a href="#t128">128</a></span><span class="t"><span class="str">        create_graph (bool, optional): If ``True``, graph of the derivative will</span>&nbsp;</span><span class="r"></span></p>
    <p id="t129" class="pln"><span class="n"><a href="#t129">129</a></span><span class="t"><span class="str">            be constructed, allowing to compute higher order derivative products.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t130" class="pln"><span class="n"><a href="#t130">130</a></span><span class="t"><span class="str">            Default: ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t131" class="pln"><span class="n"><a href="#t131">131</a></span><span class="t"><span class="str">        allow_unused (bool, optional): If ``False``, specifying inputs that were not</span>&nbsp;</span><span class="r"></span></p>
    <p id="t132" class="pln"><span class="n"><a href="#t132">132</a></span><span class="t"><span class="str">            used when computing outputs (and therefore their grad is always zero)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t133" class="pln"><span class="n"><a href="#t133">133</a></span><span class="t"><span class="str">            is an error. Defaults to ``False``.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t134" class="pln"><span class="n"><a href="#t134">134</a></span><span class="t"><span class="str">    """</span>&nbsp;</span><span class="r"></span></p>
    <p id="t135" class="mis show_mis"><span class="n"><a href="#t135">135</a></span><span class="t">    <span class="key">if</span> <span class="key">not</span> <span class="nam">only_inputs</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t136" class="mis show_mis"><span class="n"><a href="#t136">136</a></span><span class="t">        <span class="nam">warnings</span><span class="op">.</span><span class="nam">warn</span><span class="op">(</span><span class="str">"only_inputs argument is deprecated and is ignored now "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t137" class="pln"><span class="n"><a href="#t137">137</a></span><span class="t">                      <span class="str">"(defaults to True). To accumulate gradient for other "</span>&nbsp;</span><span class="r"></span></p>
    <p id="t138" class="pln"><span class="n"><a href="#t138">138</a></span><span class="t">                      <span class="str">"parts of the graph, please use torch.autograd.backward."</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t139" class="pln"><span class="n"><a href="#t139">139</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t140" class="mis show_mis"><span class="n"><a href="#t140">140</a></span><span class="t">    <span class="nam">outputs</span> <span class="op">=</span> <span class="op">(</span><span class="nam">outputs</span><span class="op">,</span><span class="op">)</span> <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">outputs</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">else</span> <span class="nam">tuple</span><span class="op">(</span><span class="nam">outputs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t141" class="mis show_mis"><span class="n"><a href="#t141">141</a></span><span class="t">    <span class="nam">inputs</span> <span class="op">=</span> <span class="op">(</span><span class="nam">inputs</span><span class="op">,</span><span class="op">)</span> <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">else</span> <span class="nam">tuple</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t142" class="pln"><span class="n"><a href="#t142">142</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t143" class="mis show_mis"><span class="n"><a href="#t143">143</a></span><span class="t">    <span class="key">if</span> <span class="nam">grad_outputs</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t144" class="mis show_mis"><span class="n"><a href="#t144">144</a></span><span class="t">        <span class="nam">grad_outputs</span> <span class="op">=</span> <span class="op">[</span><span class="key">None</span><span class="op">]</span> <span class="op">*</span> <span class="nam">len</span><span class="op">(</span><span class="nam">outputs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t145" class="mis show_mis"><span class="n"><a href="#t145">145</a></span><span class="t">    <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">grad_outputs</span><span class="op">,</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t146" class="mis show_mis"><span class="n"><a href="#t146">146</a></span><span class="t">        <span class="nam">grad_outputs</span> <span class="op">=</span> <span class="op">[</span><span class="nam">grad_outputs</span><span class="op">]</span>&nbsp;</span><span class="r"></span></p>
    <p id="t147" class="pln"><span class="n"><a href="#t147">147</a></span><span class="t">    <span class="key">else</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t148" class="mis show_mis"><span class="n"><a href="#t148">148</a></span><span class="t">        <span class="nam">grad_outputs</span> <span class="op">=</span> <span class="nam">list</span><span class="op">(</span><span class="nam">grad_outputs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t149" class="pln"><span class="n"><a href="#t149">149</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t150" class="mis show_mis"><span class="n"><a href="#t150">150</a></span><span class="t">    <span class="nam">grad_outputs</span> <span class="op">=</span> <span class="nam">_make_grads</span><span class="op">(</span><span class="nam">outputs</span><span class="op">,</span> <span class="nam">grad_outputs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t151" class="pln"><span class="n"><a href="#t151">151</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t152" class="mis show_mis"><span class="n"><a href="#t152">152</a></span><span class="t">    <span class="key">if</span> <span class="nam">retain_graph</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t153" class="mis show_mis"><span class="n"><a href="#t153">153</a></span><span class="t">        <span class="nam">retain_graph</span> <span class="op">=</span> <span class="nam">create_graph</span>&nbsp;</span><span class="r"></span></p>
    <p id="t154" class="pln"><span class="n"><a href="#t154">154</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t155" class="mis show_mis"><span class="n"><a href="#t155">155</a></span><span class="t">    <span class="key">return</span> <span class="nam">Variable</span><span class="op">.</span><span class="nam">_execution_engine</span><span class="op">.</span><span class="nam">run_backward</span><span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p id="t156" class="pln"><span class="n"><a href="#t156">156</a></span><span class="t">        <span class="nam">outputs</span><span class="op">,</span> <span class="nam">grad_outputs</span><span class="op">,</span> <span class="nam">retain_graph</span><span class="op">,</span> <span class="nam">create_graph</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p id="t157" class="pln"><span class="n"><a href="#t157">157</a></span><span class="t">        <span class="nam">inputs</span><span class="op">,</span> <span class="nam">allow_unused</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t158" class="pln"><span class="n"><a href="#t158">158</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t159" class="pln"><span class="n"><a href="#t159">159</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t160" class="pln"><span class="n"><a href="#t160">160</a></span><span class="t"><span class="com"># This function applies in case of gradient checkpointing for memory</span>&nbsp;</span><span class="r"></span></p>
    <p id="t161" class="pln"><span class="n"><a href="#t161">161</a></span><span class="t"><span class="com"># optimization. Currently, for gradient checkpointing, we only support imperative</span>&nbsp;</span><span class="r"></span></p>
    <p id="t162" class="pln"><span class="n"><a href="#t162">162</a></span><span class="t"><span class="com"># backwards call i.e. torch.autograd.backward() and the torch.autograd.grad() won't</span>&nbsp;</span><span class="r"></span></p>
    <p id="t163" class="pln"><span class="n"><a href="#t163">163</a></span><span class="t"><span class="com"># work. The reason being that: torch.autograd.grad() only calculates the grads</span>&nbsp;</span><span class="r"></span></p>
    <p id="t164" class="pln"><span class="n"><a href="#t164">164</a></span><span class="t"><span class="com"># for the inputs that are passed by user but it doesn't calculate grad for</span>&nbsp;</span><span class="r"></span></p>
    <p id="t165" class="pln"><span class="n"><a href="#t165">165</a></span><span class="t"><span class="com"># anything else e.g. model parameters like weights, bias etc. However, for</span>&nbsp;</span><span class="r"></span></p>
    <p id="t166" class="pln"><span class="n"><a href="#t166">166</a></span><span class="t"><span class="com"># torch.autograd.backward(), we would actually compute the grad for the weights as well.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t167" class="pln"><span class="n"><a href="#t167">167</a></span><span class="t"><span class="com">#</span>&nbsp;</span><span class="r"></span></p>
    <p id="t168" class="pln"><span class="n"><a href="#t168">168</a></span><span class="t"><span class="com"># This function returns whether the checkpointing is valid i.e. torch.autograd.backward</span>&nbsp;</span><span class="r"></span></p>
    <p id="t169" class="pln"><span class="n"><a href="#t169">169</a></span><span class="t"><span class="com"># or not i.e. torch.autograd.grad. The implementation works by maintaining a thread</span>&nbsp;</span><span class="r"></span></p>
    <p id="t170" class="pln"><span class="n"><a href="#t170">170</a></span><span class="t"><span class="com"># local variable in torch/csrc/autograd/engine.cpp which looks at the NodeTask</span>&nbsp;</span><span class="r"></span></p>
    <p id="t171" class="pln"><span class="n"><a href="#t171">171</a></span><span class="t"><span class="com"># in the stack and before a NodeTask is executed in evaluate_function, it</span>&nbsp;</span><span class="r"></span></p>
    <p id="t172" class="pln"><span class="n"><a href="#t172">172</a></span><span class="t"><span class="com"># checks for whether reentrant backwards is imperative or not.</span>&nbsp;</span><span class="r"></span></p>
    <p id="t173" class="pln"><span class="n"><a href="#t173">173</a></span><span class="t"><span class="com"># See https://github.com/pytorch/pytorch/pull/4594 for more discussion/context</span>&nbsp;</span><span class="r"></span></p>
    <p id="t174" class="run"><span class="n"><a href="#t174">174</a></span><span class="t"><span class="key">def</span> <span class="nam">_is_checkpoint_valid</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t175" class="mis show_mis"><span class="n"><a href="#t175">175</a></span><span class="t">    <span class="key">return</span> <span class="nam">Variable</span><span class="op">.</span><span class="nam">_execution_engine</span><span class="op">.</span><span class="nam">is_checkpoint_valid</span><span class="op">(</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t176" class="pln"><span class="n"><a href="#t176">176</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t177" class="pln"><span class="n"><a href="#t177">177</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t178" class="run"><span class="n"><a href="#t178">178</a></span><span class="t"><span class="key">def</span> <span class="nam">variable</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t179" class="mis show_mis"><span class="n"><a href="#t179">179</a></span><span class="t">    <span class="nam">warnings</span><span class="op">.</span><span class="nam">warn</span><span class="op">(</span><span class="str">"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t180" class="mis show_mis"><span class="n"><a href="#t180">180</a></span><span class="t">    <span class="key">return</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">tensor</span><span class="op">(</span><span class="op">*</span><span class="nam">args</span><span class="op">,</span> <span class="op">**</span><span class="nam">kwargs</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
    <p id="t181" class="pln"><span class="n"><a href="#t181">181</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p id="t182" class="run"><span class="n"><a href="#t182">182</a></span><span class="t"><span class="key">if</span> <span class="key">not</span> <span class="nam">torch</span><span class="op">.</span><span class="nam">_C</span><span class="op">.</span><span class="nam">_autograd_init</span><span class="op">(</span><span class="op">)</span><span class="op">:</span>&nbsp;</span><span class="r"></span></p>
    <p id="t183" class="mis show_mis"><span class="n"><a href="#t183">183</a></span><span class="t">    <span class="key">raise</span> <span class="nam">RuntimeError</span><span class="op">(</span><span class="str">"autograd initialization failed"</span><span class="op">)</span>&nbsp;</span><span class="r"></span></p>
</div>
<div id="footer">
    <div class="content">
        <p>
            <a class="nav" href="index.html">&#xab; index</a> &nbsp; &nbsp; <a class="nav" href="https://coverage.readthedocs.io">coverage.py v5.0.3</a>,
            created at 2020-03-13 13:54
        </p>
    </div>
</div>
</body>
</html>
